"""
Servi√ßo Multimodal para Gemma 3n
Implementa funcionalidades de texto, imagem, √°udio e v√≠deo conforme documenta√ß√£o oficial
"""

import os
import logging
import base64
import tempfile
import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText, pipeline
from io import BytesIO
import requests
from typing import Dict, List, Any, Union, Optional

from config.settings import BackendConfig

logger = logging.getLogger(__name__)

class MultimodalService:
    """Servi√ßo para processamento multimodal com Gemma 3n"""
    
    def __init__(self, gemma_service):
        self.gemma_service = gemma_service
        self.processor = None
        self.model = None
        self.image_text_pipeline = None
        self.initialized = False
        
        self._initialize_multimodal()
    
    def _initialize_multimodal(self):
        """Inicializar componentes multimodais de forma otimizada"""
        try:
            logger.info("üé® Inicializando servi√ßos multimodais Gemma 3n (modo otimizado)...")
            
            # Verificar se o servi√ßo principal foi inicializado
            if not self.gemma_service.is_initialized:
                logger.warning("‚ö†Ô∏è Servi√ßo Gemma principal n√£o inicializado, usando modo fallback multimodal")
                self.initialized = False
                return False
            
            # Verificar se o modelo principal j√° tem processor (para reutilizar)
            if hasattr(self.gemma_service, 'processor') and self.gemma_service.processor:
                logger.info("‚ôªÔ∏è Reutilizando processor do servi√ßo principal")
                self.processor = self.gemma_service.processor
                
            # Verificar se j√° tem modelo multimodal (para reutilizar)
            if hasattr(self.gemma_service, 'chat_model') and self.gemma_service.chat_model:
                logger.info("‚ôªÔ∏è Reutilizando modelo multimodal do servi√ßo principal")
                self.model = self.gemma_service.chat_model
                
            # Verificar se j√° tem pipeline (para reutilizar)
            if hasattr(self.gemma_service, 'pipeline') and self.gemma_service.pipeline:
                logger.info("‚ôªÔ∏è Reutilizando pipeline do servi√ßo principal")
                self.image_text_pipeline = self.gemma_service.pipeline
            
            # Se conseguiu reutilizar componentes, marcar como inicializado
            if self.processor or self.model or self.image_text_pipeline:
                self.initialized = True
                logger.info("üéØ Servi√ßos multimodais inicializados (modo reutiliza√ß√£o)!")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum componente multimodal dispon√≠vel para reutiliza√ß√£o")
                self.initialized = False
                return False
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o multimodal: {e}")
            self.initialized = False
            return False
    
    def analyze_image(self, image_data: Union[str, bytes], prompt: str = "Descreva esta imagem em detalhes.", language: str = "pt-BR") -> Dict[str, Any]:
        """
        Analisar imagem usando Gemma 3n
        Baseado na documenta√ß√£o oficial: image-text-to-text
        """
        try:
            if not self.initialized:
                return self._fallback_image_response(prompt, language)
            
            # Processar imagem
            image = self._process_image_input(image_data)
            if not image:
                return {"error": "Falha ao processar imagem"}
            
            # M√âTODO 1: Pipeline API (recomendado)
            if self.image_text_pipeline:
                response = self._analyze_with_pipeline(image, prompt, language)
                if response:
                    return response
            
            # M√âTODO 2: Chat Template com multimodal
            if self.processor and self.model:
                response = self._analyze_with_chat_template(image, prompt, language)
                if response:
                    return response
            
            # Fallback
            return self._fallback_image_response(prompt, language)
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de imagem: {e}")
            return {"error": f"Erro na an√°lise: {str(e)}"}
    
    def _analyze_with_pipeline(self, image: Image.Image, prompt: str, language: str) -> Optional[Dict[str, Any]]:
        """Analisar imagem usando Pipeline API"""
        try:
            # Usar format de mensagens conforme documenta√ß√£o
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "Voc√™ √© um assistente especializado em an√°lise de imagens. Responda em portugu√™s brasileiro."}]
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            output = self.image_text_pipeline(
                text=messages, 
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7
            )
            
            response_text = output[0]["generated_text"][-1]["content"]
            
            return {
                "success": True,
                "analysis": response_text,
                "method": "pipeline",
                "image_processed": True,
                "language": language
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Pipeline de imagem falhou: {e}")
            return None
    
    def _analyze_with_chat_template(self, image: Image.Image, prompt: str, language: str) -> Optional[Dict[str, Any]]:
        """Analisar imagem usando Chat Template"""
        try:
            # Mensagens multimodais conforme documenta√ß√£o oficial
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "Voc√™ √© um assistente especializado em an√°lise de imagens. Responda em portugu√™s brasileiro."}]
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            inputs = self.processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt"
            ).to(self.model.device, dtype=BackendConfig.get_torch_dtype())
            
            input_len = inputs["input_ids"].shape[-1]
            
            with torch.inference_mode():
                generation = self.model.generate(
                    **inputs, 
                    max_new_tokens=300,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9
                )
                
            decoded = self.processor.decode(
                generation[0][input_len:], 
                skip_special_tokens=True
            )
            
            return {
                "success": True,
                "analysis": decoded,
                "method": "chat_template",
                "image_processed": True,
                "language": language
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chat Template de imagem falhou: {e}")
            return None
    
    def process_audio(self, audio_data: Union[str, bytes], prompt: str = "Transcreva este √°udio e complete o que foi dito.", language: str = "pt-BR") -> Dict[str, Any]:
        """
        Processar √°udio usando Gemma 3n
        Baseado na documenta√ß√£o: audio-text-to-text
        """
        try:
            if not self.initialized:
                return self._fallback_audio_response(prompt, language)
            
            # Processar dados de √°udio
            audio_path = self._process_audio_input(audio_data)
            if not audio_path:
                return {"error": "Falha ao processar √°udio"}
            
            # Usar chat template para √°udio conforme documenta√ß√£o
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "audio", "audio": audio_path},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            inputs = self.processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt"
            ).to(self.model.device, dtype=BackendConfig.get_torch_dtype())
            
            input_len = inputs["input_ids"].shape[-1]
            
            with torch.inference_mode():
                generation = self.model.generate(
                    **inputs,
                    max_new_tokens=400,
                    do_sample=True,
                    temperature=0.7
                )
            
            decoded = self.processor.decode(
                generation[0][input_len:], 
                skip_special_tokens=True
            )
            
            # Limpar arquivo tempor√°rio
            if os.path.exists(audio_path):
                os.remove(audio_path)
            
            return {
                "success": True,
                "transcription": decoded,
                "method": "chat_template",
                "audio_processed": True,
                "language": language
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento de √°udio: {e}")
            return {"error": f"Erro no √°udio: {str(e)}"}
    
    def analyze_medical_image(self, image_data: Union[str, bytes], symptoms: str = "", language: str = "pt-BR") -> Dict[str, Any]:
        """An√°lise m√©dica especializada de imagens"""
        medical_prompt = f"""
        Analise esta imagem m√©dica cuidadosamente. 
        Sintomas relatados: {symptoms}
        
        Forne√ßa:
        1. Observa√ß√µes visuais detalhadas
        2. Poss√≠veis condi√ß√µes a considerar
        3. Recomenda√ß√µes para consulta m√©dica
        
        IMPORTANTE: Esta √© apenas uma an√°lise preliminar. Sempre consulte um m√©dico profissional.
        """
        
        result = self.analyze_image(image_data, medical_prompt, language)
        if result.get("success"):
            result["medical_analysis"] = True
            result["disclaimer"] = "Esta an√°lise n√£o substitui consulta m√©dica profissional"
        
        return result
    
    def _process_image_input(self, image_data: Union[str, bytes]) -> Optional[Image.Image]:
        """Processar dados de entrada de imagem"""
        try:
            if isinstance(image_data, str):
                # Se for base64
                if image_data.startswith('data:image'):
                    image_data = image_data.split(',')[1]
                    image_bytes = base64.b64decode(image_data)
                    return Image.open(BytesIO(image_bytes))
                # Se for URL
                elif image_data.startswith('http'):
                    response = requests.get(image_data)
                    return Image.open(BytesIO(response.content))
                # Se for path
                elif os.path.exists(image_data):
                    return Image.open(image_data)
            
            elif isinstance(image_data, bytes):
                return Image.open(BytesIO(image_data))
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar imagem: {e}")
            return None
    
    def _process_audio_input(self, audio_data: Union[str, bytes]) -> Optional[str]:
        """Processar dados de entrada de √°udio"""
        try:
            # Criar arquivo tempor√°rio
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
            
            if isinstance(audio_data, str):
                # Se for base64
                if audio_data.startswith('data:audio'):
                    audio_data = audio_data.split(',')[1]
                    audio_bytes = base64.b64decode(audio_data)
                    temp_file.write(audio_bytes)
                # Se for URL
                elif audio_data.startswith('http'):
                    response = requests.get(audio_data)
                    temp_file.write(response.content)
                # Se for path
                elif os.path.exists(audio_data):
                    with open(audio_data, 'rb') as f:
                        temp_file.write(f.read())
            
            elif isinstance(audio_data, bytes):
                temp_file.write(audio_data)
            
            temp_file.close()
            return temp_file.name
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar √°udio: {e}")
            return None
    
    def _fallback_image_response(self, prompt: str, language: str) -> Dict[str, Any]:
        """Resposta de fallback para an√°lise de imagem"""
        return {
            "success": False,
            "analysis": "Servi√ßo de an√°lise de imagem temporariamente indispon√≠vel. O modelo Gemma 3n multimodal n√£o est√° carregado.",
            "method": "fallback",
            "image_processed": False,
            "language": language
        }
    
    def _fallback_audio_response(self, prompt: str, language: str) -> Dict[str, Any]:
        """Resposta de fallback para processamento de √°udio"""
        return {
            "success": False,
            "transcription": "Servi√ßo de processamento de √°udio temporariamente indispon√≠vel. O modelo Gemma 3n multimodal n√£o est√° carregado.",
            "method": "fallback", 
            "audio_processed": False,
            "language": language
        }
    
    def get_capabilities(self) -> Dict[str, Any]:
        """Obter capacidades multimodais dispon√≠veis (otimizado)"""
        try:
            # Verifica√ß√£o r√°pida sem opera√ß√µes custosas
            has_processor = hasattr(self, 'processor') and self.processor is not None
            has_model = hasattr(self, 'model') and self.model is not None  
            has_pipeline = hasattr(self, 'image_text_pipeline') and self.image_text_pipeline is not None
            
            return {
                "text_generation": True,  # Sempre dispon√≠vel via servi√ßo principal
                "image_analysis": self.initialized and (has_pipeline or (has_processor and has_model)),
                "audio_processing": self.initialized and has_processor and has_model,
                "video_processing": False,  # Implementar futuramente
                "supported_image_formats": ["JPEG", "PNG", "WEBP", "BMP"],
                "supported_audio_formats": ["WAV", "MP3", "M4A"],
                "max_image_size": "768x768",
                "max_audio_duration": "60 seconds",
                "context_length": "32K tokens",
                "initialized": self.initialized,
                "components_loaded": {
                    "processor": has_processor,
                    "model": has_model,
                    "pipeline": has_pipeline
                }
            }
        except Exception as e:
            logger.error(f"‚ùå Erro ao obter capacidades: {e}")
            # Retornar resposta de fallback mesmo com erro
            return {
                "text_generation": True,
                "image_analysis": False,
                "audio_processing": False,
                "video_processing": False,
                "supported_image_formats": [],
                "supported_audio_formats": [],
                "max_image_size": "N/A",
                "max_audio_duration": "N/A",
                "context_length": "N/A",
                "initialized": False,
                "error": str(e)
            }
