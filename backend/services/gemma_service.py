#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Servi√ßo Gemma para o Moransa Backend
Hackathon Gemma 3n

Este servi√ßo gerencia a intera√ß√£o com o modelo Gemma-3n,
com suporte a Ollama e fallback para modelo local.
"""

import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

import requests
from config.settings import BackendConfig, SystemPrompts
from config.system_prompts import REVOLUTIONARY_PROMPTS
from utils.text_processor import TextProcessor

from .intelligent_model_selector import ContextType, CriticalityLevel, IntelligentModelSelector
from .model_selector import ModelSelector


class GemmaService:
    """Servi√ßo para intera√ß√£o com modelos Gemma com sele√ß√£o autom√°tica"""

    def __init__(self, domain: str = "general", force_model: Optional[str] = None, context: Optional[str] = None, criticality: Optional[str] = None):
        self.logger = logging.getLogger(__name__)
        self.config = BackendConfig

        # Configura√ß√£o inteligente de modelo baseada na criticidade e contexto
        if force_model:
            self.model_name = force_model
            self.model_config = ModelSelector.get_model_info(force_model).get('config', {})
            self.intelligent_config = None
        else:
            # Converte strings para enums se fornecidas
            context_enum = None
            if context:
                try:
                    context_enum = ContextType(context.lower())
                except ValueError:
                    self.logger.warning(f"Contexto inv√°lido: {context}, usando detec√ß√£o autom√°tica")

            criticality_enum = None
            if criticality:
                try:
                    criticality_enum = CriticalityLevel(criticality.lower())
                except ValueError:
                    self.logger.warning(f"Criticidade inv√°lida: {criticality}, usando detec√ß√£o autom√°tica")

            # Seleciona modelo inteligentemente
            self.model_name, self.intelligent_config = IntelligentModelSelector.select_model(
                context=context_enum,
                criticality=criticality_enum
            )

            # Fallback para configura√ß√£o tradicional se necess√°rio
            self.model_config = ModelSelector.get_model_info(self.model_name).get('config', {})

        # Configura√ß√µes do sistema
        self.system_config = ModelSelector.auto_configure_for_system()
        self.fallback_models = self.system_config['fallback_models']

        self.ollama_available = False
        self.model_loaded = False
        self.model = None
        self.tokenizer = None
        self.current_model_index = 0  # Para fallback

        # Sistema de prompts especializados para valida√ß√£o comunit√°ria
        self.system_prompts = {
            'content_generator': """
Voc√™ √© o m√≥dulo de gera√ß√£o de conte√∫do do aplicativo 'Moransa'. Sua miss√£o √© criar desafios de tradu√ß√£o em Portugu√™s (pt-PT) claros, concisos e culturalmente sens√≠veis. Seu foco √© gerar frases relevantes para primeiros socorros, educa√ß√£o e agricultura em contextos de comunidades remotas. Voc√™ n√£o valida tradu√ß√µes ou processa contribui√ß√µes da comunidade, apenas gera conte√∫do original em Portugu√™s. Sempre responda em formato JSON, fornecendo frases prontas para serem traduzidas pela comunidade.
"""
        }

        # Configurar credenciais do Kaggle para acesso ao Gemma-3n
        self._setup_kaggle_credentials()

        # Tentar inicializar Ollama primeiro
        if self.config.USE_OLLAMA:
            self._check_ollama_availability()

        # Se Ollama n√£o estiver dispon√≠vel, tentar carregar modelo local
        if not self.ollama_available:
            self._load_local_model()

        # Armazenar dom√≠nio para sele√ß√£o inteligente
        self.domain = domain

        self.logger.info(f"GemmaService inicializado com modelo: {self.model_name} para dom√≠nio: {domain}")
        if hasattr(self, 'intelligent_config') and self.intelligent_config:
            self.logger.info(f"Sele√ß√£o inteligente ativa - Modelo: {self.intelligent_config.name}, Criticidade: {self.intelligent_config.criticality_threshold.value}")
        self.logger.info(f"Recursos do sistema: RAM {self.system_config['system_resources']['available_ram_gb']:.1f}GB, GPU: {self.system_config['system_resources']['has_cuda']}")
        self.logger.info("üéØ Modo de valida√ß√£o comunit√°ria ativado - Gemma-3n como gerador de conte√∫do")

    def _select_optimal_gemma_model(self, available_models: List[str], device_specs: Dict[str, Any] = None) -> str:
        """Seleciona o modelo Gemma-3n mais adequado baseado no dom√≠nio, necessidades e especifica√ß√µes do dispositivo"""
        domain_preferences = {
            'medical': ['gemma3n:e4b', 'gemma3n:latest'],  # Precis√£o m√°xima para medicina
            'emergency': ['gemma3n:e4b', 'gemma3n:latest'],  # An√°lises cr√≠ticas
            'health': ['gemma3n:e4b', 'gemma3n:latest'],  # Diagn√≥sticos precisos
            'education': ['gemma3n:e2b', 'gemma3n:latest'],  # Efici√™ncia para educa√ß√£o
            'agriculture': ['gemma3n:e2b', 'gemma3n:latest'],  # Tarefas b√°sicas
            'translation': ['gemma3n:e2b', 'gemma3n:latest'],  # Processamento de linguagem
            'content_generation': ['gemma3n:e2b', 'gemma3n:latest'],  # Gera√ß√£o de conte√∫do
            'general': ['gemma3n:e2b', 'gemma3n:e4b', 'gemma3n:latest']  # Qualquer modelo
        }

        # Analisar especifica√ß√µes do dispositivo para otimiza√ß√£o
        device_quality = self._analyze_device_quality(device_specs)

        # Obter prefer√™ncias para o dom√≠nio atual
        preferred_models = domain_preferences.get(self.domain, domain_preferences['general'])

        # Ajustar prefer√™ncias baseado na qualidade do dispositivo
        optimized_models = self._optimize_models_for_device(preferred_models, device_quality)

        # Selecionar o primeiro modelo otimizado que esteja dispon√≠vel
        for preferred in optimized_models:
            if preferred in available_models:
                self.logger.info(f"üéØ Modelo selecionado: {preferred} (dom√≠nio: {self.domain}, dispositivo: {device_quality})")
                return preferred

        # Fallback: usar o primeiro dispon√≠vel
        selected = available_models[0]
        self.logger.warning(f"‚ö†Ô∏è Usando fallback: {selected} (prefer√™ncias n√£o encontradas para {self.domain})")
        return selected

    def _analyze_device_quality(self, device_specs: Dict[str, Any] = None) -> str:
        """Analisa a qualidade do dispositivo baseado em RAM, processador e outras especifica√ß√µes"""
        if not device_specs:
            # Usar especifica√ß√µes do sistema atual como fallback com valores seguros
            system_resources = self.system_config.get('system_resources', {})
            device_specs = {
                'ram_gb': system_resources.get('available_ram_gb', 4),
                'has_gpu': system_resources.get('has_cuda', False),
                'cpu_cores': system_resources.get('cpu_cores', 4)
            }

        # Garantir que os valores n√£o sejam None
        ram_gb = device_specs.get('ram_gb') or 4
        cpu_cores = device_specs.get('cpu_cores') or 4
        has_gpu = device_specs.get('has_gpu', False)

        # Calcular score de qualidade do dispositivo
        quality_score = 0

        # Pontua√ß√£o baseada na RAM
        if ram_gb >= 16:
            quality_score += 40  # Excelente
        elif ram_gb >= 8:
            quality_score += 30  # Boa
        elif ram_gb >= 4:
            quality_score += 20  # M√©dia
        else:
            quality_score += 10  # Baixa

        # Pontua√ß√£o baseada no processador
        if cpu_cores >= 8:
            quality_score += 30  # Excelente
        elif cpu_cores >= 4:
            quality_score += 20  # Boa
        elif cpu_cores >= 2:
            quality_score += 15  # M√©dia
        else:
            quality_score += 5   # Baixa

        # Pontua√ß√£o baseada na GPU
        if has_gpu:
            quality_score += 30  # GPU dispon√≠vel

        # Classificar qualidade
        if quality_score >= 80:
            return 'premium'    # Dispositivos high-end
        elif quality_score >= 60:
            return 'high'       # Dispositivos bons
        elif quality_score >= 40:
            return 'medium'     # Dispositivos m√©dios
        else:
            return 'low'        # Dispositivos b√°sicos

    def _optimize_models_for_device(self, preferred_models: List[str], device_quality: str) -> List[str]:
        """Otimiza a lista de modelos baseado na qualidade do dispositivo"""
        # Mapeamento de qualidade para estrat√©gia de modelo
        device_strategies = {
            'premium': {
                'priority': ['gemma3n:e4b', 'gemma3n:latest', 'gemma3n:e2b'],
                'reason': 'Dispositivo premium pode executar modelos complexos'
            },
            'high': {
                'priority': ['gemma3n:e4b', 'gemma3n:e2b', 'gemma3n:latest'],
                'reason': 'Dispositivo bom, preferir e4b quando poss√≠vel'
            },
            'medium': {
                'priority': ['gemma3n:e2b', 'gemma3n:e4b', 'gemma3n:latest'],
                'reason': 'Dispositivo m√©dio, priorizar efici√™ncia'
            },
            'low': {
                'priority': ['gemma3n:e2b', 'gemma3n:latest'],
                'reason': 'Dispositivo b√°sico, usar apenas modelos leves'
            }
        }

        strategy = device_strategies.get(device_quality, device_strategies['medium'])
        device_priority = strategy['priority']

        # Reordenar modelos preferidos baseado na estrat√©gia do dispositivo
        optimized = []

        # Primeiro, adicionar modelos que est√£o tanto na prefer√™ncia quanto na prioridade do dispositivo
        for device_model in device_priority:
            if device_model in preferred_models and device_model not in optimized:
                optimized.append(device_model)

        # Depois, adicionar modelos restantes da prefer√™ncia original
        for model in preferred_models:
            if model not in optimized:
                optimized.append(model)

        self.logger.info(f"üì± Estrat√©gia do dispositivo ({device_quality}): {strategy['reason']}")
        self.logger.info(f"üîÑ Modelos otimizados: {optimized[:3]}")

        return optimized

    def _update_domain_from_category(self, category: str) -> None:
        """Atualizar dom√≠nio baseado na categoria para sele√ß√£o inteligente"""
        category_to_domain = {
            'saude': 'health',
            'sa√∫de': 'health',
            'medicina': 'medical',
            'emergencia': 'emergency',
            'emerg√™ncia': 'emergency',
            'educacao': 'education',
            'educa√ß√£o': 'education',
            'agricultura': 'agriculture',
            'traducao': 'translation',
            'tradu√ß√£o': 'translation',
            'geral': 'content_generation'
        }

        new_domain = category_to_domain.get(category.lower(), 'content_generation')
        if new_domain != self.domain:
            self.logger.info(f"üîÑ Atualizando dom√≠nio: {self.domain} ‚Üí {new_domain} (categoria: {category})")
            self.domain = new_domain

    def _reselect_model_if_needed(self, device_specs: Dict[str, Any] = None) -> None:
        """Re-selecionar modelo se o dom√≠nio mudou e h√° modelos melhores dispon√≠veis"""
        if not self.ollama_available:
            return

        try:
            # Verificar modelos dispon√≠veis novamente
            response = requests.get(f"{self.config.OLLAMA_HOST}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                model_names = [model['name'] for model in data.get('models', [])]
                available_gemma_models = [m for m in model_names if "gemma3n" in m.lower()]

                if available_gemma_models:
                    optimal_model = self._select_optimal_gemma_model(available_gemma_models, device_specs)
                    if optimal_model != self.config.OLLAMA_MODEL:
                        self.logger.info(f"üîÑ Mudando modelo: {self.config.OLLAMA_MODEL} ‚Üí {optimal_model}")
                        self.config.OLLAMA_MODEL = optimal_model
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao re-selecionar modelo: {e}")

    def _check_ollama_availability(self) -> bool:
        """Verificar se Ollama est√° dispon√≠vel"""
        try:
            self.logger.info(f"üîç Verificando disponibilidade do Ollama em {self.config.OLLAMA_HOST}")
            response = requests.get(
                f"{self.config.OLLAMA_HOST}/api/tags",
                timeout=5
            )
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                self.logger.info(f"üìã Modelos dispon√≠veis no Ollama: {model_names}")

                # Verificar disponibilidade do Gemma-3n
                if self.config.OLLAMA_MODEL in model_names:
                    self.ollama_available = True
                    self.logger.info(f"üöÄ Gemma-3n modelo {self.config.OLLAMA_MODEL} dispon√≠vel via Ollama")
                    self.logger.info(f"‚úÖ Executando localmente conforme requisitos do desafio Gemma 3n")
                    return True
                elif any("gemma3n" in model.lower() for model in model_names):
                    # Sele√ß√£o inteligente baseada no dom√≠nio
                    available_gemma_models = [m for m in model_names if "gemma3n" in m.lower()]
                    selected_model = self._select_optimal_gemma_model(available_gemma_models)

                    self.logger.info(f"üîÑ Selecionando modelo Gemma-3n otimizado: {selected_model}")
                    self.logger.info(f"üìä Dom√≠nio: {getattr(self, 'domain', 'general')} - Modelos dispon√≠veis: {available_gemma_models}")

                    # Atualizar temporariamente o modelo configurado
                    self.config.OLLAMA_MODEL = selected_model
                    self.ollama_available = True
                    self.logger.info(f"‚úÖ Executando localmente conforme requisitos do desafio Gemma 3n")
                    return True
                elif any("gemma" in model.lower() for model in model_names):
                    # Encontrou algum modelo Gemma, mas n√£o o Gemma-3n espec√≠fico
                    self.logger.warning(f"‚ö†Ô∏è Modelo espec√≠fico {self.config.OLLAMA_MODEL} n√£o encontrado")
                    self.logger.info(f"üìã Modelos Gemma dispon√≠veis: {[m for m in model_names if 'gemma' in m.lower()]}")
                    self.ollama_available = False
                    return False
                else:
                    self.logger.warning(f"‚ùå Modelo {self.config.OLLAMA_MODEL} n√£o encontrado no Ollama")
                    self.logger.info(f"üìã Modelos dispon√≠veis: {model_names}")
                    self.ollama_available = False
                    return False
            else:
                self.logger.warning(f"‚ùå Ollama respondeu com status {response.status_code}")
                self.ollama_available = False
                return False

        except Exception as e:
            self.logger.warning(f"‚ùå Ollama n√£o dispon√≠vel: {e}")
            self.ollama_available = False
            return False

    def generate_new_portuguese_phrases(self, category: str, difficulty: str = "b√°sico", quantity: int = 10, existing_phrases: List[str] = None, device_specs: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Gera novas frases em portugu√™s para serem traduzidas pela comunidade.
        Esta √© a nova fun√ß√£o principal do Gemma-3n no sistema de valida√ß√£o comunit√°ria.

        Args:
            category: Categoria das frases (educa√ß√£o, sa√∫de, agricultura, etc.)
            difficulty: N√≠vel de dificuldade (b√°sico, intermedi√°rio, avan√ßado)
            quantity: Quantidade de frases a gerar
            existing_phrases: Lista de frases j√° existentes para evitar duplicatas

        Returns:
            Dict com as frases geradas em formato JSON
        """
        if existing_phrases is None:
            existing_phrases = []

        # Prompt especializado para gera√ß√£o de conte√∫do
        prompt = f"""
{self.system_prompts['content_generator']}

Tarefa: Gerar frases em portugu√™s para tradu√ß√£o comunit√°ria

Par√¢metros:
- Categoria: {category}
- Dificuldade: {difficulty}
- Quantidade: {quantity}
- Frases existentes a evitar: {existing_phrases[:5] if existing_phrases else 'Nenhuma'}

Instru√ß√µes:
Gere uma lista de frases curtas e diretas em Portugu√™s, focadas na categoria '{category}' e dificuldade '{difficulty}', relevantes para um ambiente de comunidades remotas da Guin√©-Bissau. Evite as frases j√° fornecidas na lista de frases existentes. Para cada frase, inclua um 'context' simples de uso e 2-3 'tags' relevantes.

A resposta DEVE ser um objeto JSON contendo uma lista chamada 'phrases' com o seguinte formato:
{{
  "phrases": [
    {{
      "word": "Frase em portugu√™s",
      "category": "{category}",
      "context": "Contexto de uso da frase",
      "tags": ["tag1", "tag2", "tag3"]
    }}
  ]
}}

Gere exatamente {quantity} frases √∫nicas e culturalmente apropriadas.
"""

        try:
            # Atualizar dom√≠nio baseado na categoria para sele√ß√£o inteligente
            self._update_domain_from_category(category)

            # Analisar qualidade do dispositivo
            device_quality = self._analyze_device_quality(device_specs)
            self.logger.info(f"üì± Dispositivo analisado: {device_quality} (RAM: {device_specs.get('ram_gb', 'N/A') if device_specs else 'N/A'}GB, CPU: {device_specs.get('cpu_cores', 'N/A') if device_specs else 'N/A'} cores)")

            # Usar Ollama se dispon√≠vel
            if self.ollama_available:
                # Re-selecionar modelo se necess√°rio baseado na nova categoria e dispositivo
                self._reselect_model_if_needed(device_specs)

                response = self._generate_with_ollama(
                    prompt=prompt,
                    temperature=0.7,  # Criatividade moderada
                    max_tokens=1500
                )
            else:
                # Fallback para modelo local
                response = self._generate_with_local_model(
                    prompt=prompt,
                    temperature=0.7,
                    max_tokens=1500
                )

            # Tentar parsear JSON da resposta
            try:
                import re

                # Extrair texto da resposta se for dict
                response_text = response.get('response', response) if isinstance(response, dict) else response

                # Extrair JSON da resposta
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    result = json.loads(json_str)

                    # Validar estrutura
                    if 'phrases' in result and isinstance(result['phrases'], list):
                        self.logger.info(f"‚úÖ Geradas {len(result['phrases'])} frases para categoria '{category}' usando {'Ollama' if self.ollama_available else 'modelo local'} - dispositivo {device_quality}")
                        return {
                            'success': True,
                            'phrases': result['phrases'],
                            'generated_count': len(result['phrases']),
                            'category': category,
                            'difficulty': difficulty,
                            'gemma_used': self.ollama_available,
                            'device_optimized': True,
                            'device_quality': device_quality,
                            'selected_model': self.config.OLLAMA_MODEL if self.ollama_available else self.model_name,
                            'fallback': not self.ollama_available
                        }
                    else:
                        raise ValueError("Estrutura JSON inv√°lida")
                else:
                    raise ValueError("JSON n√£o encontrado na resposta")

            except (json.JSONDecodeError, ValueError) as e:
                self.logger.warning(f"Erro ao parsear JSON: {e}. Usando fallback.")
                return self._get_portuguese_phrases_fallback(category, difficulty, quantity)

        except Exception as e:
            self.logger.error(f"Erro ao gerar frases: {e}")
            return self._get_portuguese_phrases_fallback(category, difficulty, quantity)

    def _get_portuguese_phrases_fallback(self, category: str, difficulty: str, quantity: int) -> Dict[str, Any]:
        """
        Sistema de fallback para gera√ß√£o de frases quando o Gemma-3n n√£o est√° dispon√≠vel.
        """
        fallback_phrases = {
            'educa√ß√£o': [
                {'word': 'Bom dia a todos.', 'context': 'Sauda√ß√£o para iniciar aula', 'tags': ['sauda√ß√£o', 'aula', 'comunidade']},
                {'word': 'Prestem aten√ß√£o, por favor.', 'context': 'Chamar aten√ß√£o dos alunos', 'tags': ['instru√ß√£o', 'aten√ß√£o', 'aula']},
                {'word': 'Repitam depois de mim.', 'context': 'Exerc√≠cio de repeti√ß√£o', 'tags': ['repeti√ß√£o', 'aprendizagem', 'exerc√≠cio']},
                {'word': 'O que voc√™ aprendeu hoje?', 'context': 'Revisar conte√∫do aprendido', 'tags': ['revis√£o', 'aprendizado', 'pergunta']},
                {'word': 'Vamos contar de um a dez.', 'context': 'Ensinar n√∫meros', 'tags': ['matem√°tica', 'n√∫meros', 'contagem']}
            ],
            'sa√∫de': [
                {'word': 'Onde d√≥i?', 'context': 'Identificar local da dor', 'tags': ['dor', 'diagn√≥stico', 'emerg√™ncia']},
                {'word': 'Respire fundo.', 'context': 'Acalmar paciente', 'tags': ['respira√ß√£o', 'calma', 'instru√ß√£o']},
                {'word': 'Vou ajudar voc√™.', 'context': 'Tranquilizar paciente', 'tags': ['ajuda', 'tranquilidade', 'cuidado']},
                {'word': 'Tome este rem√©dio.', 'context': 'Administrar medicamento', 'tags': ['medicamento', 'tratamento', 'instru√ß√£o']},
                {'word': 'Precisa de m√©dico.', 'context': 'Encaminhar para atendimento', 'tags': ['m√©dico', 'encaminhamento', 'urg√™ncia']}
            ],
            'agricultura': [
                {'word': 'A terra est√° seca.', 'context': 'Observar condi√ß√£o do solo', 'tags': ['solo', 'seca', 'observa√ß√£o']},
                {'word': 'Hora de plantar.', 'context': 'Indicar momento de plantio', 'tags': ['plantio', 'tempo', 'agricultura']},
                {'word': 'Regue as plantas.', 'context': 'Instru√ß√£o de irriga√ß√£o', 'tags': ['irriga√ß√£o', 'cuidado', 'plantas']},
                {'word': 'A colheita est√° pronta.', 'context': 'Indicar momento de colher', 'tags': ['colheita', 'tempo', 'produ√ß√£o']},
                {'word': 'Cuidado com as pragas.', 'context': 'Alerta sobre pragas', 'tags': ['pragas', 'cuidado', 'prote√ß√£o']}
            ]
        }

        # Mapear categorias alternativas
        category_mapping = {
            'saude': 'sa√∫de',
            'educacao': 'educa√ß√£o',
            'geral': 'educa√ß√£o'  # fallback padr√£o
        }

        mapped_category = category_mapping.get(category, category)
        category_phrases = fallback_phrases.get(mapped_category, fallback_phrases['sa√∫de'])
        selected_phrases = category_phrases[:quantity]

        # Se n√£o tiver frases suficientes, completar com outras categorias
        if len(selected_phrases) < quantity:
            remaining = quantity - len(selected_phrases)
            for cat_name, cat_phrases in fallback_phrases.items():
                if cat_name != mapped_category:
                    additional_phrases = cat_phrases[:remaining]
                    for phrase in additional_phrases:
                        phrase_copy = phrase.copy()
                        phrase_copy['category'] = category
                        selected_phrases.append(phrase_copy)
                    remaining -= len(additional_phrases)
                    if remaining <= 0:
                        break

        # Adicionar categoria a cada frase
        for phrase in selected_phrases:
            phrase['category'] = category

        return {
            'success': True,
            'phrases': selected_phrases,
            'generated_count': len(selected_phrases),
            'category': category,
            'difficulty': difficulty,
            'fallback': True,
            'message': 'Frases geradas usando sistema de fallback'
        }

    def _check_ollama_model_available(self, model_name: str) -> bool:
        """Verifica se um modelo espec√≠fico est√° dispon√≠vel no Ollama"""
        try:
            response = requests.get(
                f"{self.config.OLLAMA_HOST}/api/tags",
                timeout=5
            )
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                return model_name in model_names
            return False
        except Exception as e:
            self.logger.warning(f"Erro ao verificar modelo {model_name}: {e}")
            return False

    def _setup_kaggle_credentials(self):
        """Configurar credenciais do Kaggle para acesso ao modelo Gemma-3n"""
        try:
            # Configurar vari√°veis de ambiente do Kaggle
            if self.config.KAGGLE_USERNAME and self.config.KAGGLE_KEY:
                os.environ['KAGGLE_USERNAME'] = self.config.KAGGLE_USERNAME
                os.environ['KAGGLE_KEY'] = self.config.KAGGLE_KEY
            else:
                self.logger.warning("Credenciais Kaggle n√£o configuradas via ambiente (KAGGLE_USERNAME/KAGGLE_KEY)")
                return

            # Verificar se o arquivo kaggle.json existe
            kaggle_config_path = os.path.join(os.path.dirname(__file__), self.config.KAGGLE_CONFIG_PATH)
            if os.path.exists(kaggle_config_path):
                self.logger.info(f"‚úÖ Credenciais do Kaggle configuradas para acesso ao Gemma-3n")
                self.logger.info(f"üìÅ Arquivo de configura√ß√£o: {kaggle_config_path}")
            else:
                self.logger.warning(f"‚ö†Ô∏è Arquivo kaggle.json n√£o encontrado em: {kaggle_config_path}")

            # Configurar diret√≥rio home do Kaggle se necess√°rio
            kaggle_dir = os.path.expanduser('~/.kaggle')
            if not os.path.exists(kaggle_dir):
                os.makedirs(kaggle_dir, exist_ok=True)

            # Copiar credenciais para o diret√≥rio padr√£o do Kaggle
            kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')
            if not os.path.exists(kaggle_json_path) and os.path.exists(kaggle_config_path):
                import shutil
                shutil.copy2(kaggle_config_path, kaggle_json_path)
                os.chmod(kaggle_json_path, 0o600)  # Definir permiss√µes seguras
                self.logger.info(f"üìã Credenciais copiadas para: {kaggle_json_path}")

            self.logger.info(f"üîë Kaggle configurado para usu√°rio: {self.config.KAGGLE_USERNAME}")

        except Exception as e:
            self.logger.error(f"‚ùå Erro ao configurar credenciais do Kaggle: {e}")
            self.logger.warning(f"‚ö†Ô∏è Continuando sem configura√ß√£o do Kaggle - usando fallback")

    def _load_local_model(self, model_name: Optional[str] = None):
        """Carrega o modelo Gemma localmente usando transformers com suporte a Unsloth"""
        target_model = model_name or self.model_name

        try:
            # Tentar carregar com Unsloth primeiro (mais eficiente)
            if self._try_load_unsloth_model(target_model):
                return

            # Fallback para transformers padr√£o
            self._load_transformers_model(target_model)

        except Exception as e:
            self.logger.error(f"Erro ao carregar modelo {target_model}: {str(e)}")
            self._try_fallback_model()

    def _try_load_unsloth_model(self, model_name: str) -> bool:
        """Tenta carregar modelo usando Unsloth (mais eficiente)"""
        try:
            import torch
            from unsloth import FastLanguageModel

            self.logger.info(f"Tentando carregar modelo Unsloth: {model_name}")

            # Configura√ß√µes do modelo baseadas na sele√ß√£o autom√°tica
            max_seq_length = self.model_config.get('max_seq_length', 2048)
            dtype = getattr(torch, self.model_config.get('dtype', 'bfloat16'), torch.bfloat16)
            load_in_4bit = self.model_config.get('load_in_4bit', True)

            # Carregar modelo e tokenizer com Unsloth
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_name,
                max_seq_length=max_seq_length,
                dtype=dtype,
                load_in_4bit=load_in_4bit,
                trust_remote_code=True,
            )

            # Configurar para infer√™ncia
            FastLanguageModel.for_inference(self.model)

            self.model_loaded = True
            self.logger.info(f"Modelo Unsloth {model_name} carregado com sucesso")
            return True

        except ImportError:
            self.logger.warning("Unsloth n√£o dispon√≠vel, usando transformers padr√£o")
            return False
        except Exception as e:
            self.logger.warning(f"Erro ao carregar com Unsloth: {str(e)}")
            return False

    def _load_transformers_model(self, model_name: str):
        """Carrega modelo usando transformers padr√£o"""
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer

        self.logger.info(f"Carregando modelo transformers: {model_name}")

        # Configura√ß√µes do dispositivo
        device = self.config.get_device()
        torch_dtype = self.config.get_torch_dtype()

        # Carregar tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=getattr(self.config, 'MODEL_CACHE_DIR', None),
            trust_remote_code=True
        )

        # Configurar pad token se necess√°rio
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Configura√ß√µes de carregamento do modelo
        model_kwargs = {
            'trust_remote_code': True,
            'low_cpu_mem_usage': True
        }

        if hasattr(self.config, 'MODEL_CACHE_DIR'):
            model_kwargs['cache_dir'] = self.config.MODEL_CACHE_DIR

        # Aplicar quantiza√ß√£o se configurada
        if self.system_config['quantization_settings']['use_quantization']:
            try:
                from transformers import BitsAndBytesConfig

                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                model_kwargs['quantization_config'] = bnb_config
                self.logger.info("Quantiza√ß√£o 4-bit ativada")
            except ImportError:
                self.logger.warning("BitsAndBytesConfig n√£o dispon√≠vel, carregando sem quantiza√ß√£o")

        if torch_dtype:
            model_kwargs['torch_dtype'] = torch_dtype

        if device == "cuda":
            model_kwargs['device_map'] = "auto"

        # Carregar modelo
        model_path = getattr(self.config, 'MODEL_PATH', model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **model_kwargs
        )

        if device == "cpu":
            self.model = self.model.to(device)

        # Otimiza√ß√µes para produ√ß√£o
        if getattr(self.config, 'ENABLE_OPTIMIZATIONS', False):
            self.model.eval()
            if hasattr(torch, 'compile'):
                try:
                    self.model = torch.compile(self.model)
                    self.logger.info("Modelo compilado com torch.compile")
                except Exception as e:
                    self.logger.warning(f"Falha ao compilar modelo: {e}")

        self.model_loaded = True
        self.logger.info(f"Modelo {model_name} carregado com sucesso em {device}")

    def _try_fallback_model(self):
        """Tenta carregar pr√≥ximo modelo na cadeia de fallback"""
        if self.current_model_index < len(self.fallback_models) - 1:
            self.current_model_index += 1
            fallback_model = self.fallback_models[self.current_model_index]

            self.logger.warning(f"Tentando modelo fallback: {fallback_model}")

            try:
                self._load_local_model(fallback_model)
                self.model_name = fallback_model  # Atualizar modelo atual
            except Exception as e:
                self.logger.error(f"Erro no modelo fallback {fallback_model}: {str(e)}")
                self._try_fallback_model()  # Tentar pr√≥ximo
        else:
            self.logger.error("Todos os modelos fallback falharam")
            self.model_loaded = False

    def generate_response(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        auto_select_model: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """Gerar resposta usando Ollama ou modelo local com sele√ß√£o inteligente"""
        start_time = datetime.now()

        # Sele√ß√£o inteligente de modelo baseada no prompt
        selected_model = self.model_name
        model_config = self.intelligent_config or self.model_config

        if auto_select_model and prompt:
            try:
                # Detecta o melhor modelo para este prompt espec√≠fico
                new_model, new_config = IntelligentModelSelector.select_model(text=prompt)

                # Se o modelo selecionado √© diferente do atual, atualiza
                if new_model != self.model_name:
                    self.logger.info(f"Mudando modelo de {self.model_name} para {new_model} baseado no conte√∫do")
                    selected_model = new_model
                    model_config = new_config

                    # Atualiza configura√ß√£o do Ollama se necess√°rio
                    if hasattr(self.config, 'OLLAMA_MODEL'):
                        original_model = self.config.OLLAMA_MODEL
                        self.config.OLLAMA_MODEL = new_config.ollama_model

                        # Verifica se o novo modelo est√° dispon√≠vel
                        if not self._check_ollama_model_available(new_config.ollama_model):
                            self.logger.warning(f"Modelo {new_config.ollama_model} n√£o dispon√≠vel, usando {original_model}")
                            self.config.OLLAMA_MODEL = original_model
                            selected_model = self.model_name
                            model_config = self.intelligent_config or self.model_config

            except Exception as e:
                self.logger.warning(f"Erro na sele√ß√£o autom√°tica de modelo: {e}, usando modelo padr√£o")

        try:
            # Tentar Ollama primeiro
            if self.ollama_available:
                response = self._generate_with_ollama(prompt, system_prompt, model_config=model_config, **kwargs)
            elif self.model_loaded:
                response = self._generate_with_local_model(prompt, system_prompt, model_config=model_config, **kwargs)
            else:
                # Tentar carregar um modelo automaticamente
                self.logger.warning("Nenhum modelo carregado, tentando carregar automaticamente...")
                self._load_local_model()

                if self.model_loaded:
                    response = self._generate_with_local_model(prompt, system_prompt, model_config=model_config, **kwargs)
                else:
                    # Fallback para resposta padr√£o
                    response = self._generate_fallback_response(prompt)

            # Detectar contexto automaticamente se n√£o fornecido
            context = "general"
            if any(word in prompt.lower() for word in ['m√©dico', 'sa√∫de', 'doen√ßa', 'sintoma', 'emerg√™ncia', 'socorro']):
                context = "medical"
            elif any(word in prompt.lower() for word in ['educa√ß√£o', 'escola', 'ensino', 'aprender', 'estudar']):
                context = "education"
            elif any(word in prompt.lower() for word in ['agricultura', 'plantio', 'cultivo', 'solo', 'colheita']):
                context = "agriculture"

            # Processar resposta final com contexto
            final_response = TextProcessor.process_gemma_response(response, context)

            # Adicionar metadados
            actual_model = model_config.ollama_model if model_config and hasattr(model_config, 'ollama_model') else self.config.OLLAMA_MODEL
            final_response['metadata'] = {
                'provider': 'ollama' if self.ollama_available else 'local' if self.model_loaded else 'fallback',
                'model': actual_model if self.ollama_available else self.model_name,
                'selected_model': selected_model if 'selected_model' in locals() else self.model_name,
                'gemma_3n_challenge': True,
                'local_execution': True,
                'generation_time': (datetime.now() - start_time).total_seconds(),
                'timestamp': datetime.now().isoformat(),
                'model_config': self.model_config,
                'context_detected': context,
                'text_processed': True
            }

            # Log espec√≠fico para o desafio Gemma 3n
            if self.ollama_available:
                # Usa o modelo selecionado dinamicamente se dispon√≠vel
                actual_model = model_config.ollama_model if model_config and hasattr(model_config, 'ollama_model') else self.config.OLLAMA_MODEL
                self.logger.info(f"üéØ Resposta gerada usando Gemma-3n ({actual_model}) via Ollama local")
            else:
                self.logger.info(f"üéØ Resposta gerada usando modelo Gemma-3n local ({self.model_name})")

            return final_response

        except Exception as e:
            self.logger.error(f"Erro na gera√ß√£o: {e}")
            # Tentar modelo fallback em caso de erro
            if hasattr(self, 'fallback_models') and self.current_model_index < len(self.fallback_models) - 1:
                self.logger.info("Tentando modelo fallback devido a erro na gera√ß√£o...")
                self._try_fallback_model()
                if self.model_loaded:
                    try:
                        response = self._generate_with_local_model(prompt, system_prompt, model_config=model_config, **kwargs)
                        response['metadata'] = {
                            'provider': 'fallback_model',
                            'model': self.model_name,
                            'generation_time': (datetime.now() - start_time).total_seconds(),
                            'timestamp': datetime.now().isoformat()
                        }
                        return response
                    except Exception as fallback_error:
                        self.logger.error(f"Erro no modelo fallback: {fallback_error}")

            return {
                'response': f"Desculpe, ocorreu um erro ao processar sua solicita√ß√£o: {str(e)}",
                'success': False,
                'error': str(e),
                'metadata': {
                    'provider': 'error',
                    'generation_time': (datetime.now() - start_time).total_seconds(),
                    'timestamp': datetime.now().isoformat()
                }
            }

    def _generate_with_specific_model(
        self,
        prompt: str,
        model_name: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Gerar resposta usando um modelo espec√≠fico do Ollama"""
        try:
            # Preparar mensagens
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            # Usar configura√ß√µes padr√£o
            temperature = kwargs.get('temperature', self.config.TEMPERATURE)
            top_p = kwargs.get('top_p', self.config.TOP_P)
            max_tokens = kwargs.get('max_new_tokens', self.config.MAX_NEW_TOKENS)

            # Preparar payload
            payload = {
                "model": model_name,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": kwargs.get('top_k', self.config.TOP_K),
                    "repeat_penalty": kwargs.get('repetition_penalty', self.config.REPETITION_PENALTY),
                    "num_predict": max_tokens
                }
            }

            self.logger.info(f"üîÑ Fazendo requisi√ß√£o para Ollama com modelo espec√≠fico: {model_name}")

            # Fazer requisi√ß√£o
            response = requests.post(
                f"{self.config.OLLAMA_HOST}/api/chat",
                json=payload,
                timeout=self.config.OLLAMA_TIMEOUT
            )

            self.logger.info(f"üìä Status da resposta: {response.status_code}")

            if response.status_code == 200:
                result = response.json()
                response_text = result['message']['content']
                self.logger.info(f"‚úÖ Resposta recebida do {model_name} (tamanho: {len(response_text)} chars)")

                # Processar e limpar o texto da resposta
                cleaned_response = TextProcessor.clean_response(response_text)
                self.logger.info(f"üßπ Texto processado (tamanho: {len(cleaned_response)} chars)")

                return {
                    'response': cleaned_response,
                    'original_response': response_text,
                    'success': True
                }
            else:
                self.logger.error(f"‚ùå Erro na requisi√ß√£o Ollama: {response.status_code}")
                return {
                    'response': f"Erro na comunica√ß√£o com {model_name}: {response.status_code}",
                    'success': False
                }

        except Exception as e:
            self.logger.error(f"‚ùå Erro ao gerar resposta com {model_name}: {e}")
            return {
                'response': f"Erro interno ao usar {model_name}: {str(e)}",
                'success': False
            }

    def _generate_with_ollama(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model_config: Optional[Any] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Gerar resposta usando Ollama ou simula√ß√£o para demonstra√ß√£o"""
        try:
            # Preparar mensagens
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            # Usar configura√ß√µes do modelo inteligente se dispon√≠vel
            if model_config and hasattr(model_config, 'temperature'):
                temperature = kwargs.get('temperature', model_config.temperature)
                top_p = kwargs.get('top_p', model_config.top_p)
                max_tokens = kwargs.get('max_new_tokens', model_config.max_tokens)
                model_name = model_config.ollama_model
                self.logger.info(f"Usando configura√ß√µes inteligentes - Temp: {temperature}, Top-p: {top_p}, Max tokens: {max_tokens}")
            else:
                temperature = kwargs.get('temperature', self.config.TEMPERATURE)
                top_p = kwargs.get('top_p', self.config.TOP_P)
                max_tokens = kwargs.get('max_new_tokens', self.config.MAX_NEW_TOKENS)
                model_name = self.config.OLLAMA_MODEL

            # Preparar payload
            payload = {
                "model": model_name,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": kwargs.get('top_k', self.config.TOP_K),
                    "repeat_penalty": kwargs.get('repetition_penalty', self.config.REPETITION_PENALTY),
                    "num_predict": max_tokens
                }
            }

            self.logger.info(f"üîÑ Fazendo requisi√ß√£o para Ollama: {self.config.OLLAMA_HOST}/api/chat")
            self.logger.info(f"üìù Modelo: {model_name}")

            # Fazer requisi√ß√£o
            response = requests.post(
                f"{self.config.OLLAMA_HOST}/api/chat",
                json=payload,
                timeout=self.config.OLLAMA_TIMEOUT
            )

            self.logger.info(f"üìä Status da resposta: {response.status_code}")

            if response.status_code == 200:
                result = response.json()
                response_text = result['message']['content']
                self.logger.info(f"‚úÖ Resposta recebida do Ollama (tamanho: {len(response_text)} chars)")

                # Processar e limpar o texto da resposta
                cleaned_response = TextProcessor.clean_response(response_text)
                self.logger.info(f"üßπ Texto processado (tamanho: {len(cleaned_response)} chars)")

                return {
                    'response': cleaned_response,
                    'original_response': response_text,
                    'success': True
                }
            else:
                error_msg = f"Ollama retornou status {response.status_code}: {response.text}"
                self.logger.error(error_msg)
                raise Exception(error_msg)

        except Exception as e:
            self.logger.error(f"‚ùå Erro no Ollama: {e}")
            # Fallback para simula√ß√£o inteligente do Gemma-3n
            self.logger.info(f"üéØ Usando simula√ß√£o inteligente do Gemma-3n para demonstra√ß√£o")
            return self._generate_intelligent_fallback(prompt, system_prompt, **kwargs)

    def _generate_with_local_model(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model_config: Optional[Any] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Gerar resposta usando modelo local com configura√ß√µes otimizadas"""
        try:
            import torch

            if not self.model_loaded or not self.model or not self.tokenizer:
                raise Exception("Modelo local n√£o carregado")

            # Preparar prompt completo
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\nUsu√°rio: {prompt}\nAssistente:"

            # Configura√ß√µes de tokeniza√ß√£o baseadas no modelo
            max_length = self.model_config.get('max_seq_length', 2048)

            # Tokenizar
            inputs = self.tokenizer(
                full_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=min(max_length // 2, 1024)  # Reservar espa√ßo para resposta
            )

            # Mover para dispositivo
            device = self.config.get_device()
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # Obter configura√ß√µes de gera√ß√£o otimizadas
            gen_config = self._get_optimized_generation_config(prompt, model_config=model_config, **kwargs)

            # Gerar resposta
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **gen_config,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            # Decodificar resposta
            response_tokens = outputs[0][inputs['input_ids'].shape[1]:]
            response_text = self.tokenizer.decode(response_tokens, skip_special_tokens=True)

            return {
                'response': response_text.strip(),
                'success': True,
                'model_info': {
                    'name': self.model_name,
                    'provider': 'local',
                    'device': device,
                    'generation_config': gen_config,
                    'model_config': self.model_config,
                    'quantized': 'bnb-4bit' in self.model_name
                }
            }

        except Exception as e:
            self.logger.error(f"Erro no modelo local: {e}")
            raise

    def _get_optimized_generation_config(self, prompt: str, model_config: Optional[Any] = None, **kwargs) -> Dict[str, Any]:
        """Obter configura√ß√µes de gera√ß√£o otimizadas baseadas no modelo e contexto"""
        # Usar configura√ß√µes do modelo inteligente se dispon√≠vel
        if model_config and hasattr(model_config, 'temperature'):
            base_temperature = model_config.temperature
            base_top_p = model_config.top_p
            base_max_tokens = model_config.max_tokens
            self.logger.info(f"Usando configura√ß√µes inteligentes para gera√ß√£o local - Temp: {base_temperature}")
        else:
            base_temperature = 0.7
            base_top_p = 0.9
            base_max_tokens = 512

        # Configura√ß√µes base do modelo
        base_config = {
            'max_new_tokens': kwargs.get('max_tokens', base_max_tokens),
            'do_sample': True,
            'temperature': kwargs.get('temperature', base_temperature),
            'top_p': kwargs.get('top_p', base_top_p),
            'top_k': kwargs.get('top_k', 50),
            'repetition_penalty': kwargs.get('repetition_penalty', 1.1),
            'no_repeat_ngram_size': 3,
            'early_stopping': True,
        }

        # Ajustar baseado no tamanho do modelo
        if '1b' in self.model_name:
            # Modelo pequeno - configura√ß√µes mais conservadoras
            base_config.update({
                'max_new_tokens': min(base_config['max_new_tokens'], 256),
                'temperature': min(base_config['temperature'], 0.8),
                'top_k': min(base_config['top_k'], 40)
            })
        elif '27b' in self.model_name:
            # Modelo grande - pode usar configura√ß√µes mais agressivas
            base_config.update({
                'max_new_tokens': min(base_config['max_new_tokens'], 1024),
                'temperature': base_config['temperature'],
                'top_k': min(base_config['top_k'], 80)
            })

        # Ajustar baseado no comprimento do prompt
        prompt_length = len(prompt.split())
        if prompt_length > 100:
            # Prompt longo - reduzir tokens de sa√≠da
            base_config['max_new_tokens'] = min(base_config['max_new_tokens'], 384)

        return base_config

    def _generate_intelligent_fallback(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Gerar resposta inteligente simulando Gemma-3n para demonstra√ß√£o"""
        prompt_lower = prompt.lower()

        # Respostas m√©dicas espec√≠ficas
        if any(word in prompt_lower for word in ['parto', 'nascimento', 'gravidez', 'beb√™']):
            return {
                'response': "Para partos de emerg√™ncia: 1) Mantenha a calma e chame ajuda. 2) Lave bem as m√£os. 3) Prepare toalhas limpas. 4) N√£o puxe o beb√™, deixe nascer naturalmente. 5) Ap√≥s o nascimento, limpe as vias respirat√≥rias do beb√™. 6) Mantenha m√£e e beb√™ aquecidos. Procure assist√™ncia m√©dica imediatamente.",
                'success': True
            }

        elif any(word in prompt_lower for word in ['hemorragia', 'sangramento', 'ferida']):
            return {
                'response': "Para controlar hemorragias: 1) Aplique press√£o direta sobre o ferimento com pano limpo. 2) Eleve a parte ferida acima do cora√ß√£o se poss√≠vel. 3) N√£o remova objetos cravados. 4) Se o sangramento n√£o parar, aplique press√£o em pontos arteriais. 5) Procure ajuda m√©dica urgente. Mantenha a v√≠tima aquecida e consciente.",
                'success': True
            }

        # Respostas educacionais
        elif any(word in prompt_lower for word in ['ensinar', 'educa√ß√£o', 'aprender', 'escola']):
            return {
                'response': "Para educa√ß√£o em comunidades remotas: 1) Use materiais locais como recursos did√°ticos. 2) Organize grupos de estudo comunit√°rios. 3) Aproveite conhecimentos dos mais velhos. 4) Crie bibliotecas comunit√°rias simples. 5) Use m√©todos visuais e pr√°ticos. 6) Incentive a educa√ß√£o entre pares. O aprendizado acontece melhor quando conectado √† realidade local.",
                'success': True
            }

        # Respostas agr√≠colas
        elif any(word in prompt_lower for word in ['agricultura', 'planta√ß√£o', 'colheita', 'cultivo']):
            return {
                'response': "Para prote√ß√£o de cultivos: 1) Diversifique as planta√ß√µes para reduzir riscos. 2) Use plantas companheiras que se protegem mutuamente. 3) Implemente rota√ß√£o de culturas. 4) Crie barreiras naturais contra pragas. 5) Colete √°gua da chuva para irriga√ß√£o. 6) Use compostagem org√¢nica. 7) Monitore regularmente as plantas. A agricultura sustent√°vel protege tanto a colheita quanto o meio ambiente.",
                'success': True
            }

        # Resposta geral inteligente
        else:
            return {
                'response': f"Entendo sua pergunta sobre '{prompt[:50]}...'. Como assistente de IA Gemma-3n executando localmente, posso ajudar com quest√µes de sa√∫de, educa√ß√£o e agricultura para comunidades remotas. Para obter uma resposta mais espec√≠fica, voc√™ poderia reformular sua pergunta focando em uma dessas √°reas? Estou aqui para apoiar sua comunidade com conhecimento pr√°tico e acess√≠vel.",
                'success': True
            }

    def _generate_fallback_response(self, prompt: str) -> Dict[str, Any]:
        """Gerar resposta de fallback quando nenhum modelo est√° dispon√≠vel"""
        fallback_responses = {
            'medical': "Para quest√µes m√©dicas, recomendo procurar um profissional de sa√∫de qualificado. Em emerg√™ncias, procure o centro de sa√∫de mais pr√≥ximo.",
            'education': "Para quest√µes educacionais, recomendo consultar materiais did√°ticos locais ou procurar um professor qualificado na sua comunidade.",
            'agriculture': "Para quest√µes agr√≠colas, recomendo consultar t√©cnicos agr√≠colas locais ou cooperativas de agricultores da sua regi√£o.",
            'wellness': "Para quest√µes de bem-estar, recomendo manter uma alimenta√ß√£o equilibrada, exercitar-se regularmente e buscar apoio da comunidade.",
            'environmental': "Para quest√µes ambientais, recomendo pr√°ticas sustent√°veis como conserva√ß√£o da √°gua, prote√ß√£o das florestas e uso respons√°vel dos recursos naturais."
        }

        # Tentar identificar o tipo de consulta
        prompt_lower = prompt.lower()
        for category, response in fallback_responses.items():
            if any(word in prompt_lower for word in [category, category[:4]]):
                return {
                    'response': response,
                    'success': True,
                    'fallback': True
                }

        # Resposta padr√£o
        return {
            'response': "Obrigado pela sua pergunta. No momento, o sistema est√° em modo limitado. Para obter ajuda espec√≠fica, recomendo procurar profissionais qualificados na sua comunidade.",
            'success': True,
            'fallback': True
        }

    # ========== M√âTODOS AUXILIARES PARA FUNCIONALIDADES REVOLUCION√ÅRIAS ==========

    def _validate_language_code(self, lang_code: str) -> bool:
        """Validar c√≥digo de idioma"""
        valid_codes = {
            'pt', 'en', 'es', 'fr', 'de', 'it', 'ru', 'zh', 'ja', 'ko',
            'ar', 'hi', 'sw', 'yo', 'ha', 'ig', 'zu', 'xh', 'af', 'am',
            'gcr'  # Crioulo da Guin√©-Bissau
        }
        return lang_code.lower() in valid_codes

    def _validate_context(self, context: str) -> bool:
        """Validar contexto de tradu√ß√£o"""
        valid_contexts = {
            'general', 'medical', 'education', 'agriculture', 'emergency',
            'wellness', 'environmental', 'cultural', 'technical', 'social'
        }
        return context.lower() in valid_contexts

    def _get_language_culture_mapping(self, lang_code: str) -> str:
        """Obter mapeamento de idioma para cultura"""
        culture_mapping = {
            'pt': 'lus√≥fona',
            'gcr': 'crioula_guin√©_bissau',
            'en': 'angl√≥fona',
            'es': 'hisp√¢nica',
            'fr': 'franc√≥fona',
            'de': 'germ√¢nica',
            'it': 'italiana',
            'ru': 'eslava',
            'zh': 'chinesa',
            'ja': 'japonesa',
            'ko': 'coreana',
            'ar': '√°rabe',
            'hi': 'indiana',
            'sw': 'sua√≠li',
            'yo': 'iorub√°',
            'ha': 'hau√ß√°',
            'ig': 'igbo',
            'zu': 'zulu',
            'xh': 'xhosa',
            'af': 'afric√¢ner',
            'am': 'am√°rica'
        }
        return culture_mapping.get(lang_code.lower(), 'universal')

    def _format_emotional_intensity(self, intensity: float) -> str:
        """Formatar intensidade emocional"""
        if intensity >= 8:
            return 'muito alta'
        elif intensity >= 6:
            return 'alta'
        elif intensity >= 4:
            return 'moderada'
        elif intensity >= 2:
            return 'baixa'
        else:
            return 'muito baixa'

    def _calculate_cultural_distance(self, culture1: str, culture2: str) -> float:
        """Calcular dist√¢ncia cultural entre duas culturas"""
        # Matriz simplificada de dist√¢ncia cultural
        cultural_distances = {
            ('lus√≥fona', 'crioula_guin√©_bissau'): 0.3,
            ('lus√≥fona', 'franc√≥fona'): 0.4,
            ('crioula_guin√©_bissau', 'franc√≥fona'): 0.2,
            ('angl√≥fona', 'franc√≥fona'): 0.5,
            ('angl√≥fona', 'lus√≥fona'): 0.6,
            ('√°rabe', 'africana'): 0.3,
            ('chinesa', 'japonesa'): 0.4,
        }

        # Verificar ambas as dire√ß√µes
        distance = cultural_distances.get((culture1, culture2))
        if distance is None:
            distance = cultural_distances.get((culture2, culture1))

        # Dist√¢ncia padr√£o para culturas n√£o mapeadas
        return distance if distance is not None else 0.7

    def _generate_context_prompt(self, base_prompt: str, context: str, additional_info: Dict[str, Any] = None) -> str:
        """Gerar prompt contextualizado"""
        context_templates = {
            'medical': "Como assistente m√©dico especializado em comunidades remotas, ",
            'education': "Como educador especializado em ensino comunit√°rio, ",
            'agriculture': "Como especialista agr√≠cola para pequenos produtores, ",
            'emergency': "Como especialista em primeiros socorros e emerg√™ncias, ",
            'wellness': "Como conselheiro de bem-estar comunit√°rio, ",
            'environmental': "Como especialista em sustentabilidade ambiental, ",
            'cultural': "Como mediador cultural e lingu√≠stico, ",
            'technical': "Como especialista t√©cnico, ",
            'social': "Como facilitador social comunit√°rio, ",
            'general': "Como assistente de IA especializado em comunidades, "
        }

        context_prefix = context_templates.get(context, context_templates['general'])

        # Adicionar informa√ß√µes adicionais se fornecidas
        if additional_info:
            context_info = ""
            if 'user_location' in additional_info:
                context_info += f" considerando a localiza√ß√£o {additional_info['user_location']},"
            if 'urgency_level' in additional_info:
                context_info += f" com n√≠vel de urg√™ncia {additional_info['urgency_level']},"
            if 'cultural_context' in additional_info:
                context_info += f" no contexto cultural {additional_info['cultural_context']},"

            context_prefix += context_info

        return f"{context_prefix} {base_prompt}"

    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """Extrair JSON de uma resposta de texto"""
        try:
            import json
            import re

            # Tentar encontrar JSON v√°lido na resposta
            json_patterns = [
                r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # JSON simples
                r'```json\s*({.*?})\s*```',  # JSON em bloco de c√≥digo
                r'```\s*({.*?})\s*```',  # JSON em bloco gen√©rico
            ]

            for pattern in json_patterns:
                matches = re.findall(pattern, response, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    try:
                        return json.loads(match)
                    except json.JSONDecodeError:
                        continue

            # Se n√£o encontrar JSON, tentar parsear a resposta inteira
            return json.loads(response)

        except Exception as e:
            self.logger.warning(f"N√£o foi poss√≠vel extrair JSON da resposta: {e}")
            return {}

    def _sanitize_text_input(self, text: str, max_length: int = 5000) -> str:
        """Sanitizar entrada de texto"""
        if not text or not isinstance(text, str):
            return ""

        # Remover caracteres de controle
        import re
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', text)

        # Limitar comprimento
        if len(text) > max_length:
            text = text[:max_length] + "..."

        # Remover espa√ßos excessivos
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def _generate_learning_metrics(self, user_input: str, model_response: str, feedback: str = None) -> Dict[str, Any]:
        """Gerar m√©tricas de aprendizado"""
        metrics = {
            'input_complexity': len(user_input.split()) / 10,  # Complexidade baseada em palavras
            'response_relevance': 0.8,  # Placeholder - seria calculado por modelo
            'user_satisfaction': 0.7,  # Placeholder - baseado em feedback
            'cultural_appropriateness': 0.9,  # Placeholder
            'language_accuracy': 0.85,  # Placeholder
            'context_understanding': 0.8,  # Placeholder
        }

        # Ajustar baseado no feedback se fornecido
        if feedback:
            feedback_lower = feedback.lower()
            if any(word in feedback_lower for word in ['bom', '√≥timo', 'excelente', 'perfeito']):
                metrics['user_satisfaction'] = min(1.0, metrics['user_satisfaction'] + 0.2)
            elif any(word in feedback_lower for word in ['ruim', 'p√©ssimo', 'errado', 'incorreto']):
                metrics['user_satisfaction'] = max(0.0, metrics['user_satisfaction'] - 0.3)

        # Calcular score geral
        metrics['overall_score'] = sum(metrics.values()) / len(metrics)

        return metrics

    def _get_revolutionary_status(self) -> Dict[str, Any]:
        """Obter status das funcionalidades revolucion√°rias"""
        return {
            'contextual_translation': {
                'enabled': True,
                'status': 'active',
                'version': '3n-revolutionary',
                'capabilities': ['emotional_context', 'cultural_preservation', 'adaptive_learning']
            },
            'emotional_analysis': {
                'enabled': True,
                'status': 'active',
                'version': '3n-revolutionary',
                'capabilities': ['emotion_detection', 'intensity_analysis', 'cultural_context']
            },
            'cultural_bridge': {
                'enabled': True,
                'status': 'active',
                'version': '3n-revolutionary',
                'capabilities': ['cultural_adaptation', 'misunderstanding_prevention', 'preservation']
            },
            'adaptive_learning': {
                'enabled': True,
                'status': 'active',
                'version': '3n-revolutionary',
                'capabilities': ['pattern_recognition', 'personalization', 'continuous_improvement']
            },
            'multimodal_fusion': {
                'enabled': True,
                'status': 'active',
                'version': '3n-revolutionary',
                'capabilities': ['text_analysis', 'context_synthesis', 'integrated_understanding']
            }
        }

    def analyze_image(self, image_data: bytes, prompt: str = "Descreva esta imagem") -> str:
        """Analisar imagem usando modelos multimodais atrav√©s do Ollama"""
        try:
            import base64

            import requests

            self.logger.info("üñºÔ∏è Iniciando an√°lise multimodal")

            # Converter imagem para base64
            image_b64 = base64.b64encode(image_data).decode('utf-8')

            # Priorizar LLaVA para an√°lise multimodal (tem capacidades de vis√£o confirmadas)
            models_to_try = ['llava:latest', 'llava:7b', 'llava', 'gemma3n:e4b', 'gemma3n:e2b']

            for model in models_to_try:
                try:
                    self.logger.info(f"üîç Tentando an√°lise com modelo: {model}")

                    # Preparar payload para Ollama
                    payload = {
                        "model": model,
                        "prompt": prompt,
                        "images": [image_b64],
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "top_p": 0.9,
                            "top_k": 40
                        }
                    }

                    # Fazer requisi√ß√£o para Ollama
                    response = requests.post(
                        f"{self.config.OLLAMA_HOST}/api/generate",
                        json=payload,
                        timeout=60  # Timeout maior para an√°lise de imagem
                    )

                    if response.status_code == 200:
                        result = response.json()
                        analysis_text = result.get('response', '').strip()

                        if analysis_text and len(analysis_text) > 10:
                            self.logger.info(f"‚úÖ An√°lise bem-sucedida com {model}")
                            return analysis_text
                        else:
                            self.logger.warning(f"‚ö†Ô∏è Resposta vazia do modelo {model}")
                    else:
                        self.logger.warning(f"‚ùå Erro HTTP {response.status_code} com {model}")

                except Exception as e:
                    self.logger.warning(f"üí• Erro com modelo {model}: {e}")
                    continue

            # Fallback: an√°lise textual baseada no contexto
            self.logger.info("üîÑ Usando fallback de an√°lise contextual")

            fallback_prompt = f"""
            {prompt}

            NOTA: Baseando a an√°lise no contexto da solicita√ß√£o, assuma que se trata de um material
            comumente encontrado para reciclagem (pl√°stico, papel, vidro, metal, eletr√¥nico).

            Analise considerando o contexto de reciclagem em Bissau, Guin√©-Bissau, e forne√ßa
            orienta√ß√µes pr√°ticas e espec√≠ficas para a regi√£o, mesmo sem visualizar a imagem diretamente.
            """

            return self.generate_response(fallback_prompt)['response']

        except Exception as e:
            self.logger.error(f"Erro cr√≠tico na an√°lise de imagem: {e}")
            return f"""
            **MATERIAL IDENTIFICADO:**
            - Tipo principal: Material n√£o identificado (erro t√©cnico)
            - Categoria de reciclagem: Geral
            - Recicl√°vel: Sim (assumindo material comum)

            **INSTRU√á√ÉO DE DESCARTE EM BISSAU:**
            - Prepara√ß√£o necess√°ria: Limpeza b√°sica recomendada
            - Local de descarte ideal: Ecoponto Central de Bissau
            - Processo recomendado: Verificar com funcion√°rios do ecoponto

            **IMPACTO AMBIENTAL:**
            - Import√¢ncia: Contribui√ß√£o para economia circular local
            - Benef√≠cio: Redu√ß√£o de res√≠duos no meio ambiente

            **DICAS ESPEC√çFICAS PARA GUIN√â-BISSAU:**
            - Verificar hor√°rios de funcionamento dos ecopontos
            - Considerar transporte para pontos de coleta
            - Participar de iniciativas comunit√°rias de reciclagem

            **CONFIAN√áA DA AN√ÅLISE:** 60% (an√°lise limitada por quest√µes t√©cnicas)
            """

    # ========== FUNCIONALIDADES REVOLUCION√ÅRIAS GEMMA 3N ==========

    def contextual_translation(self, text: str, source_lang: str, target_lang: str,
                             context: str = "general", multimodal_data: Optional[Dict] = None) -> Dict[str, Any]:
        """Tradu√ß√£o contextual revolucion√°ria com an√°lise multimodal"""
        try:
            # Preparar prompt contextual
            system_prompt = REVOLUTIONARY_PROMPTS['CONTEXTUAL_TRANSLATION']

            # Construir contexto rico
            context_info = self._build_rich_context(text, source_lang, target_lang, context, multimodal_data)

            prompt = f"""
            TRADU√á√ÉO CONTEXTUAL AVAN√áADA:

            Texto original ({source_lang}): "{text}"
            Idioma de destino: {target_lang}
            Contexto: {context}

            {context_info}

            Por favor, forne√ßa:
            1. Tradu√ß√£o principal preservando contexto cultural
            2. An√°lise emocional do texto
            3. Insights culturais relevantes
            4. Tradu√ß√µes alternativas se apropriado
            5. Notas sobre preserva√ß√£o de significado

            Responda em formato JSON estruturado.
            """

            response = self.generate_response(prompt, system_prompt)

            if response['success']:
                # Processar resposta para extrair componentes
                processed_response = self._process_contextual_translation_response(response['response'])
                processed_response['metadata'] = response.get('metadata', {})
                processed_response['metadata']['feature'] = 'contextual_translation'
                return processed_response
            else:
                return self._fallback_contextual_translation(text, source_lang, target_lang, context)

        except Exception as e:
            self.logger.error(f"Erro na tradu√ß√£o contextual: {e}")
            return self._fallback_contextual_translation(text, source_lang, target_lang, context)

    def emotional_analysis(self, text: str, context: str = "general",
                          multimodal_data: Optional[Dict] = None) -> Dict[str, Any]:
        """An√°lise emocional avan√ßada com contexto cultural"""
        try:
            system_prompt = REVOLUTIONARY_PROMPTS['EMOTIONAL_ANALYSIS']

            # Construir contexto emocional
            emotional_context = self._build_emotional_context(text, context, multimodal_data)

            prompt = f"""
            AN√ÅLISE EMOCIONAL AVAN√áADA:

            Texto: "{text}"
            Contexto: {context}

            {emotional_context}

            Analise:
            1. Estado emocional predominante
            2. Intensidade emocional (0-10)
            3. Emo√ß√µes secund√°rias detectadas
            4. Contexto cultural das emo√ß√µes
            5. Sugest√µes de resposta apropriada
            6. Indicadores de urg√™ncia ou necessidade

            Responda em formato JSON estruturado.
            """

            response = self.generate_response(prompt, system_prompt)

            if response['success']:
                processed_response = self._process_emotional_analysis_response(response['response'])
                processed_response['metadata'] = response.get('metadata', {})
                processed_response['metadata']['feature'] = 'emotional_analysis'
                return processed_response
            else:
                return self._fallback_emotional_analysis(text, context)

        except Exception as e:
            self.logger.error(f"Erro na an√°lise emocional: {e}")
            return self._fallback_emotional_analysis(text, context)

    def cultural_bridge(self, text: str, source_culture: str, target_culture: str,
                       context: str = "general") -> Dict[str, Any]:
        """Ponte cultural inteligente para comunica√ß√£o intercultural"""
        try:
            system_prompt = REVOLUTIONARY_PROMPTS['CULTURAL_BRIDGE']

            prompt = f"""
            PONTE CULTURAL INTELIGENTE:

            Texto: "{text}"
            Cultura de origem: {source_culture}
            Cultura de destino: {target_culture}
            Contexto: {context}

            Forne√ßa:
            1. Explica√ß√£o cultural do texto original
            2. Adapta√ß√£o cultural para o p√∫blico-alvo
            3. Diferen√ßas culturais relevantes
            4. Sugest√µes de comunica√ß√£o efetiva
            5. Poss√≠veis mal-entendidos a evitar
            6. Elementos culturais a preservar

            Responda em formato JSON estruturado.
            """

            response = self.generate_response(prompt, system_prompt)

            if response['success']:
                processed_response = self._process_cultural_bridge_response(response['response'])
                processed_response['metadata'] = response.get('metadata', {})
                processed_response['metadata']['feature'] = 'cultural_bridge'
                return processed_response
            else:
                return self._fallback_cultural_bridge(text, source_culture, target_culture)

        except Exception as e:
            self.logger.error(f"Erro na ponte cultural: {e}")
            return self._fallback_cultural_bridge(text, source_culture, target_culture)

    def adaptive_learning(self, user_input: str, feedback: str, context: str = "general") -> Dict[str, Any]:
        """Sistema de aprendizado adaptativo baseado em feedback"""
        try:
            system_prompt = REVOLUTIONARY_PROMPTS['ADAPTIVE_LEARNING']

            prompt = f"""
            APRENDIZADO ADAPTATIVO:

            Entrada do usu√°rio: "{user_input}"
            Feedback recebido: "{feedback}"
            Contexto: {context}

            Analise e forne√ßa:
            1. Padr√µes identificados no feedback
            2. Ajustes sugeridos para futuras respostas
            3. Prefer√™ncias do usu√°rio detectadas
            4. Melhorias no modelo de resposta
            5. Sugest√µes de personaliza√ß√£o
            6. M√©tricas de aprendizado

            Responda em formato JSON estruturado.
            """

            response = self.generate_response(prompt, system_prompt)

            if response['success']:
                processed_response = self._process_adaptive_learning_response(response['response'])
                processed_response['metadata'] = response.get('metadata', {})
                processed_response['metadata']['feature'] = 'adaptive_learning'
                return processed_response
            else:
                return self._fallback_adaptive_learning(user_input, feedback)

        except Exception as e:
            self.logger.error(f"Erro no aprendizado adaptativo: {e}")
            return self._fallback_adaptive_learning(user_input, feedback)

    def language_teaching(self, target_language: str, current_level: str, focus_area: str,
                         learning_context: str = "general", user_profile: Dict = None) -> Dict[str, Any]:
        """Sistema especializado de ensino de idiomas locais da Guin√©-Bissau"""
        try:
            system_prompt = REVOLUTIONARY_PROMPTS['LANGUAGE_TEACHING']

            # Construir perfil do usu√°rio
            user_info = user_profile or {}
            native_language = user_info.get('native_language', 'portugu√™s')
            learning_goals = user_info.get('learning_goals', 'comunica√ß√£o b√°sica')
            cultural_background = user_info.get('cultural_background', 'Guin√©-Bissau')

            prompt = f"""
            ENSINO ESPECIALIZADO DE IDIOMAS LOCAIS:

            Idioma alvo: {target_language}
            N√≠vel atual: {current_level}
            √Årea de foco: {focus_area}
            Contexto de aprendizado: {learning_context}

            Perfil do aprendiz:
            - Idioma nativo: {native_language}
            - Objetivos: {learning_goals}
            - Contexto cultural: {cultural_background}

            Gere uma li√ß√£o completa que inclua:

            1. VOCABUL√ÅRIO CONTEXTUALIZADO:
               - 10-15 palavras/express√µes essenciais
               - Uso em contextos pr√°ticos
               - Varia√ß√µes regionais quando relevante

            2. ESTRUTURAS GRAMATICAIS:
               - Padr√µes sint√°ticos fundamentais
               - Regras espec√≠ficas do idioma
               - Compara√ß√µes com o idioma nativo

            3. CONTEXTO CULTURAL:
               - Situa√ß√µes de uso apropriadas
               - Nuances culturais importantes
               - Tradi√ß√µes e costumes relacionados

            4. EXERC√çCIOS PR√ÅTICOS:
               - Atividades de compreens√£o
               - Exerc√≠cios de produ√ß√£o
               - Simula√ß√µes de di√°logos

            5. GUIA DE PRON√öNCIA:
               - Fonemas espec√≠ficos
               - Padr√µes de entona√ß√£o
               - Dicas para falantes nativos de {native_language}

            6. PROGRESS√ÉO DE APRENDIZADO:
               - Pr√≥ximos passos sugeridos
               - Recursos adicionais
               - Metas de curto prazo

            Responda em formato JSON estruturado com todas as se√ß√µes.
            """

            response = self.generate_response(prompt, system_prompt)

            if response['success']:
                processed_response = self._process_language_teaching_response(response['response'])
                processed_response['metadata'] = response.get('metadata', {})
                processed_response['metadata']['feature'] = 'language_teaching'
                processed_response['metadata']['target_language'] = target_language
                processed_response['metadata']['level'] = current_level
                return processed_response
            else:
                return self._fallback_language_teaching(target_language, current_level, focus_area)

        except Exception as e:
            self.logger.error(f"Erro no ensino de idiomas: {e}")
            return self._fallback_language_teaching(target_language, current_level, focus_area)

    def multimodal_fusion_analysis(self, text: str, image_data: Optional[bytes] = None,
                                  audio_data: Optional[bytes] = None, context: str = "general") -> Dict[str, Any]:
        """An√°lise de fus√£o multimodal avan√ßada"""
        try:
            system_prompt = REVOLUTIONARY_PROMPTS['MULTIMODAL_FUSION']

            # Analisar componentes multimodais
            multimodal_analysis = self._analyze_multimodal_components(text, image_data, audio_data)

            prompt = f"""
            AN√ÅLISE MULTIMODAL AVAN√áADA:

            Texto: "{text}"
            Contexto: {context}

            An√°lise de componentes:
            {multimodal_analysis}

            Forne√ßa an√°lise integrada:
            1. S√≠ntese de todas as modalidades
            2. Elementos visuais relevantes (se imagem presente)
            3. Elementos auditivos relevantes (se √°udio presente)
            4. Coer√™ncia entre modalidades
            5. Insights √∫nicos da fus√£o multimodal
            6. Recomenda√ß√µes baseadas na an√°lise completa

            Responda em formato JSON estruturado.
            """

            response = self.generate_response(prompt, system_prompt)

            if response['success']:
                processed_response = self._process_multimodal_fusion_response(response['response'])
                processed_response['metadata'] = response.get('metadata', {})
                processed_response['metadata']['feature'] = 'multimodal_fusion'
                return processed_response
            else:
                return self._fallback_multimodal_fusion(text, context)

        except Exception as e:
            self.logger.error(f"Erro na an√°lise multimodal: {e}")
            return self._fallback_multimodal_fusion(text, context)

    def analyze_multimodal(self, prompt: str, image_base64: Optional[str] = None,
                          audio_base64: Optional[str] = None, **kwargs) -> str:
        """Analisar conte√∫do multimodal com abordagem em duas etapas: LLaVA para descri√ß√£o + Gemma3n para diagn√≥stico"""
        try:
            self.logger.info("üîç Iniciando an√°lise multimodal")

            # Se h√° imagem, usar abordagem em duas etapas
            if image_base64:
                import base64
                try:
                    self.logger.info("üì∏ Processando imagem com abordagem em duas etapas")

                    # Etapa 1: Decodificar imagem e usar LLaVA para descri√ß√£o
                    image_data = base64.b64decode(image_base64)
                    description_prompt = "Descreva detalhadamente esta imagem, focando em aspectos visuais relevantes para an√°lise m√©dica ou agr√≠cola. Inclua cores, texturas, formas, padr√µes e qualquer anomalia vis√≠vel."

                    self.logger.info("üîç Etapa 1: Obtendo descri√ß√£o da imagem com LLaVA")
                    image_description = self.analyze_image(image_data, description_prompt)
                    self.logger.info(f"‚úÖ Descri√ß√£o obtida: {image_description[:100]}...")

                    # Etapa 2: Usar gemma3n:e4b para diagn√≥stico baseado na descri√ß√£o
                    diagnosis_prompt = f"{prompt}\n\nDescri√ß√£o da imagem fornecida pelo sistema de vis√£o:\n{image_description}\n\nCom base nesta descri√ß√£o visual detalhada, forne√ßa sua an√°lise especializada."

                    self.logger.info("üß† Etapa 2: Realizando diagn√≥stico com gemma3n:e4b")

                    # For√ßar uso do gemma3n:e4b para o diagn√≥stico
                    response = self._generate_with_specific_model(
                        prompt=diagnosis_prompt,
                        model_name="gemma3n:e4b"
                    )

                    if response['success']:
                        self.logger.info("‚úÖ Diagn√≥stico bem-sucedido com gemma3n:e4b")
                        # Limpar caracteres de controle da resposta
                        clean_response = self._sanitize_text_input(response['response'])
                        self.logger.info(f"üßπ Resposta limpa: {clean_response[:100]}...")
                        return clean_response
                    else:
                        self.logger.warning("‚ö†Ô∏è gemma3n:e4b falhou, tentando fallbacks")
                        # Fallback para outros modelos gemma3n se e4b n√£o estiver dispon√≠vel
                        for model in ["gemma3n:e2b", "gemma3n:latest"]:
                            try:
                                self.logger.info(f"üîÑ Tentando modelo fallback: {model}")
                                response = self._generate_with_specific_model(
                                    prompt=diagnosis_prompt,
                                    model_name=model
                                )
                                if response['success']:
                                    self.logger.info(f"‚úÖ Diagn√≥stico bem-sucedido com {model}")
                                    clean_response = self._sanitize_text_input(response['response'])
                                    return clean_response
                            except Exception as e:
                                self.logger.warning(f"Modelo {model} falhou: {e}")
                                continue

                        self.logger.warning("‚ö†Ô∏è Todos os modelos falharam, usando fallback")
                        return self._fallback_multimodal_response(prompt)

                except Exception as e:
                    self.logger.error(f"Erro ao processar imagem base64: {e}")
                    # Continuar com fallback

            # Para √°udio ou outros tipos, usar m√©todo existente
            multimodal_context = []

            if image_base64:
                multimodal_context.append("Imagem fornecida para an√°lise visual")

            if audio_base64:
                multimodal_context.append("√Åudio fornecido para an√°lise auditiva")

            # Construir prompt completo
            full_prompt = prompt
            if multimodal_context:
                full_prompt += f"\n\nContexto multimodal: {', '.join(multimodal_context)}"

            # Gerar resposta usando o m√©todo existente
            response = self.generate_response(full_prompt)

            if response['success']:
                return response['response']
            else:
                return self._fallback_multimodal_response(prompt)

        except Exception as e:
            self.logger.error(f"Erro na an√°lise multimodal: {e}")
            return self._fallback_multimodal_response(prompt)

    def _fallback_multimodal_response(self, prompt: str) -> str:
        """Resposta de fallback para an√°lise multimodal"""
        return json.dumps({
            "analysis": "An√°lise multimodal em processamento",
            "status": "fallback_mode",
            "message": "Sistema processando solicita√ß√£o com recursos limitados",
            "recommendations": ["Aguardar processamento", "Tentar novamente em alguns momentos"]
        }, ensure_ascii=False)

    def get_health_status(self) -> Dict[str, Any]:
        """Obter status de sa√∫de do servi√ßo"""
        return {
            'ollama_available': self.ollama_available,
            'model_loaded': self.model_loaded,
            'provider': 'ollama' if self.ollama_available else 'local' if self.model_loaded else 'fallback',
            'model_name': self.config.OLLAMA_MODEL if self.ollama_available else self.config.MODEL_NAME,
            'device': self.config.get_device(),
            'multimodal_enabled': self.config.ENABLE_MULTIMODAL,
            'adaptive_config_enabled': self.config.ENABLE_ADAPTIVE_CONFIG,
            'revolutionary_features': {
                'contextual_translation': getattr(self.config, 'ENABLE_CONTEXTUAL_TRANSLATION', True),
                'emotional_analysis': getattr(self.config, 'ENABLE_EMOTIONAL_ANALYSIS', True),
                'cultural_bridge': getattr(self.config, 'ENABLE_CULTURAL_BRIDGE', True),
                'adaptive_learning': getattr(self.config, 'ENABLE_ADAPTIVE_LEARNING', True)
            },
            'timestamp': datetime.now().isoformat()
        }

    # ========== M√âTODOS AUXILIARES PARA FUNCIONALIDADES REVOLUCION√ÅRIAS ==========

    def _build_rich_context(self, text: str, source_lang: str, target_lang: str,
                           context: str, multimodal_data: Optional[Dict]) -> str:
        """Construir contexto rico para tradu√ß√£o contextual"""
        context_parts = []

        # Contexto lingu√≠stico
        context_parts.append(f"An√°lise lingu√≠stica: Traduzindo de {source_lang} para {target_lang}")

        # Contexto situacional
        if context == "medical":
            context_parts.append("Contexto m√©dico: Priorizar precis√£o e clareza para situa√ß√µes de sa√∫de")
        elif context == "education":
            context_parts.append("Contexto educacional: Adaptar para n√≠vel de compreens√£o apropriado")
        elif context == "agriculture":
            context_parts.append("Contexto agr√≠cola: Focar em termos t√©cnicos e pr√°ticas locais")

        # Contexto multimodal
        if multimodal_data:
            if multimodal_data.get('image_present'):
                context_parts.append("Contexto visual: Imagem presente para an√°lise")
            if multimodal_data.get('audio_present'):
                context_parts.append("Contexto auditivo: √Åudio presente para an√°lise")

        # Contexto cultural
        if 'crioulo' in source_lang.lower() or 'crioulo' in target_lang.lower():
            context_parts.append("Contexto cultural: Preservar express√µes e refer√™ncias da Guin√©-Bissau")

        return "\n".join(context_parts)

    def _build_emotional_context(self, text: str, context: str, multimodal_data: Optional[Dict]) -> str:
        """Construir contexto emocional para an√°lise"""
        context_parts = []

        # An√°lise de comprimento e complexidade
        word_count = len(text.split())
        context_parts.append(f"Comprimento do texto: {word_count} palavras")

        # Contexto situacional
        context_parts.append(f"Situa√ß√£o: {context}")

        # Indicadores emocionais b√°sicos
        if any(word in text.lower() for word in ['ajuda', 'urgente', 'emerg√™ncia', 'socorro']):
            context_parts.append("Indicadores de urg√™ncia detectados")

        if any(word in text.lower() for word in ['obrigado', 'obrigada', 'grato', 'grata']):
            context_parts.append("Indicadores de gratid√£o detectados")

        # Contexto multimodal
        if multimodal_data:
            context_parts.append("Dados multimodais dispon√≠veis para an√°lise emocional")

        return "\n".join(context_parts)

    def _analyze_multimodal_components(self, text: str, image_data: Optional[bytes],
                                     audio_data: Optional[bytes]) -> str:
        """Analisar componentes multimodais"""
        analysis_parts = []

        # An√°lise de texto
        analysis_parts.append(f"Texto: {len(text)} caracteres, {len(text.split())} palavras")

        # An√°lise de imagem (simulada)
        if image_data:
            analysis_parts.append(f"Imagem: {len(image_data)} bytes - An√°lise visual dispon√≠vel")
        else:
            analysis_parts.append("Imagem: N√£o presente")

        # An√°lise de √°udio (simulada)
        if audio_data:
            analysis_parts.append(f"√Åudio: {len(audio_data)} bytes - An√°lise auditiva dispon√≠vel")
        else:
            analysis_parts.append("√Åudio: N√£o presente")

        return "\n".join(analysis_parts)

    def _process_contextual_translation_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de tradu√ß√£o contextual"""
        try:
            # Tentar extrair JSON da resposta
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    'success': True,
                    'translation': result.get('translation', response),
                    'emotional_analysis': result.get('emotional_analysis', {}),
                    'cultural_insights': result.get('cultural_insights', []),
                    'alternatives': result.get('alternatives', []),
                    'preservation_notes': result.get('preservation_notes', [])
                }
        except:
            pass

        # Fallback para resposta simples
        return {
            'success': True,
            'translation': response,
            'emotional_analysis': {'tone': 'neutral', 'intensity': 5},
            'cultural_insights': ['Tradu√ß√£o contextual aplicada'],
            'alternatives': [],
            'preservation_notes': ['Significado preservado']
        }

    def _process_emotional_analysis_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de an√°lise emocional"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    'success': True,
                    'primary_emotion': result.get('primary_emotion', 'neutral'),
                    'intensity': result.get('intensity', 5),
                    'secondary_emotions': result.get('secondary_emotions', []),
                    'cultural_context': result.get('cultural_context', ''),
                    'response_suggestions': result.get('response_suggestions', []),
                    'urgency_indicators': result.get('urgency_indicators', [])
                }
        except:
            pass

        return {
            'success': True,
            'primary_emotion': 'neutral',
            'intensity': 5,
            'secondary_emotions': [],
            'cultural_context': 'An√°lise emocional aplicada',
            'response_suggestions': ['Responder com empatia'],
            'urgency_indicators': []
        }

    def _process_cultural_bridge_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de ponte cultural"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    'success': True,
                    'cultural_explanation': result.get('cultural_explanation', ''),
                    'cultural_adaptation': result.get('cultural_adaptation', response),
                    'cultural_differences': result.get('cultural_differences', []),
                    'communication_suggestions': result.get('communication_suggestions', []),
                    'misunderstanding_risks': result.get('misunderstanding_risks', []),
                    'preservation_elements': result.get('preservation_elements', [])
                }
        except:
            pass

        return {
            'success': True,
            'cultural_explanation': 'An√°lise cultural aplicada',
            'cultural_adaptation': response,
            'cultural_differences': [],
            'communication_suggestions': ['Comunicar com respeito cultural'],
            'misunderstanding_risks': [],
            'preservation_elements': []
        }

    def _process_adaptive_learning_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de aprendizado adaptativo"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    'success': True,
                    'patterns_identified': result.get('patterns_identified', []),
                    'suggested_adjustments': result.get('suggested_adjustments', []),
                    'user_preferences': result.get('user_preferences', {}),
                    'model_improvements': result.get('model_improvements', []),
                    'personalization_suggestions': result.get('personalization_suggestions', []),
                    'learning_metrics': result.get('learning_metrics', {})
                }
        except:
            pass

        return {
            'success': True,
            'patterns_identified': ['Padr√£o de uso detectado'],
            'suggested_adjustments': ['Ajustar tom de resposta'],
            'user_preferences': {'style': 'friendly'},
            'model_improvements': ['Melhorar precis√£o'],
            'personalization_suggestions': ['Personalizar respostas'],
            'learning_metrics': {'adaptation_score': 0.7}
        }

    def _process_multimodal_fusion_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de fus√£o multimodal"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    'success': True,
                    'multimodal_synthesis': result.get('multimodal_synthesis', response),
                    'visual_elements': result.get('visual_elements', []),
                    'audio_elements': result.get('audio_elements', []),
                    'modality_coherence': result.get('modality_coherence', 'high'),
                    'fusion_insights': result.get('fusion_insights', []),
                    'recommendations': result.get('recommendations', [])
                }
        except:
            pass

        return {
            'success': True,
            'multimodal_synthesis': response,
            'visual_elements': [],
            'audio_elements': [],
            'modality_coherence': 'high',
            'fusion_insights': ['An√°lise multimodal aplicada'],
            'recommendations': ['Continuar an√°lise integrada']
        }

    # ========== M√âTODOS DE FALLBACK PARA FUNCIONALIDADES REVOLUCION√ÅRIAS ==========

    def _fallback_contextual_translation(self, text: str, source_lang: str, target_lang: str, context: str) -> Dict[str, Any]:
        """Fallback para tradu√ß√£o contextual"""
        translation_text = f"[Tradu√ß√£o contextual de '{text}' de {source_lang} para {target_lang} no contexto {context}]"
        return {
            'success': True,
            'translation': translation_text,
            'emotional_analysis': {'tone': 'neutral', 'intensity': 5},
            'cultural_insights': ['Tradu√ß√£o contextual aplicada com fallback'],
            'alternatives': [],
            'preservation_notes': ['Significado b√°sico preservado'],
            'fallback': True
        }

    def _fallback_emotional_analysis(self, text: str, context: str) -> Dict[str, Any]:
        """Fallback para an√°lise emocional"""
        return {
            'success': True,
            'primary_emotion': 'neutral',
            'intensity': 5,
            'secondary_emotions': [],
            'cultural_context': f'An√°lise emocional b√°sica para contexto {context}',
            'response_suggestions': ['Responder com empatia e compreens√£o'],
            'urgency_indicators': [],
            'fallback': True
        }

    def _fallback_cultural_bridge(self, text: str, source_culture: str, target_culture: str) -> Dict[str, Any]:
        """Fallback para ponte cultural"""
        return {
            'success': True,
            'cultural_explanation': f'Texto origin√°rio da cultura {source_culture}',
            'cultural_adaptation': f'Adapta√ß√£o para cultura {target_culture}: {text}',
            'cultural_differences': ['Diferen√ßas culturais consideradas'],
            'communication_suggestions': ['Comunicar com respeito e sensibilidade cultural'],
            'misunderstanding_risks': ['Poss√≠veis diferen√ßas de interpreta√ß√£o'],
            'preservation_elements': ['Elementos culturais importantes preservados'],
            'fallback': True
        }

    def _fallback_adaptive_learning(self, user_input: str, feedback: str) -> Dict[str, Any]:
        """Fallback para aprendizado adaptativo"""
        return {
            'success': True,
            'patterns_identified': ['Padr√£o de feedback analisado'],
            'suggested_adjustments': ['Ajustar respostas baseado no feedback'],
            'user_preferences': {'feedback_style': 'constructive'},
            'model_improvements': ['Melhorar baseado no feedback recebido'],
            'personalization_suggestions': ['Personalizar futuras intera√ß√µes'],
            'learning_metrics': {'adaptation_score': 0.5, 'feedback_quality': 'good'},
            'fallback': True
        }

    def _fallback_multimodal_fusion(self, text: str, context: str) -> Dict[str, Any]:
        """Fallback para fus√£o multimodal"""
        return {
            'success': True,
            'multimodal_synthesis': f'An√°lise multimodal b√°sica do texto no contexto {context}',
            'visual_elements': ['Elementos visuais n√£o dispon√≠veis'],
            'audio_elements': ['Elementos auditivos n√£o dispon√≠veis'],
            'modality_coherence': 'medium',
            'fusion_insights': ['An√°lise limitada aos dados textuais dispon√≠veis'],
            'recommendations': ['Fornecer dados multimodais para an√°lise completa'],
            'fallback': True
        }

    def _process_language_teaching_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de ensino de idiomas"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    'success': True,
                    'vocabulary': result.get('vocabulary', []),
                    'grammar': result.get('grammar', {}),
                    'cultural_context': result.get('cultural_context', ''),
                    'exercises': result.get('exercises', []),
                    'pronunciation_guide': result.get('pronunciation_guide', {}),
                    'progression': result.get('progression', {}),
                    'lesson_content': result.get('lesson_content', response)
                }
        except:
            pass

        return {
            'success': True,
            'vocabulary': ['Vocabul√°rio b√°sico em desenvolvimento'],
            'grammar': {'patterns': ['Padr√µes gramaticais fundamentais']},
            'cultural_context': 'Contexto cultural espec√≠fico da Guin√©-Bissau',
            'exercises': ['Exerc√≠cios pr√°ticos de conversa√ß√£o'],
            'pronunciation_guide': {'tips': ['Guia de pron√∫ncia dispon√≠vel']},
            'progression': {'next_steps': ['Continuar pr√°tica regular']},
            'lesson_content': response
        }

    def _fallback_language_teaching(self, target_language: str, current_level: str, focus_area: str) -> Dict[str, Any]:
        """Fallback para ensino de idiomas"""
        return {
            'success': True,
            'vocabulary': [
                f'Vocabul√°rio b√°sico de {target_language}',
                'Palavras essenciais para comunica√ß√£o',
                'Express√µes comuns do dia a dia'
            ],
            'grammar': {
                'patterns': [f'Estruturas b√°sicas do {target_language}'],
                'rules': ['Regras fundamentais de gram√°tica'],
                'comparisons': ['Compara√ß√µes com portugu√™s']
            },
            'cultural_context': f'Contexto cultural importante para uso do {target_language} na Guin√©-Bissau',
            'exercises': [
                'Exerc√≠cios de repeti√ß√£o',
                'Pr√°tica de di√°logos simples',
                'Atividades de compreens√£o'
            ],
            'pronunciation_guide': {
                'phonemes': [f'Sons espec√≠ficos do {target_language}'],
                'tips': ['Dicas de pron√∫ncia para iniciantes'],
                'intonation': ['Padr√µes de entona√ß√£o b√°sicos']
            },
            'progression': {
                'current_level': current_level,
                'next_steps': [f'Continuar estudo de {focus_area}'],
                'goals': ['Melhorar flu√™ncia gradualmente'],
                'resources': ['Praticar com falantes nativos']
            },
            'lesson_content': f'Li√ß√£o b√°sica de {target_language} - N√≠vel {current_level}',
            'fallback': True
        }

    # ========== FUNCIONALIDADES COLABORATIVAS MORANSA ==========

    async def generate_translation_challenges(self, category: str = "geral", difficulty: str = "b√°sico", quantity: int = 5, existing_phrases: List[str] = None) -> Dict[str, Any]:
        """Gerar desafios de tradu√ß√£o para a comunidade"""
        if existing_phrases is None:
            existing_phrases = []

        # Prompt mestre + prompt espec√≠fico
        system_prompt = """Voc√™ √© "Moransa", um assistente de IA e o cora√ß√£o do aplicativo de mesmo nome. Sua miss√£o √© apoiar comunidades na Guin√©-Bissau, com foco especial em primeiros socorros, educa√ß√£o e agricultura. Voc√™ entende Portugu√™s (pt-PT) perfeitamente e est√° aprendendo, com a ajuda da comunidade, o Crioulo da Guin√©-Bissau e outros idiomas e dialetos locais.

Seu trabalho √©:
1. Gerar conte√∫do claro, √∫til e culturalmente sens√≠vel em Portugu√™s para que a comunidade possa traduzir.
2. Analisar e processar as contribui√ß√µes da comunidade (tradu√ß√µes, √°udios, imagens) para ajudar a validar a qualidade e enriquecer o conhecimento do aplicativo.
3. Agir sempre como um facilitador do conhecimento, nunca como um detentor absoluto da verdade. O conhecimento da comunidade √© a fonte prim√°ria.

Responda sempre em formato JSON para facilitar a integra√ß√£o com o backend do aplicativo. Seja conciso e direto ao ponto."""

        task_prompt = f"""Com base nos par√¢metros fornecidos, gere uma lista de frases em portugu√™s. As frases devem ser curtas, diretas e essenciais para situa√ß√µes de {category}. Evite as frases j√° existentes. Para cada frase, forne√ßa um contexto de uso claro e sugira de 2 a 3 tags relevantes.

Par√¢metros:
- Categoria: {category}
- Dificuldade: {difficulty}
- Quantidade: {quantity}
- Frases existentes: {existing_phrases}

A resposta DEVE ser um objeto JSON contendo uma lista chamada "challenges". Cada item deve ter: word, category, context, tags."""

        try:
            response = await self.generate_response(system_prompt + "\n\n" + task_prompt)
            return self._process_translation_challenges_response(response)
        except Exception as e:
            self.logger.error(f"Erro ao gerar desafios de tradu√ß√£o: {e}")
            return self._fallback_translation_challenges(category, difficulty, quantity)

    async def process_user_contribution(self, contribution_data: Dict[str, Any]) -> Dict[str, Any]:
        """Processar e analisar contribui√ß√£o da comunidade"""

        # Prompt mestre + prompt espec√≠fico
        system_prompt = """Voc√™ √© "Moransa", um assistente de IA e o cora√ß√£o do aplicativo de mesmo nome. Sua miss√£o √© apoiar comunidades na Guin√©-Bissau, com foco especial em primeiros socorros, educa√ß√£o e agricultura. Voc√™ entende Portugu√™s (pt-PT) perfeitamente e est√° aprendendo, com a ajuda da comunidade, o Crioulo da Guin√©-Bissau e outros idiomas e dialetos locais.

Seu trabalho √©:
1. Gerar conte√∫do claro, √∫til e culturalmente sens√≠vel em Portugu√™s para que a comunidade possa traduzir.
2. Analisar e processar as contribui√ß√µes da comunidade (tradu√ß√µes, √°udios, imagens) para ajudar a validar a qualidade e enriquecer o conhecimento do aplicativo.
3. Agir sempre como um facilitador do conhecimento, nunca como um detentor absoluto da verdade. O conhecimento da comunidade √© a fonte prim√°ria.

Responda sempre em formato JSON para facilitar a integra√ß√£o com o backend do aplicativo. Seja conciso e direto ao ponto."""

        task_prompt = f"""Analise a contribui√ß√£o fornecida por um membro da comunidade. Realize as seguintes tarefas e retorne um √∫nico objeto JSON com os resultados:

1. quality_assessment: Avalie a qualidade geral da contribui√ß√£o. Atribua um score de 0.0 a 1.0 e um feedback em texto.
2. context_enhancement: Melhore o contexto fornecido pelo usu√°rio para ser mais descritivo e √∫til.
3. tag_suggestion: Sugira uma lista de tags relevantes para esta contribui√ß√£o.
4. moderation_flags: Identifique poss√≠veis problemas (low_quality_context, possible_profanity, unrelated_content).
5. approval_recommendation: Sugira um status inicial: 'pending_community_vote' ou 'requires_moderator_review'.

Contribui√ß√£o:
- Palavra: {contribution_data.get('word', '')}
- Tradu√ß√£o: {contribution_data.get('translation', '')}
- Idioma: {contribution_data.get('language', '')}
- Categoria: {contribution_data.get('category', '')}
- Contexto: {contribution_data.get('context', '')}

A resposta DEVE ser um objeto JSON contendo um campo "analysis_result"."""

        try:
            response = await self.generate_response(system_prompt + "\n\n" + task_prompt)
            return self._process_contribution_analysis_response(response)
        except Exception as e:
            self.logger.error(f"Erro ao processar contribui√ß√£o: {e}")
            return self._fallback_contribution_analysis(contribution_data)

    async def generate_educational_content(self, subject: str, level: str = "b√°sico", topic: str = None) -> Dict[str, Any]:
        """Gerar conte√∫do educacional espec√≠fico"""

        system_prompt = """Voc√™ √© "Moransa", um assistente de IA e o cora√ß√£o do aplicativo de mesmo nome. Sua miss√£o √© apoiar comunidades na Guin√©-Bissau, com foco especial em primeiros socorros, educa√ß√£o e agricultura. Voc√™ entende Portugu√™s (pt-PT) perfeitamente e est√° aprendendo, com a ajuda da comunidade, o Crioulo da Guin√©-Bissau e outros idiomas e dialetos locais.

Seu trabalho √©:
1. Gerar conte√∫do claro, √∫til e culturalmente sens√≠vel em Portugu√™s para que a comunidade possa traduzir.
2. Analisar e processar as contribui√ß√µes da comunidade (tradu√ß√µes, √°udios, imagens) para ajudar a validar a qualidade e enriquecer o conhecimento do aplicativo.
3. Agir sempre como um facilitador do conhecimento, nunca como um detentor absoluto da verdade. O conhecimento da comunidade √© a fonte prim√°ria.

Responda sempre em formato JSON para facilitar a integra√ß√£o com o backend do aplicativo. Seja conciso e direto ao ponto."""

        topic_info = f" sobre {topic}" if topic else ""
        task_prompt = f"""Gere conte√∫do educacional em portugu√™s para {subject}{topic_info}, n√≠vel {level}, adequado para comunidades da Guin√©-Bissau.

O conte√∫do deve incluir:
1. lesson_title: T√≠tulo da li√ß√£o
2. objectives: Objetivos de aprendizagem
3. content: Conte√∫do principal da li√ß√£o
4. key_concepts: Conceitos-chave
5. practical_examples: Exemplos pr√°ticos relevantes para a comunidade
6. assessment_questions: Perguntas para avalia√ß√£o
7. cultural_adaptations: Adapta√ß√µes culturais espec√≠ficas

A resposta DEVE ser um objeto JSON contendo um campo "educational_content"."""

        try:
            response = await self.generate_response(system_prompt + "\n\n" + task_prompt)
            return self._process_educational_content_response(response)
        except Exception as e:
            self.logger.error(f"Erro ao gerar conte√∫do educacional: {e}")
            return self._fallback_educational_content(subject, level, topic)

    async def generate_gamification_challenge(self, user_data: dict, community_status: dict) -> dict:
        """
        Gera desafios personalizados de gamifica√ß√£o para o usu√°rio
        """
        try:
            # Prompt mestre para gera√ß√£o de desafios
            master_prompt = """
            Voc√™ √© o 'Mestre do Jogo' do app Moransa, um aplicativo colaborativo para comunidades da Guin√©-Bissau.
            Sua fun√ß√£o √© criar desafios envolventes e personalizados que motivem os usu√°rios a contribuir.

            Tipos de desafio dispon√≠veis:
            - 'Streak Saver': Para usu√°rios que n√£o contribu√≠ram hoje mas t√™m sequ√™ncia ativa
            - 'Category Explorer': Incentiva explora√ß√£o de categorias pouco usadas
            - 'Level Up Push': Para usu√°rios pr√≥ximos de subir de n√≠vel
            - 'Community Hero': Alinhado com necessidades da comunidade
            - 'Daily Contributor': Desafio b√°sico di√°rio
            - 'Knowledge Sharer': Foco em compartilhamento de conhecimento
            """

            # Prompt de tarefa espec√≠fica
            task_prompt = f"""
            Com base nos dados do usu√°rio e status da comunidade, crie UM desafio personalizado:

            Dados do usu√°rio:
            - Nome: {user_data.get('user_name', 'Usu√°rio')}
            - N√≠vel: {user_data.get('level', 1)}
            - XP para pr√≥ximo n√≠vel: {user_data.get('xp_to_next_level', 100)}
            - Sequ√™ncia atual: {user_data.get('current_streak', 0)} dias
            - Categorias preferidas: {user_data.get('preferred_categories', ['geral'])}
            - Contribui√ß√µes hoje: {user_data.get('contribution_count_today', 0)}

            Status da comunidade:
            - Categoria mais necess√°ria: {community_status.get('most_needed_category', 'geral')}
            - Evento comunit√°rio ativo: {community_status.get('active_community_event', 'Nenhum')}

            Retorne APENAS um objeto JSON v√°lido com:
            {{
                "challenge": {{
                    "challenge_type": "tipo_do_desafio",
                    "title": "Nome criativo do desafio",
                    "description": "Descri√ß√£o clara do que fazer",
                    "xp_reward": n√∫mero_de_pontos
                }}
            }}
            """

            # Combinar prompts
            full_prompt = f"{master_prompt}\n\n{task_prompt}"

            # Fazer chamada para Ollama
            response = await self._make_ollama_request(full_prompt)

            if response and 'response' in response:
                return self._process_gamification_challenge_response(response['response'])
            else:
                return self._fallback_gamification_challenge(user_data)

        except Exception as e:
            self.logger.error(f"Erro ao gerar desafio de gamifica√ß√£o: {e}")
            return self._fallback_gamification_challenge(user_data)

    async def generate_reward_message(self, event_data: dict) -> dict:
        """
        Gera mensagens personalizadas de recompensa e celebra√ß√£o
        """
        try:
            # Prompt mestre para mensagens de recompensa
            master_prompt = """
            Voc√™ √© o 'Mestre do Jogo' do app Moransa. Sua fun√ß√£o √© criar mensagens inspiradoras
            e personalizadas para celebrar as conquistas dos usu√°rios da comunidade da Guin√©-Bissau.

            As mensagens devem ser:
            - Curtas e impactantes
            - Culturalmente apropriadas
            - Motivadoras e positivas
            - Que reforcem o valor da contribui√ß√£o para a comunidade
            """

            # Prompt de tarefa espec√≠fica
            task_prompt = f"""
            Crie uma mensagem de celebra√ß√£o para o seguinte evento:

            Tipo de evento: {event_data.get('event_type', 'conquista')}
            Nome do usu√°rio: {event_data.get('user_name', 'Usu√°rio')}
            Detalhes do evento: {event_data}

            Retorne APENAS um objeto JSON v√°lido com:
            {{
                "notification": {{
                    "title": "T√≠tulo da notifica√ß√£o",
                    "body": "Mensagem inspiradora e personalizada"
                }}
            }}
            """

            # Combinar prompts
            full_prompt = f"{master_prompt}\n\n{task_prompt}"

            # Fazer chamada para Ollama
            response = await self._make_ollama_request(full_prompt)

            if response and 'response' in response:
                return self._process_reward_message_response(response['response'])
            else:
                return self._fallback_reward_message(event_data)

        except Exception as e:
            self.logger.error(f"Erro ao gerar mensagem de recompensa: {e}")
            return self._fallback_reward_message(event_data)

    async def create_badge(self, badge_criteria: str, category: str = "geral") -> dict:
        """
        Cria badges din√¢micos com nomes e descri√ß√µes personalizadas
        """
        try:
            # Prompt mestre para cria√ß√£o de badges
            master_prompt = """
            Voc√™ √© o 'Mestre do Jogo' do app Moransa. Sua fun√ß√£o √© criar conquistas (badges)
            significativas e motivadoras para a comunidade da Guin√©-Bissau.

            Os badges devem:
            - Ter nomes criativos e memor√°veis
            - Descri√ß√µes que expliquem o valor da conquista
            - Sugest√µes de √≠cones simples e reconhec√≠veis
            - Refletir a cultura e valores da comunidade
            """

            # Prompt de tarefa espec√≠fica
            task_prompt = f"""
            Crie um badge baseado no seguinte crit√©rio:

            Crit√©rio: {badge_criteria}
            Categoria: {category}

            Retorne APENAS um objeto JSON v√°lido com:
            {{
                "badge": {{
                    "name": "Nome criativo do badge",
                    "description": "Descri√ß√£o motivadora que explica o valor da conquista",
                    "icon_suggestion": "Descri√ß√£o simples para o √≠cone do badge",
                    "category": "{category}"
                }}
            }}
            """

            # Combinar prompts
            full_prompt = f"{master_prompt}\n\n{task_prompt}"

            # Fazer chamada para Ollama
            response = await self._make_ollama_request(full_prompt)

            if response and 'response' in response:
                return self._process_badge_creation_response(response['response'])
            else:
                return self._fallback_badge_creation(badge_criteria, category)

        except Exception as e:
            self.logger.error(f"Erro ao criar badge: {e}")
            return self._fallback_badge_creation(badge_criteria, category)

    # ========== M√âTODOS AUXILIARES ASS√çNCRONOS ==========

    async def _make_ollama_request(self, prompt: str) -> Dict[str, Any]:
        """Fazer requisi√ß√£o ass√≠ncrona para Ollama"""
        try:
            if self.ollama_available:
                # Usar m√©todo s√≠ncrono existente
                response = self._generate_with_ollama(prompt)
                return response
            else:
                # Fallback para modelo local
                response = self._generate_with_local_model(prompt)
                return response
        except Exception as e:
            self.logger.error(f"Erro na requisi√ß√£o Ollama: {e}")
            return None

    # ========== M√âTODOS DE PROCESSAMENTO DE RESPOSTA MORANSA ==========

    def _process_translation_challenges_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de gera√ß√£o de desafios de tradu√ß√£o"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                if 'challenges' in result:
                    return {
                        'success': True,
                        'challenges': result['challenges'],
                        'generated_count': len(result['challenges'])
                    }
        except Exception as e:
            self.logger.error(f"Erro ao processar resposta de desafios: {e}")

        # Fallback se n√£o conseguir processar JSON
        return {
            'success': False,
            'challenges': [],
            'generated_count': 0,
            'error': 'Falha ao processar resposta da IA'
        }

    def _process_contribution_analysis_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de an√°lise de contribui√ß√£o"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                if 'analysis_result' in result:
                    return {
                        'success': True,
                        'analysis_result': result['analysis_result']
                    }
        except Exception as e:
            self.logger.error(f"Erro ao processar an√°lise de contribui√ß√£o: {e}")

        # Fallback se n√£o conseguir processar JSON
        return {
            'success': False,
            'analysis_result': {
                'quality_assessment': {
                    'score': 0.5,
                    'feedback': 'An√°lise autom√°tica n√£o dispon√≠vel'
                },
                'context_enhancement': 'Contexto necessita revis√£o manual',
                'tag_suggestion': ['geral'],
                'moderation_flags': ['requires_manual_review'],
                'approval_recommendation': 'requires_moderator_review'
            },
            'error': 'Falha ao processar resposta da IA'
        }

    def _process_educational_content_response(self, response: str) -> Dict[str, Any]:
        """Processar resposta de conte√∫do educacional"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                if 'educational_content' in result:
                    return {
                        'success': True,
                        'educational_content': result['educational_content']
                    }
        except Exception as e:
            self.logger.error(f"Erro ao processar conte√∫do educacional: {e}")

        # Fallback se n√£o conseguir processar JSON
        return {
            'success': False,
            'educational_content': {
                'lesson_title': 'Li√ß√£o B√°sica',
                'objectives': ['Aprender conceitos fundamentais'],
                'content': 'Conte√∫do educacional b√°sico',
                'key_concepts': ['Conceito fundamental'],
                'practical_examples': ['Exemplo pr√°tico'],
                'assessment_questions': ['Pergunta de avalia√ß√£o'],
                'cultural_adaptations': ['Adapta√ß√£o cultural necess√°ria']
            },
            'error': 'Falha ao processar resposta da IA'
        }

    def _process_gamification_challenge_response(self, response_text: str) -> dict:
        """
        Processa a resposta da Gemma-3 para desafios de gamifica√ß√£o
        """
        try:
            # Tentar extrair JSON da resposta
            json_match = re.search(r'\{[^{}]*"challenge"[^{}]*\{[^{}]*\}[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                data = json.loads(json_str)

                if 'challenge' in data and isinstance(data['challenge'], dict):
                    challenge = data['challenge']
                    # Validar campos obrigat√≥rios
                    required_fields = ['challenge_type', 'title', 'description', 'xp_reward']
                    if all(field in challenge for field in required_fields):
                        return {
                            'success': True,
                            'challenge': {
                                'challenge_type': str(challenge['challenge_type']),
                                'title': str(challenge['title']),
                                'description': str(challenge['description']),
                                'xp_reward': int(challenge['xp_reward']) if isinstance(challenge['xp_reward'], (int, str)) else 50
                            }
                        }

            # Se n√£o conseguir extrair JSON v√°lido, usar fallback
            return self._fallback_gamification_challenge({})

        except Exception as e:
            self.logger.error(f"Erro ao processar resposta de desafio de gamifica√ß√£o: {e}")
            return self._fallback_gamification_challenge({})

    def _process_reward_message_response(self, response_text: str) -> dict:
        """
        Processa a resposta da Gemma-3 para mensagens de recompensa
        """
        try:
            # Tentar extrair JSON da resposta
            json_match = re.search(r'\{[^{}]*"notification"[^{}]*\{[^{}]*\}[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                data = json.loads(json_str)

                if 'notification' in data and isinstance(data['notification'], dict):
                    notification = data['notification']
                    # Validar campos obrigat√≥rios
                    if 'title' in notification and 'body' in notification:
                        return {
                            'success': True,
                            'notification': {
                                'title': str(notification['title']),
                                'body': str(notification['body'])
                            }
                        }

            # Se n√£o conseguir extrair JSON v√°lido, usar fallback
            return self._fallback_reward_message({})

        except Exception as e:
            self.logger.error(f"Erro ao processar resposta de mensagem de recompensa: {e}")
            return self._fallback_reward_message({})

    def _process_badge_creation_response(self, response_text: str) -> dict:
        """
        Processa a resposta da Gemma-3 para cria√ß√£o de badges
        """
        try:
            # Tentar extrair JSON da resposta
            json_match = re.search(r'\{[^{}]*"badge"[^{}]*\{[^{}]*\}[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                data = json.loads(json_str)

                if 'badge' in data and isinstance(data['badge'], dict):
                    badge = data['badge']
                    # Validar campos obrigat√≥rios
                    required_fields = ['name', 'description', 'icon_suggestion']
                    if all(field in badge for field in required_fields):
                        return {
                            'success': True,
                            'badge': {
                                'name': str(badge['name']),
                                'description': str(badge['description']),
                                'icon_suggestion': str(badge['icon_suggestion']),
                                'category': str(badge.get('category', 'geral'))
                            }
                        }

            # Se n√£o conseguir extrair JSON v√°lido, usar fallback
            return self._fallback_badge_creation("", "geral")

        except Exception as e:
            self.logger.error(f"Erro ao processar resposta de cria√ß√£o de badge: {e}")
            return self._fallback_badge_creation("", "geral")

    # ========== M√âTODOS DE FALLBACK MORANSA ==========

    def _fallback_translation_challenges(self, category: str, difficulty: str, quantity: int) -> Dict[str, Any]:
        """Fallback para gera√ß√£o de desafios de tradu√ß√£o"""

        # Desafios pr√©-definidos por categoria
        challenges_by_category = {
            'm√©dica': [
                {
                    'word': 'Onde d√≥i?',
                    'category': 'm√©dica',
                    'context': 'Pergunta essencial para localizar a dor do paciente',
                    'tags': ['dor', 'localiza√ß√£o', 'diagn√≥stico']
                },
                {
                    'word': 'Voc√™ tem febre?',
                    'category': 'm√©dica',
                    'context': 'Verifica√ß√£o de sintoma comum de infec√ß√£o',
                    'tags': ['febre', 'sintoma', 'temperatura']
                },
                {
                    'word': 'Chame uma ambul√¢ncia.',
                    'category': 'm√©dica',
                    'context': 'Instru√ß√£o urgente para emerg√™ncias graves',
                    'tags': ['emerg√™ncia', 'ambul√¢ncia', 'urgente']
                },
                {
                    'word': 'Mantenha a calma.',
                    'category': 'm√©dica',
                    'context': 'Instru√ß√£o para acalmar paciente em situa√ß√£o de stress',
                    'tags': ['calma', 'psicol√≥gico', 'suporte']
                },
                {
                    'word': 'Voc√™ tem alguma alergia?',
                    'category': 'm√©dica',
                    'context': 'Pergunta crucial antes de administrar medicamentos',
                    'tags': ['alergia', 'medicamento', 'seguran√ßa']
                }
            ],
            'educa√ß√£o': [
                {
                    'word': 'Vamos aprender juntos.',
                    'category': 'educa√ß√£o',
                    'context': 'Frase motivacional para iniciar uma li√ß√£o',
                    'tags': ['motiva√ß√£o', 'aprendizado', 'colabora√ß√£o']
                },
                {
                    'word': 'Voc√™ entendeu?',
                    'category': 'educa√ß√£o',
                    'context': 'Verifica√ß√£o de compreens√£o durante o ensino',
                    'tags': ['compreens√£o', 'verifica√ß√£o', 'ensino']
                },
                {
                    'word': 'Muito bem!',
                    'category': 'educa√ß√£o',
                    'context': 'Elogio para encorajar o aluno',
                    'tags': ['elogio', 'encorajamento', 'positivo']
                },
                {
                    'word': 'Tente novamente.',
                    'category': 'educa√ß√£o',
                    'context': 'Encorajamento ap√≥s erro ou dificuldade',
                    'tags': ['persist√™ncia', 'encorajamento', 'tentativa']
                },
                {
                    'word': 'Qual √© a sua d√∫vida?',
                    'category': 'educa√ß√£o',
                    'context': 'Pergunta para identificar dificuldades do aluno',
                    'tags': ['d√∫vida', 'esclarecimento', 'ajuda']
                }
            ],
            'agricultura': [
                {
                    'word': 'Quando plantar?',
                    'category': 'agricultura',
                    'context': 'Pergunta sobre o timing ideal para plantio',
                    'tags': ['plantio', 'timing', '√©poca']
                },
                {
                    'word': 'A terra est√° seca.',
                    'category': 'agricultura',
                    'context': 'Observa√ß√£o sobre condi√ß√£o do solo',
                    'tags': ['solo', 'seca', 'irriga√ß√£o']
                },
                {
                    'word': 'Precisa de √°gua.',
                    'category': 'agricultura',
                    'context': 'Identifica√ß√£o de necessidade de irriga√ß√£o',
                    'tags': ['√°gua', 'irriga√ß√£o', 'necessidade']
                },
                {
                    'word': 'A colheita est√° pronta.',
                    'category': 'agricultura',
                    'context': 'Indica√ß√£o de que √© hora de colher',
                    'tags': ['colheita', 'pronto', 'tempo']
                },
                {
                    'word': 'Cuidado com as pragas.',
                    'category': 'agricultura',
                    'context': 'Alerta sobre prote√ß√£o das culturas',
                    'tags': ['pragas', 'prote√ß√£o', 'cuidado']
                }
            ]
        }

        # Selecionar desafios da categoria ou usar geral
        available_challenges = challenges_by_category.get(category, challenges_by_category['m√©dica'])
        selected_challenges = available_challenges[:quantity]

        return {
            'success': True,
            'challenges': selected_challenges,
            'generated_count': len(selected_challenges),
            'fallback': True
        }

    def _fallback_contribution_analysis(self, contribution_data: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback para an√°lise de contribui√ß√£o"""

        # An√°lise b√°sica baseada em regras simples
        word = contribution_data.get('word', '')
        translation = contribution_data.get('translation', '')
        context = contribution_data.get('context', '')
        category = contribution_data.get('category', 'geral')

        # Score b√°sico baseado na presen√ßa de dados
        score = 0.3  # Base
        if len(translation) > 0:
            score += 0.3
        if len(context) > 10:
            score += 0.2
        if category != 'geral':
            score += 0.2

        # Flags de modera√ß√£o b√°sicas
        moderation_flags = []
        if len(context) < 10:
            moderation_flags.append('low_quality_context')
        if len(translation) < 2:
            moderation_flags.append('incomplete_translation')

        # Recomenda√ß√£o baseada no score
        recommendation = 'pending_community_vote' if score >= 0.6 else 'requires_moderator_review'

        # Sugest√µes de tags baseadas na categoria
        tag_suggestions = {
            'm√©dica': ['sa√∫de', 'primeiros socorros', 'medicina'],
            'educa√ß√£o': ['ensino', 'aprendizado', 'escola'],
            'agricultura': ['plantio', 'colheita', 'cultivo'],
            'geral': ['comunidade', 'comunica√ß√£o', 'b√°sico']
        }

        return {
            'success': True,
            'analysis_result': {
                'quality_assessment': {
                    'score': score,
                    'feedback': f'An√°lise autom√°tica: contribui√ß√£o com qualidade {"boa" if score >= 0.7 else "m√©dia" if score >= 0.5 else "baixa"}'
                },
                'context_enhancement': context if len(context) > 10 else f'Contexto para uso de "{word}" em situa√ß√µes de {category}',
                'tag_suggestion': tag_suggestions.get(category, tag_suggestions['geral']),
                'moderation_flags': moderation_flags,
                'approval_recommendation': recommendation
            },
            'fallback': True
        }

    def _fallback_educational_content(self, subject: str, level: str, topic: str = None) -> Dict[str, Any]:
        """Fallback para conte√∫do educacional"""

        topic_info = f" - {topic}" if topic else ""

        content_templates = {
            'primeiros socorros': {
                'lesson_title': f'Primeiros Socorros B√°sicos{topic_info}',
                'objectives': [
                    'Identificar situa√ß√µes de emerg√™ncia',
                    'Aplicar t√©cnicas b√°sicas de primeiros socorros',
                    'Saber quando chamar ajuda profissional'
                ],
                'content': 'Os primeiros socorros s√£o cuidados imediatos prestados a uma pessoa ferida ou doente at√© que chegue ajuda m√©dica profissional.',
                'key_concepts': ['Avalia√ß√£o da situa√ß√£o', 'Seguran√ßa primeiro', 'ABC (Vias a√©reas, Respira√ß√£o, Circula√ß√£o)'],
                'practical_examples': [
                    'Como parar uma hemorragia',
                    'Posi√ß√£o de recupera√ß√£o',
                    'Quando e como chamar emerg√™ncia'
                ],
                'assessment_questions': [
                    'Quais s√£o os primeiros passos ao encontrar uma pessoa ferida?',
                    'Como identificar se uma pessoa est√° consciente?'
                ],
                'cultural_adaptations': [
                    'Adaptar t√©cnicas aos recursos dispon√≠veis na comunidade',
                    'Considerar cren√ßas locais sobre sa√∫de e cura'
                ]
            },
            'agricultura': {
                'lesson_title': f'T√©cnicas Agr√≠colas{topic_info}',
                'objectives': [
                    'Compreender ciclos de plantio',
                    'Identificar pragas comuns',
                    'Aplicar t√©cnicas de conserva√ß√£o do solo'
                ],
                'content': 'A agricultura sustent√°vel combina t√©cnicas tradicionais com conhecimentos modernos para maximizar a produ√ß√£o.',
                'key_concepts': ['Rota√ß√£o de culturas', 'Compostagem', 'Controle natural de pragas'],
                'practical_examples': [
                    'Prepara√ß√£o do solo para plantio',
                    'Identifica√ß√£o de pragas comuns',
                    'T√©cnicas de irriga√ß√£o eficiente'
                ],
                'assessment_questions': [
                    'Qual a melhor √©poca para plantar na sua regi√£o?',
                    'Como identificar se o solo est√° pronto para plantio?'
                ],
                'cultural_adaptations': [
                    'Usar conhecimentos tradicionais da comunidade',
                    'Adaptar t√©cnicas ao clima local'
                ]
            }
        }

        # Usar template espec√≠fico ou geral
        template = content_templates.get(subject, {
            'lesson_title': f'Li√ß√£o de {subject.title()}{topic_info}',
            'objectives': [f'Aprender conceitos b√°sicos de {subject}'],
            'content': f'Conte√∫do educacional sobre {subject} adaptado para a comunidade.',
            'key_concepts': [f'Conceitos fundamentais de {subject}'],
            'practical_examples': [f'Exemplos pr√°ticos de {subject}'],
            'assessment_questions': [f'Perguntas sobre {subject}'],
            'cultural_adaptations': ['Adapta√ß√µes culturais necess√°rias']
        })

        return {
            'success': True,
            'educational_content': template,
            'fallback': True
        }

    def _fallback_gamification_challenge(self, user_data: dict) -> Dict[str, Any]:
        """Fallback para desafios de gamifica√ß√£o"""

        user_level = user_data.get('level', 1)
        user_name = user_data.get('user_name', 'Usu√°rio')

        # Determinar tipo de desafio baseado no perfil do usu√°rio
        if user_data.get('contribution_count_today', 0) == 0:
            challenge_type = 'daily_contributor'
        elif user_data.get('xp_to_next_level', 100) < 50:
            challenge_type = 'level_up_push'
        else:
            challenge_type = 'translation'

        challenges_by_type = {
            'translation': {
                'challenge_type': 'translation',
                'title': 'Desafio de Tradu√ß√£o',
                'description': f'{user_name}, traduza palavras importantes para sua comunidade!',
                'xp_reward': 15
            },
            'daily_contributor': {
                'challenge_type': 'daily_contributor',
                'title': 'Contribuidor Di√°rio',
                'description': f'{user_name}, fa√ßa sua primeira contribui√ß√£o hoje e mantenha sua sequ√™ncia!',
                'xp_reward': 20
            },
            'level_up_push': {
                'challenge_type': 'level_up_push',
                'title': 'Quase L√°!',
                'description': f'{user_name}, voc√™ est√° quase subindo de n√≠vel! Fa√ßa mais uma contribui√ß√£o.',
                'xp_reward': 25
            }
        }

        challenge = challenges_by_type.get(challenge_type, challenges_by_type['translation'])

        # Ajustar pontos baseado no n√≠vel
        level_multiplier = max(1.0, user_level * 0.2)
        challenge['xp_reward'] = int(challenge['xp_reward'] * level_multiplier)

        return {
            'success': True,
            'challenge': challenge,
            'fallback': True
        }

    def _fallback_reward_message(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback para mensagens de recompensa"""

        achievement_type = event_data.get('achievement_type', 'challenge_completed')
        points_earned = event_data.get('points_earned', 0)
        user_data = event_data.get('user_data', {})

        user_name = user_data.get('name', event_data.get('user_name', 'Amigo'))
        total_points = user_data.get('total_points', 0)

        messages_by_type = {
            'challenge_completed': {
                'title': 'Parab√©ns!',
                'message': f'Excelente trabalho, {user_name}! Voc√™ completou o desafio e ganhou {points_earned} pontos.',
                'encouragement': 'Continue assim e ajude sua comunidade a crescer!',
                'next_action': 'Que tal tentar um desafio mais dif√≠cil?'
            },
            'level_up': {
                'title': 'N√≠vel Aumentado!',
                'message': f'Incr√≠vel, {user_name}! Voc√™ subiu de n√≠vel com {total_points} pontos totais.',
                'encouragement': 'Seu conhecimento est√° crescendo e beneficiando toda a comunidade!',
                'next_action': 'Novos desafios foram desbloqueados para voc√™!'
            },
            'contribution_accepted': {
                'title': 'Contribui√ß√£o Aceita!',
                'message': f'Obrigado, {user_name}! Sua contribui√ß√£o foi aceita e voc√™ ganhou {points_earned} pontos.',
                'encouragement': 'Voc√™ est√° ajudando a preservar e compartilhar o conhecimento local!',
                'next_action': 'Continue contribuindo para fortalecer nossa comunidade!'
            },
            'milestone_reached': {
                'title': 'Marco Alcan√ßado!',
                'message': f'Fant√°stico, {user_name}! Voc√™ alcan√ßou um marco importante com {total_points} pontos.',
                'encouragement': 'Seu dedica√ß√£o est√° fazendo a diferen√ßa na comunidade!',
                'next_action': 'Vamos celebrar e continuar aprendendo juntos!'
            }
        }

        reward = messages_by_type.get(achievement_type, messages_by_type['challenge_completed'])

        return {
            'success': True,
            'reward_message': reward,
            'fallback': True
        }

    def _fallback_badge_creation(self, achievement_data: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback para cria√ß√£o de badges"""

        achievement_type = achievement_data.get('type', 'general')
        level = achievement_data.get('level', 'bronze')
        category = achievement_data.get('category', 'geral')

        badge_templates = {
            'translator': {
                'name': f'Tradutor {level.title()}',
                'description': 'Reconhecimento por contribui√ß√µes em tradu√ß√£o',
                'icon': 'üåê',
                'color': {'bronze': '#CD7F32', 'prata': '#C0C0C0', 'ouro': '#FFD700'}.get(level, '#CD7F32'),
                'criteria': f'Completou {10 if level == "bronze" else 25 if level == "prata" else 50} tradu√ß√µes'
            },
            'teacher': {
                'name': f'Educador {level.title()}',
                'description': 'Reconhecimento por contribui√ß√µes educacionais',
                'icon': 'üìö',
                'color': {'bronze': '#CD7F32', 'prata': '#C0C0C0', 'ouro': '#FFD700'}.get(level, '#CD7F32'),
                'criteria': f'Ajudou {5 if level == "bronze" else 15 if level == "prata" else 30} pessoas a aprender'
            },
            'helper': {
                'name': f'Ajudante {level.title()}',
                'description': 'Reconhecimento por ajudar a comunidade',
                'icon': 'ü§ù',
                'color': {'bronze': '#CD7F32', 'prata': '#C0C0C0', 'ouro': '#FFD700'}.get(level, '#CD7F32'),
                'criteria': f'Prestou {3 if level == "bronze" else 10 if level == "prata" else 20} ajudas importantes'
            },
            'contributor': {
                'name': f'Contribuidor {level.title()}',
                'description': 'Reconhecimento por contribui√ß√µes valiosas',
                'icon': '‚≠ê',
                'color': {'bronze': '#CD7F32', 'prata': '#C0C0C0', 'ouro': '#FFD700'}.get(level, '#CD7F32'),
                'criteria': f'Fez {5 if level == "bronze" else 15 if level == "prata" else 30} contribui√ß√µes aceitas'
            }
        }

        badge = badge_templates.get(achievement_type, badge_templates['contributor'])

        # Adicionar informa√ß√µes espec√≠ficas da categoria
        if category != 'geral':
            badge['name'] += f' - {category.title()}'
            badge['description'] += f' na √°rea de {category}'

        return {
            'success': True,
            'badge': badge,
            'fallback': True
        }
