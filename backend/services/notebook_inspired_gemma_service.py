#!/usr/bin/env python3
"""
Backend Otimizado Bu Fala - Inspirado no Notebook Local
Aplicando as otimiza√ß√µes descobertas no notebook para o DocsGemmaService
"""

import os
import logging
import torch
import time
from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer
from config.settings import BackendConfig
from utils.fallback_responses import FallbackResponseGenerator

logger = logging.getLogger(__name__)

class NotebookInspiredGemmaService:
    """
    Servi√ßo Gemma otimizado baseado nas descobertas do notebook local
    Implementa as mesmas t√©cnicas que funcionaram no Jupyter
    """
    
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.is_initialized = False
        self.fallback_generator = FallbackResponseGenerator()
        
        # Configura√ß√µes otimizadas inspiradas no notebook
        self.optimized_config = {
            "max_new_tokens": 100,      # ‚ö° Reduzido de 400
            "temperature": 0.7,         # üìä Mesmo do notebook 
            "top_p": 0.9,              # üìä Mesmo do notebook
            "do_sample": True,          # üìä Mesmo do notebook
            "pad_token_id": None,       # üîß Ser√° definido ap√≥s carregar tokenizer
            "torch_dtype": torch.float32  # üéØ CPU otimizado
        }
        
        self.initialize()
    
    def initialize(self):
        """Inicializar modelo seguindo a abordagem do notebook"""
        try:
            logger.info("üöÄ Inicializando Notebook-Inspired Gemma Service...")
            
            # Usar o mesmo caminho que funcionou no notebook
            model_path = r"C:\Users\fbg67\.cache\kagglehub\models\google\gemma-3n\transformers\gemma-3n-e2b-it\1"
            
            if not os.path.exists(model_path):
                logger.error(f"‚ùå Modelo n√£o encontrado: {model_path}")
                logger.info("üí° Execute o notebook primeiro para baixar o modelo")
                return False
            
            start_time = time.time()
            
            logger.info("üì¶ Carregando tokenizer (inspirado no notebook)...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=True
            )
            
            logger.info("üß† Carregando modelo (inspirado no notebook)...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=True,
                torch_dtype=self.optimized_config["torch_dtype"]
            )
            
            # Configurar pad_token_id como no notebook
            self.optimized_config["pad_token_id"] = self.tokenizer.eos_token_id
            
            # Usar CPU como no notebook (mais est√°vel)
            device = "cpu"  # For√ßar CPU como funcionou no notebook
            logger.info(f"üì± Usando dispositivo: {device}")
            
            # N√£o chamar .to(device) se j√° est√° na CPU por padr√£o
            
            load_time = time.time() - start_time
            self.is_initialized = True
            
            logger.info(f"‚úÖ Notebook-Inspired Gemma carregado em {load_time:.2f}s!")
            logger.info("üéØ Configura√ß√£o otimizada ativa - respostas em ~60s")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o inspirada: {e}")
            return False
    
    def notebook_style_query(self, input_text, context="agricultura"):
        """
        Fun√ß√£o gemma_query() inspirada diretamente no notebook
        Implementa√ß√£o quase id√™ntica √† que funcionou no Jupyter
        """
        try:
            if not self.is_initialized:
                logger.warning("‚ö†Ô∏è Modelo n√£o inicializado")
                return self._fallback_response(input_text, context)
            
            # Verificar se o input n√£o est√° vazio (como no notebook)
            if not input_text.strip():
                return "Por favor, digite uma pergunta."
            
            start_time = time.time()
            logger.info(f"ü§ñ Processando (notebook style): {input_text[:50]}...")
            
            # Preparar input exatamente como no notebook
            inputs = self.tokenizer(input_text, return_tensors="pt")
            
            # Mover para mesmo device que o modelo (CPU)
            inputs = inputs.to(self.model.device)
            
            # Gerar resposta com configura√ß√µes do notebook
            with torch.no_grad():  # Exatamente como no notebook
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.optimized_config["max_new_tokens"],  # 100
                    do_sample=self.optimized_config["do_sample"],            # True
                    temperature=self.optimized_config["temperature"],        # 0.7
                    top_p=self.optimized_config["top_p"],                   # 0.9
                    pad_token_id=self.optimized_config["pad_token_id"]      # eos_token_id
                )
            
            # Decodificar resposta exatamente como no notebook
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Remover o prompt original da resposta (como no notebook)
            if input_text in response:
                response = response.replace(input_text, "").strip()
            
            generation_time = time.time() - start_time
            
            if response and len(response) > 5:
                logger.info(f"‚úÖ Resposta notebook-style gerada em {generation_time:.2f}s")
                return f"{response}\n\n[Notebook-Inspired Gemma ‚Ä¢ {generation_time:.1f}s]"
            else:
                logger.warning("‚ö†Ô∏è Resposta vazia ou muito curta")
                return self._fallback_response(input_text, context)
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o notebook-style: {e}")
            return self._fallback_response(input_text, context)
    
    def create_optimized_prompt(self, prompt, context="agricultura"):
        """
        Criar prompts otimizados e concisos (inspirado no notebook)
        Evitar contexto excessivo que causa lentid√£o
        """
        
        # Templates super concisos - inspirados no notebook
        concise_templates = {
            "agricultura": f"Como especialista agr√≠cola, responda: {prompt}",
            "medico": f"Como m√©dico, responda: {prompt}",
            "educacao": f"Como educador, responda: {prompt}",
            "geral": prompt  # Prompt direto como no notebook
        }
        
        # Retornar prompt conciso
        return concise_templates.get(context, prompt)
    
    def generate_response(self, prompt, language="pt-BR", context="agricultura"):
        """M√©todo de compatibilidade - usando abordagem do notebook"""
        try:
            logger.info(f"üîÑ Usando abordagem notebook para: {prompt[:50]}...")
            
            # Criar prompt otimizado e conciso
            optimized_prompt = self.create_optimized_prompt(prompt, context)
            
            # Usar fun√ß√£o notebook-style
            response = self.notebook_style_query(optimized_prompt, context)
            
            logger.info(f"‚úÖ Resposta notebook-inspired gerada com sucesso")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Erro no m√©todo compatibilidade: {e}")
            return self._fallback_response(prompt, context)
    
    def _fallback_response(self, prompt, context="agricultura"):
        """Resposta de fallback contextualizada"""
        response = self.fallback_generator.generate_response(prompt, context)
        return f"{response}\n\n[Fallback Response ‚Ä¢ Notebook-Gemma indispon√≠vel]"
    
    def get_status(self):
        """Status do servi√ßo inspirado no notebook"""
        return {
            "service": "Notebook-Inspired Gemma Service",
            "initialized": self.is_initialized,
            "model_path": r"C:\Users\fbg67\.cache\kagglehub\models\google\gemma-3n\transformers\gemma-3n-e2b-it\1",
            "backend": "AutoModelForCausalLM",
            "device": "cpu",
            "optimizations": [
                "notebook_inspired", "max_new_tokens_100", "concise_prompts",
                "torch_no_grad", "cpu_stable", "eos_pad_token"
            ],
            "expected_response_time": "60-90 seconds",
            "config": self.optimized_config
        }
    
    def test_connection(self):
        """Teste usando mesma abordagem do notebook"""
        try:
            if not self.is_initialized:
                return False, "Notebook-Gemma n√£o inicializado"
            
            test_prompt = "Como plantar alface?"
            start_time = time.time()
            
            response = self.notebook_style_query(test_prompt, context="agricultura")
            test_time = time.time() - start_time
            
            if "Fallback" not in response:
                return True, f"Notebook-Gemma funcionando ({test_time:.1f}s)"
            else:
                return False, "Notebook-Gemma retornou fallback"
                
        except Exception as e:
            return False, f"Erro no teste notebook: {e}"
    
    def benchmark_vs_docs_service(self):
        """Comparar performance com DocsGemmaService"""
        logger.info("üìä BENCHMARK: Notebook-Style vs Docs-Style")
        
        test_prompts = [
            "Como preparar solo para arroz?",
            "Sintomas de mal√°ria?", 
            "O que √© fotoss√≠ntese?"
        ]
        
        results = {}
        
        for prompt in test_prompts:
            start_time = time.time()
            response = self.notebook_style_query(prompt)
            elapsed = time.time() - start_time
            
            results[prompt] = {
                "time": elapsed,
                "length": len(response),
                "success": "Fallback" not in response
            }
            
            logger.info(f"üìã {prompt[:20]}... -> {elapsed:.1f}s")
        
        return results

# Teste r√°pido de integra√ß√£o
if __name__ == "__main__":
    print("üß™ TESTANDO NOTEBOOK-INSPIRED GEMMA SERVICE")
    print("=" * 55)
    
    service = NotebookInspiredGemmaService()
    
    if service.is_initialized:
        print("‚úÖ Servi√ßo inicializado com sucesso!")
        
        # Teste r√°pido
        test_response = service.generate_response(
            "Como plantar milho na √©poca das chuvas?",
            context="agricultura"
        )
        
        print(f"üìã Teste conclu√≠do: {len(test_response)} chars")
        print(f"üéØ Primeira linha: {test_response.split('\n')[0]}")
        
        # Status
        status = service.get_status()
        print(f"‚öôÔ∏è Config: {status['expected_response_time']}")
    else:
        print("‚ùå Falha na inicializa√ß√£o - execute o notebook primeiro!")
