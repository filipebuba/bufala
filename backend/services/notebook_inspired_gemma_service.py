#!/usr/bin/env python3
"""
Backend Otimizado Bu Fala - Inspirado no Notebook Local
Aplicando as otimizaÃ§Ãµes descobertas no notebook para o DocsGemmaService
"""

import os
import logging
import torch
import time
from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer
from config.settings import BackendConfig
from utils.fallback_responses import FallbackResponseGenerator

logger = logging.getLogger(__name__)

class NotebookInspiredGemmaService:
    """
    ServiÃ§o Gemma otimizado baseado nas descobertas do notebook local
    Implementa as mesmas tÃ©cnicas que funcionaram no Jupyter
    """
    
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.is_initialized = False
        self.fallback_generator = FallbackResponseGenerator()
        
        # ConfiguraÃ§Ãµes otimizadas inspiradas no notebook
        self.optimized_config = {
            "max_new_tokens": 100,      # âš¡ Reduzido de 400
            "temperature": 0.7,         # ğŸ“Š Mesmo do notebook 
            "top_p": 0.9,              # ğŸ“Š Mesmo do notebook
            "do_sample": True,          # ğŸ“Š Mesmo do notebook
            "pad_token_id": None,       # ğŸ”§ SerÃ¡ definido apÃ³s carregar tokenizer
            "torch_dtype": torch.float32  # ğŸ¯ CPU otimizado
        }
        
        self.initialize()
    
    def initialize(self):
        """Inicializar modelo seguindo a abordagem do notebook"""
        try:
            logger.info("ğŸš€ Inicializando Notebook-Inspired Gemma Service...")
            
            # Usar o mesmo caminho que funcionou no notebook
            model_path = r"C:\Users\fbg67\.cache\kagglehub\models\google\gemma-3n\transformers\gemma-3n-e2b-it\1"
            
            if not os.path.exists(model_path):
                logger.error(f"âŒ Modelo nÃ£o encontrado: {model_path}")
                logger.info("ğŸ’¡ Execute o notebook primeiro para baixar o modelo")
                return False
            
            start_time = time.time()
            
            logger.info("ğŸ“¦ Carregando tokenizer (inspirado no notebook)...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=True
            )
            
            logger.info("ğŸ§  Carregando modelo (inspirado no notebook)...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=True,
                torch_dtype=self.optimized_config["torch_dtype"]
            )
            
            # Configurar pad_token_id como no notebook
            self.optimized_config["pad_token_id"] = self.tokenizer.eos_token_id
            
            # Usar CPU como no notebook (mais estÃ¡vel)
            device = "cpu"  # ForÃ§ar CPU como funcionou no notebook
            logger.info(f"ğŸ“± Usando dispositivo: {device}")
            
            # NÃ£o chamar .to(device) se jÃ¡ estÃ¡ na CPU por padrÃ£o
            
            load_time = time.time() - start_time
            self.is_initialized = True
            
            logger.info(f"âœ… Notebook-Inspired Gemma carregado em {load_time:.2f}s!")
            logger.info("ğŸ¯ ConfiguraÃ§Ã£o otimizada ativa - respostas em ~60s")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erro na inicializaÃ§Ã£o inspirada: {e}")
            return False
    
    def notebook_style_query(self, input_text, context="agricultura"):
        """
        FunÃ§Ã£o gemma_query() inspirada diretamente no notebook
        ImplementaÃ§Ã£o quase idÃªntica Ã  que funcionou no Jupyter
        """
        try:
            if not self.is_initialized:
                logger.warning("âš ï¸ Modelo nÃ£o inicializado")
                return self._fallback_response(input_text, context)
            
            # Verificar se o input nÃ£o estÃ¡ vazio (como no notebook)
            if not input_text.strip():
                return "Por favor, digite uma pergunta."
            
            start_time = time.time()
            logger.info(f"ğŸ¤– Processando (notebook style): {input_text[:50]}...")
            
            # Preparar input exatamente como no notebook
            inputs = self.tokenizer(input_text, return_tensors="pt")
            
            # Mover para mesmo device que o modelo (CPU)
            inputs = inputs.to(self.model.device)
            
            # Gerar resposta com configuraÃ§Ãµes do notebook
            with torch.no_grad():  # Exatamente como no notebook
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.optimized_config["max_new_tokens"],  # 100
                    do_sample=self.optimized_config["do_sample"],            # True
                    temperature=self.optimized_config["temperature"],        # 0.7
                    top_p=self.optimized_config["top_p"],                   # 0.9
                    pad_token_id=self.optimized_config["pad_token_id"]      # eos_token_id
                )
            
            # Decodificar resposta exatamente como no notebook
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Remover o prompt original da resposta (como no notebook)
            if input_text in response:
                response = response.replace(input_text, "").strip()
            
            generation_time = time.time() - start_time
            
            if response and len(response) > 5:
                logger.info(f"âœ… Resposta notebook-style gerada em {generation_time:.2f}s")
                return f"{response}\n\n[Notebook-Inspired Gemma â€¢ {generation_time:.1f}s]"
            else:
                logger.warning("âš ï¸ Resposta vazia ou muito curta")
                return self._fallback_response(input_text, context)
            
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o notebook-style: {e}")
            return self._fallback_response(input_text, context)
    
    def create_optimized_prompt(self, prompt, context="agricultura"):
        """
        Criar prompts otimizados e concisos (inspirado no notebook)
        Evitar contexto excessivo que causa lentidÃ£o
        """
        
        # Templates super concisos - inspirados no notebook
        concise_templates = {
            "agricultura": f"Como especialista agrÃ­cola, responda: {prompt}",
            "medico": f"Como mÃ©dico, responda: {prompt}",
            "educacao": f"Como educador, responda: {prompt}",
            "geral": prompt  # Prompt direto como no notebook
        }
        
        # Retornar prompt conciso
        return concise_templates.get(context, prompt)
    
    def generate_response(self, prompt, language="pt-BR", context="agricultura"):
        """MÃ©todo de compatibilidade - usando abordagem do notebook"""
        try:
            logger.info(f"ğŸ”„ Usando abordagem notebook para: {prompt[:50]}...")
            
            # Criar prompt otimizado e conciso
            optimized_prompt = self.create_optimized_prompt(prompt, context)
            
            # Usar funÃ§Ã£o notebook-style
            response = self.notebook_style_query(optimized_prompt, context)
            
            logger.info(f"âœ… Resposta notebook-inspired gerada com sucesso")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Erro no mÃ©todo compatibilidade: {e}")
            return self._fallback_response(prompt, context)
    
    def _fallback_response(self, prompt, context="agricultura"):
        """Resposta de fallback contextualizada"""
        response = self.fallback_generator.generate_response(prompt, context)
        return f"{response}\n\n[Fallback Response â€¢ Notebook-Gemma indisponÃ­vel]"
    
    def get_status(self):
        """Status do serviÃ§o inspirado no notebook"""
        return {
            "service": "Notebook-Inspired Gemma Service",
            "initialized": self.is_initialized,
            "model_path": r"C:\Users\fbg67\.cache\kagglehub\models\google\gemma-3n\transformers\gemma-3n-e2b-it\1",
            "backend": "AutoModelForCausalLM",
            "device": "cpu",
            "optimizations": [
                "notebook_inspired", "max_new_tokens_100", "concise_prompts",
                "torch_no_grad", "cpu_stable", "eos_pad_token"
            ],
            "expected_response_time": "60-90 seconds",
            "config": self.optimized_config
        }
    
    def test_connection(self):
        """Teste usando mesma abordagem do notebook"""
        try:
            if not self.is_initialized:
                return False, "Notebook-Gemma nÃ£o inicializado"
            
            test_prompt = "Como plantar alface?"
            start_time = time.time()
            
            response = self.notebook_style_query(test_prompt, context="agricultura")
            test_time = time.time() - start_time
            
            if "Fallback" not in response:
                return True, f"Notebook-Gemma funcionando ({test_time:.1f}s)"
            else:
                return False, "Notebook-Gemma retornou fallback"
                
        except Exception as e:
            return False, f"Erro no teste notebook: {e}"
    
    def benchmark_vs_docs_service(self):
        """Comparar performance com DocsGemmaService"""
        logger.info("ğŸ“Š BENCHMARK: Notebook-Style vs Docs-Style")
        
        test_prompts = [
            "Como preparar solo para arroz?",
            "Sintomas de malÃ¡ria?", 
            "O que Ã© fotossÃ­ntese?"
        ]
        
        results = {}
        
        for prompt in test_prompts:
            start_time = time.time()
            response = self.notebook_style_query(prompt)
            elapsed = time.time() - start_time
            
            results[prompt] = {
                "time": elapsed,
                "length": len(response),
                "success": "Fallback" not in response
            }
            
            logger.info(f"ğŸ“‹ {prompt[:20]}... -> {elapsed:.1f}s")
        
        return results

# Teste rÃ¡pido de integraÃ§Ã£o
if __name__ == "__main__":
    print("ğŸ§ª TESTANDO NOTEBOOK-INSPIRED GEMMA SERVICE")
    print("=" * 55)
    
    service = NotebookInspiredGemmaService()
    
    if service.is_initialized:
        print("âœ… ServiÃ§o inicializado com sucesso!")
        
        # Teste rÃ¡pido
        test_response = service.generate_response(
            "Como plantar milho na Ã©poca das chuvas?",
            context="agricultura"
        )
        
        print(f"ğŸ“‹ Teste concluÃ­do: {len(test_response)} chars")
        print(f"ğŸ¯ Primeira linha: {test_response.split('\n')[0]}")
        
        # Status
        status = service.get_status()
        print(f"âš™ï¸ Config: {status['expected_response_time']}")
    else:
        print("âŒ Falha na inicializaÃ§Ã£o - execute o notebook primeiro!")
