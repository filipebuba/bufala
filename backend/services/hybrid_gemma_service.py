"""
Servi√ßo Gemma 3n H√≠brido - M√°xima Otimiza√ß√£o
Combina√ß√£o das melhores pr√°ticas da documenta√ß√£o oficial + otimiza√ß√µes avan√ßadas
"""

import os
import logging
import torch
import time
import threading
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from transformers import AutoProcessor, AutoModelForImageTextToText
from config.settings import BackendConfig
from utils.fallback_responses import FallbackResponseGenerator

logger = logging.getLogger(__name__)

class HybridGemmaService:
    """Servi√ßo Gemma 3n h√≠brido com m√°xima otimiza√ß√£o"""
    
    def __init__(self):
        self.processor = None
        self.model = None
        self.is_initialized = False
        self.fallback_generator = FallbackResponseGenerator()
        self.model_path = BackendConfig.MODEL_PATH
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.generation_lock = threading.Lock()
        self.initialize()
    
    def initialize(self):
        """Inicializar modelo com otimiza√ß√µes h√≠bridas"""
        try:
            logger.info("üöÄ Inicializando Hybrid Gemma Service...")
            
            if not os.path.exists(self.model_path):
                logger.error(f"‚ùå Modelo n√£o encontrado: {self.model_path}")
                return False
            
            start_time = time.time()
            
            # Configura√ß√µes de otimiza√ß√£o
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            # Processor otimizado
            logger.info("üì¶ Carregando processor h√≠brido...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_path,
                local_files_only=True,
                trust_remote_code=True,
                use_fast=True  # Tokenizer r√°pido
            )
            
            # Modelo com otimiza√ß√µes h√≠bridas
            logger.info("üß† Carregando modelo h√≠brido...")
            self.model = AutoModelForImageTextToText.from_pretrained(
                self.model_path,
                torch_dtype=torch.float32,   # Usar float32 para estabilidade
                device_map=None,             # Evitar meta tensors
                local_files_only=True,
                trust_remote_code=True,
                low_cpu_mem_usage=True       # Baixo uso de RAM
            )
            
            # Configurar para infer√™ncia
            self.model.eval()
            
            # Compilar modelo se poss√≠vel (PyTorch 2.0+)
            if hasattr(torch, 'compile'):
                try:
                    logger.info("‚ö° Compilando modelo para m√°xima velocidade...")
                    self.model = torch.compile(self.model, mode="max-autotune")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Compila√ß√£o n√£o dispon√≠vel: {e}")
            
            load_time = time.time() - start_time
            self.is_initialized = True
            
            logger.info(f"‚úÖ Hybrid Gemma carregado em {load_time:.2f}s!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o h√≠brida: {e}")
            return False
    
    def _create_optimized_messages(self, prompt, context="agricultura"):
        """Criar mensagens otimizadas para m√°xima efici√™ncia"""
        
        # Prompts otimizados e concisos
        systems = {
            "agricultura": "Especialista em agricultura. Responda de forma pr√°tica e direta.",
            "medico": "Assistente m√©dico qualificado. Informe com precis√£o m√©dica.",
            "educacao": "Professor experiente. Explique de forma clara e did√°tica.",
            "geral": "Assistente inteligente. Responda de forma √∫til e precisa."
        }
        
        system_prompt = systems.get(context, systems["geral"])
        
        # Formato otimizado para velocidade
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]
            },
            {
                "role": "user",
                "content": [{"type": "text", "text": prompt}]
            }
        ]
        
        return messages
    
    def _generate_with_timeout(self, inputs, timeout=45):
        """Gera√ß√£o com timeout para evitar travamentos"""
        def generate():
            with torch.inference_mode():
                return self.model.generate(
                    **inputs,
                    max_new_tokens=400,         # Otimizado para velocidade
                    do_sample=True,
                    temperature=0.7,            # Menos criativo, mais r√°pido
                    top_p=0.85,                 # Nucleus sampling otimizado
                    top_k=40,                   # Top-k sampling para velocidade
                    repetition_penalty=1.1,     # Evitar repeti√ß√µes
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    use_cache=True              # Cache para velocidade
                )
        
        try:
            future = self.executor.submit(generate)
            return future.result(timeout=timeout)
        except TimeoutError:
            logger.warning(f"‚è∞ Timeout de {timeout}s atingido")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o: {e}")
            return None
    
    def ask_gemma(self, prompt, context="agricultura", timeout=60):
        """M√©todo principal h√≠brido otimizado"""
        try:
            if not self.is_initialized:
                logger.warning("‚ö†Ô∏è Modelo h√≠brido n√£o inicializado")
                return self._fallback_response(prompt, context)
            
            with self.generation_lock:  # Thread safety
                start_time = time.time()
                
                # Criar mensagens otimizadas
                messages = self._create_optimized_messages(prompt, context)
                
                logger.info(f"üî• Processando com Hybrid Gemma: {prompt[:50]}...")
                
                # Aplicar chat template otimizado
                inputs = self.processor.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    tokenize=True,
                    return_dict=True,
                    return_tensors="pt"
                ).to(self.model.device, dtype=self.model.dtype)
                
                input_len = inputs["input_ids"].shape[-1]
                
                # Gerar com timeout
                generation = self._generate_with_timeout(inputs, timeout)
                
                if generation is None:
                    logger.warning("‚ö†Ô∏è Gera√ß√£o falhou ou timeout")
                    return self._fallback_response(prompt, context)
                
                # Extrair apenas a resposta gerada
                generation = generation[0][input_len:]
                
                # Decodificar otimizado
                decoded = self.processor.decode(
                    generation,
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True
                )
                
                generation_time = time.time() - start_time
                
                if decoded and len(decoded.strip()) > 5:
                    result = decoded.strip()
                    logger.info(f"‚úÖ Resposta h√≠brida gerada em {generation_time:.2f}s")
                    return f"{result}\n\n[Hybrid Gemma ‚Ä¢ {generation_time:.1f}s]"
                else:
                    logger.warning("‚ö†Ô∏è Resposta vazia do modelo h√≠brido")
                    return self._fallback_response(prompt, context)
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o h√≠brida: {e}")
            return self._fallback_response(prompt, context)
    
    def _fallback_response(self, prompt, context="agricultura"):
        """Resposta de fallback contextualizada"""
        response = self.fallback_generator.generate_response(prompt, context)
        return f"{response}\n\n[Fallback Response ‚Ä¢ Modelo h√≠brido indispon√≠vel]"
    
    def get_status(self):
        """Status do servi√ßo h√≠brido"""
        return {
            "service": "Hybrid Gemma Service",
            "initialized": self.is_initialized,
            "model_path": self.model_path,
            "backend": "AutoModelForImageTextToText",
            "optimizations": [
                "bfloat16", "inference_mode", "torch_compile", 
                "thread_safety", "timeout_control", "cache_enabled",
                "fast_tokenizer", "low_cpu_mem"
            ]
        }
    
    def test_connection(self):
        """Teste otimizado de funcionamento"""
        try:
            if not self.is_initialized:
                return False, "Modelo h√≠brido n√£o inicializado"
            
            test_prompt = "Teste de velocidade"
            start_time = time.time()
            
            response = self.ask_gemma(test_prompt, context="geral")
            test_time = time.time() - start_time
            
            if "Fallback" not in response:
                return True, f"Modelo h√≠brido funcionando ({test_time:.1f}s)"
            else:
                return False, "Modelo h√≠brido retornou fallback"
                
        except Exception as e:
            return False, f"Erro no teste h√≠brido: {e}"
    
    def __del__(self):
        """Cleanup ao destruir o objeto"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)
