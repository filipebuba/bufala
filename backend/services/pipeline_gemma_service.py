"""
Servi√ßo Gemma 3n Pipeline - Baseado na Documenta√ß√£o Oficial
Implementa√ß√£o otimizada usando pipeline API para m√°xima velocidade
"""

import os
import logging
import torch
import time
from transformers import pipeline
from config.settings import BackendConfig
from utils.fallback_responses import FallbackResponseGenerator

logger = logging.getLogger(__name__)

class PipelineGemmaService:
    """Servi√ßo Gemma 3n usando pipeline API da documenta√ß√£o oficial"""
    
    def __init__(self):
        self.pipe = None
        self.is_initialized = False
        self.fallback_generator = FallbackResponseGenerator()
        self.model_path = BackendConfig.MODEL_PATH
        self.initialize()
    
    def initialize(self):
        """Inicializar pipeline otimizado conforme documenta√ß√£o oficial"""
        try:
            logger.info("üöÄ Inicializando Pipeline Gemma Service...")
            
            if not os.path.exists(self.model_path):
                logger.error(f"‚ùå Modelo n√£o encontrado: {self.model_path}")
                return False
            
            start_time = time.time()
            
            # Pipeline otimizado conforme documenta√ß√£o oficial
            logger.info("üî• Criando pipeline otimizado...")
            self.pipe = pipeline(
                "text-generation",     # Task correta para modelos de texto
                model=self.model_path,
                device_map=None,       # Usar None para evitar meta tensors
                torch_dtype=torch.float32,  # Usar float32 para estabilidade
                trust_remote_code=True,
                local_files_only=True
            )
            
            load_time = time.time() - start_time
            self.is_initialized = True
            
            logger.info(f"‚úÖ Pipeline Gemma carregado em {load_time:.2f}s!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")
            return False
    
    def _create_chat_messages(self, prompt, context="agricultura"):
        """Criar mensagens usando chat template da documenta√ß√£o oficial"""
        
        # Sistema especializado por contexto
        system_prompts = {
            "agricultura": "Voc√™ √© um especialista em agricultura brasileira. Responda de forma clara, pr√°tica e √∫til sobre cultivo, plantio, pragas, solo e t√©cnicas agr√≠colas.",
            "medico": "Voc√™ √© um assistente m√©dico especializado. Forne√ßa informa√ß√µes m√©dicas precisas, mas sempre recomende consultar um profissional de sa√∫de.",
            "educacao": "Voc√™ √© um educador experiente. Explique conceitos de forma did√°tica, clara e adequada ao n√≠vel do estudante.",
            "geral": "Voc√™ √© um assistente √∫til e conhecedor. Responda de forma clara, precisa e informativa."
        }
        
        system_content = system_prompts.get(context, system_prompts["geral"])
        
        # Formato de mensagens conforme documenta√ß√£o oficial
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": system_content}]
            },
            {
                "role": "user", 
                "content": [{"type": "text", "text": prompt}]
            }
        ]
        
        return messages
    
    def ask_gemma(self, prompt, context="agricultura", timeout=60):
        """M√©todo principal usando pipeline API otimizada"""
        try:
            if not self.is_initialized:
                logger.warning("‚ö†Ô∏è Pipeline n√£o inicializado, usando fallback")
                return self._fallback_response(prompt, context)
            
            start_time = time.time()
            
            # Criar prompt simples
            system_prompts = {
                "agricultura": "Voc√™ √© um especialista em agricultura brasileira. Responda de forma clara, pr√°tica e √∫til sobre cultivo, plantio, pragas, solo e t√©cnicas agr√≠colas.",
                "medico": "Voc√™ √© um assistente m√©dico especializado. Forne√ßa informa√ß√µes m√©dicas precisas, mas sempre recomende consultar um profissional de sa√∫de.",
                "educacao": "Voc√™ √© um educador experiente. Explique conceitos de forma did√°tica, clara e adequada ao n√≠vel do estudante.",
                "geral": "Voc√™ √© um assistente √∫til e conhecedor. Responda de forma clara, precisa e informativa."
            }
            
            system_content = system_prompts.get(context, system_prompts["geral"])
            full_prompt = f"{system_content}\n\nUsu√°rio: {prompt}\nAssistente:"
            
            logger.info(f"ü§ñ Gerando resposta com pipeline para: {prompt[:50]}...")
            
            # Gerar usando pipeline
            output = self.pipe(
                full_prompt,
                max_new_tokens=512,
                temperature=0.8,
                top_p=0.9,
                do_sample=True,
                return_full_text=False
            )
            
            generation_time = time.time() - start_time
            
            # Extrair resposta gerada
            if output and len(output) > 0:
                if isinstance(output, list):
                    generated_text = output[0].get("generated_text", "")
                else:
                    generated_text = str(output)
                
                result = generated_text.strip()
                
                if result and len(result) > 10:
                    logger.info(f"‚úÖ Resposta pipeline gerada em {generation_time:.2f}s")
                    return f"{result}\n\n[Pipeline Gemma ‚Ä¢ {generation_time:.1f}s]"
            
            logger.warning("‚ö†Ô∏è Resposta vazia do pipeline")
            return self._fallback_response(prompt, context)
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o pipeline: {e}")
            return self._fallback_response(prompt, context)
    
    def _fallback_response(self, prompt, context="agricultura"):
        """Resposta de fallback contextualizada"""
        response = self.fallback_generator.generate_response(prompt, context)
        return f"{response}\n\n[Fallback Response ‚Ä¢ Pipeline n√£o dispon√≠vel]"
    
    def get_status(self):
        """Status do servi√ßo"""
        return {
            "service": "Pipeline Gemma Service",
            "initialized": self.is_initialized,
            "model_path": self.model_path,
            "backend": "transformers.pipeline",
            "optimizations": ["auto_device_map", "auto_dtype", "chat_templates"]
        }
    
    def test_connection(self):
        """Teste r√°pido de conex√£o"""
        try:
            if not self.is_initialized:
                return False, "Pipeline n√£o inicializado"
            
            test_prompt = "Ol√°, como voc√™ est√°?"
            start_time = time.time()
            
            response = self.ask_gemma(test_prompt, context="geral")
            test_time = time.time() - start_time
            
            if "Fallback" not in response:
                return True, f"Pipeline funcionando ({test_time:.1f}s)"
            else:
                return False, "Pipeline retornou fallback"
                
        except Exception as e:
            return False, f"Erro no teste: {e}"
