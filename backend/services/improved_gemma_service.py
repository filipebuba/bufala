"""
Servi√ßo Gemma 3n otimizado baseado na implementa√ß√£o bem-sucedida do Kaggle
"""

import logging
import traceback
import gc
import torch
import threading
import time
from datetime import datetime
from transformers import AutoProcessor, AutoModelForImageTextToText

from config.settings import BackendConfig

logger = logging.getLogger(__name__)

class ImprovedGemmaService:
    """Servi√ßo Gemma 3n melhorado baseado na implementa√ß√£o de sucesso"""
    
    def __init__(self):
        self.processor = None
        self.model = None
        self.is_initialized = False
        self.loading_method = "improved"
        
        # Tentar inicializar
        self._initialize()
    
    def _initialize(self):
        """Inicializar o servi√ßo com a nova abordagem"""
        try:
            logger.info("üöÄ Inicializando Gemma 3n com abordagem melhorada...")
            
            # Carregar processor
            logger.info("üì¶ Carregando processor...")
            self.processor = AutoProcessor.from_pretrained(
                BackendConfig.MODEL_PATH,
                local_files_only=True,
                trust_remote_code=True
            )
            
            # Carregar modelo multimodal
            logger.info("üß† Carregando modelo multimodal...")
            self.model = AutoModelForImageTextToText.from_pretrained(
                BackendConfig.MODEL_PATH,
                torch_dtype="auto",  # Deixar PyTorch decidir o melhor tipo
                device_map="auto",   # Distribui√ß√£o autom√°tica de dispositivos
                local_files_only=True,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            self.is_initialized = True
            logger.info("‚úÖ Modelo Gemma 3n carregado com sucesso!")
            logger.info(f"üìä Dispositivo do modelo: {self.model.device}")
            logger.info(f"üìä Tipo de dados: {self.model.dtype}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            self.is_initialized = False
    
    def generate_response(self, prompt, language='pt-BR', subject='general'):
        """Gerar resposta usando a nova abordagem"""
        if not self.is_initialized:
            logger.warning("‚ö†Ô∏è Modelo n√£o inicializado, usando fallback")
            return self._generate_fallback_response(prompt, language, subject)
        
        try:
            return self._generate_with_improved_method(prompt, language, subject)
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o: {e}")
            return self._generate_fallback_response(prompt, language, subject)
    
    def _generate_with_improved_method(self, prompt, language, subject):
        """Gerar usando o m√©todo melhorado com timeout"""
        
        def _generate_internal():
            logger.info(f"üîß Gerando resposta melhorada para: {prompt[:50]}...")
            start_time = time.time()
            
            # Formatar prompt
            formatted_prompt = self._format_prompt(prompt, language, subject)
            logger.info(f"üìù Prompt formatado: {formatted_prompt[:100]}...")
            
            # Processar entrada (como no exemplo de sucesso)
            inputs = self.processor(
                text=formatted_prompt, 
                return_tensors="pt"
            ).to(self.model.device, dtype=self.model.dtype)
            
            logger.info(f"üìä Input shape: {list(inputs.values())[0].shape}")
            logger.info(f"üíæ Input device: {list(inputs.values())[0].device}")
            
            # Gerar resposta (usando configura√ß√µes do exemplo de sucesso)
            logger.info("üß† Iniciando gera√ß√£o...")
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,  # Aumentado para respostas mais completas
                    disable_compile=True,  # Chave do sucesso!
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.processor.tokenizer.pad_token_id if hasattr(self.processor, 'tokenizer') else None
                )
            
            # Decodificar resposta
            text = self.processor.batch_decode(
                outputs,
                skip_special_tokens=True,  # Mudan√ßa: skip_special_tokens=True
                clean_up_tokenization_spaces=True  # Mudan√ßa: limpar espa√ßos
            )
            
            generated_text = text[0] if text else ""
            
            # Remover o prompt original da resposta
            if formatted_prompt in generated_text:
                generated_text = generated_text.replace(formatted_prompt, "").strip()
            
            end_time = time.time()
            duration = end_time - start_time
            
            logger.info(f"üìù Texto gerado em {duration:.2f}s ({len(generated_text)} chars)")
            logger.info(f"üìÑ Preview: {generated_text[:200]}...")
            
            if generated_text and len(generated_text.strip()) >= 10:
                timestamp = datetime.now().strftime("%H:%M:%S")
                full_response = f"{generated_text.strip()}\n\n[Gerado por Gemma-3n Melhorado em {duration:.1f}s √†s {timestamp}]"
                logger.info(f"‚úÖ Resposta gerada com sucesso em {duration:.2f}s!")
                return full_response
            else:
                logger.warning(f"‚ö†Ô∏è Resposta muito curta: '{generated_text[:100]}'")
                return None
        
        # Usar threading para timeout de 45 segundos
        result_container = [None]
        exception_container = [None]
        
        def wrapper():
            try:
                result_container[0] = _generate_internal()
            except Exception as e:
                exception_container[0] = e
        
        thread = threading.Thread(target=wrapper)
        thread.daemon = True
        thread.start()
        thread.join(timeout=45)  # 45 segundos de timeout
        
        if thread.is_alive():
            logger.error("‚è∞ Timeout na gera√ß√£o (45s excedidos)")
            return self._generate_fallback_response(prompt, language, subject)
        
        if exception_container[0]:
            logger.error(f"‚ùå Erro na gera√ß√£o: {exception_container[0]}")
            return self._generate_fallback_response(prompt, language, subject)
        
        if result_container[0]:
            return result_container[0]
        else:
            logger.warning("‚ö†Ô∏è Resultado vazio, usando fallback")
            return self._generate_fallback_response(prompt, language, subject)
    
    def _format_prompt(self, prompt, language, subject):
        """Formatar prompt de acordo com o contexto"""
        context_map = {
            'medical': 'Voc√™ √© um assistente m√©dico especializado em sa√∫de comunit√°ria.',
            'agriculture': 'Voc√™ √© um especialista em agricultura sustent√°vel e cultivos tropicais.',
            'education': 'Voc√™ √© um educador especializado em ensino adaptativo.',
            'general': 'Voc√™ √© um assistente √∫til e informativo.'
        }
        
        context = context_map.get(subject, context_map['general'])
        
        if language.startswith('crioulo') or language == 'crp':
            context += " Responda em crioulo da Guin√©-Bissau."
        else:
            context += " Responda em portugu√™s brasileiro."
        
        return f"{context}\n\nPergunta: {prompt}\n\nResposta:"
    
    def _generate_fallback_response(self, prompt, language, subject):
        """Gerar resposta de fallback"""
        fallback_responses = {
            'agriculture': {
                'pt-BR': 'Para quest√µes agr√≠colas, recomendo: verificar a qualidade do solo, usar sementes certificadas, e seguir pr√°ticas sustent√°veis. Consulte um agr√¥nomo local para orienta√ß√µes espec√≠ficas.',
                'crioulo': 'Pa kestion di agrikultura, i misti: verifika tera, uza siminti bon, i sigui pratika sustentavel. Fala ku agronomo di zona pa orientason.'
            },
            'medical': {
                'pt-BR': 'Para quest√µes m√©dicas, √© importante procurar orienta√ß√£o profissional. Em caso de emerg√™ncia, procure o hospital mais pr√≥ximo.',
                'crioulo': 'Pa kestion di saude, i misti buskada orientason profisional. Si e emerjensia, bai na hospital mas perto.'
            },
            'general': {
                'pt-BR': 'Desculpe, n√£o consegui processar sua solicita√ß√£o no momento. Tente novamente em alguns instantes.',
                'crioulo': 'Diskulpa, n kapa prosesu bo pedidu agora. Tenta fali dja dja.'
            }
        }
        
        lang_key = 'crioulo' if language.startswith('crioulo') or language == 'crp' else 'pt-BR'
        subject_responses = fallback_responses.get(subject, fallback_responses['general'])
        
        return f"{subject_responses[lang_key]}\n\n[Resposta de emerg√™ncia - Servi√ßo temporariamente indispon√≠vel]"
    
    def get_status(self):
        """Obter status do servi√ßo"""
        return {
            'initialized': self.is_initialized,
            'loading_method': self.loading_method,
            'model_available': self.model is not None,
            'processor_available': self.processor is not None,
            'device': str(self.model.device) if self.model else 'N/A',
            'dtype': str(self.model.dtype) if self.model else 'N/A'
        }
