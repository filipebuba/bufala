"""
Backend Bu Fala Otimizado para Limita√ß√µes de Mem√≥ria
Implementa estrat√©gias para usar o Gemma 3n com recursos limitados
"""

import os
import logging
import torch
import traceback
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoProcessor,
    pipeline,
    BitsAndBytesConfig
)
from config.settings import BackendConfig, SystemPrompts
from utils.fallback_responses import FallbackResponseGenerator
import gc

logger = logging.getLogger(__name__)

class OptimizedGemmaService:
    """Servi√ßo Gemma otimizado para recursos limitados"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.pipeline = None
        self.is_initialized = False
        self.loading_method = None
        self.generation_config = BackendConfig.get_generation_config()
        self.fallback_generator = FallbackResponseGenerator()
        
        # Configura√ß√µes de otimiza√ß√£o
        self.max_memory_usage = "6GB"  # Limite de mem√≥ria
        self.use_disk_offload = True
        self.use_quantization = True
        
        self.initialize()
    
    def initialize(self):
        """Inicializar com estrat√©gias de otimiza√ß√£o de mem√≥ria"""
        try:
            logger.info("üöÄ Inicializando Gemma 3n com otimiza√ß√µes de mem√≥ria...")
            
            # Verificar modelo
            if not os.path.exists(BackendConfig.MODEL_PATH):
                logger.error(f"‚ùå Modelo n√£o encontrado: {BackendConfig.MODEL_PATH}")
                self._fallback_to_simple_responses()
                return False
            
            # Estrat√©gia 1: Tentar carregamento com quantiza√ß√£o
            if self._load_with_quantization():
                logger.info("‚úÖ Carregado com quantiza√ß√£o")
                self.loading_method = "quantized"
                self.is_initialized = True
                return True
            
            # Estrat√©gia 2: Tentar carregamento com disk offload
            if self._load_with_disk_offload():
                logger.info("‚úÖ Carregado com disk offload")
                self.loading_method = "disk_offload"
                self.is_initialized = True
                return True
            
            # Estrat√©gia 3: Carregar apenas tokenizer para processamento local
            if self._load_tokenizer_only():
                logger.info("‚úÖ Carregado apenas tokenizer")
                self.loading_method = "tokenizer_only"
                self.is_initialized = True
                return True
            
            # Estrat√©gia 4: Fallback completo
            logger.warning("‚ö†Ô∏è Usando fallback completo")
            self._fallback_to_simple_responses()
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")
            self._fallback_to_simple_responses()
            return True  # Sempre retorna True para manter o sistema funcionando
    
    def _load_with_quantization(self):
        """Tentar carregamento com quantiza√ß√£o para reduzir uso de mem√≥ria"""
        try:
            logger.info("üì¶ Tentando carregamento com quantiza√ß√£o...")
            
            # Configura√ß√£o de quantiza√ß√£o
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
                llm_int8_skip_modules=None,
                llm_int8_enable_fp32_cpu_offload=True
            )
            
            # Carregar tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                BackendConfig.MODEL_PATH,
                local_files_only=True,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Tentar carregar modelo com quantiza√ß√£o
            self.model = AutoModelForCausalLM.from_pretrained(
                BackendConfig.MODEL_PATH,
                quantization_config=quantization_config,
                device_map="auto",
                local_files_only=True,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )
            
            logger.info("‚úÖ Modelo carregado com quantiza√ß√£o 8-bit")
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Quantiza√ß√£o falhou: {e}")
            return False
    
    def _load_with_disk_offload(self):
        """Tentar carregamento com offload para disco"""
        try:
            logger.info("üíæ Tentando carregamento com disk offload...")
            
            # Carregar tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                BackendConfig.MODEL_PATH,
                local_files_only=True,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Carregar modelo diretamente na CPU para evitar problemas de meta tensor
            self.model = AutoModelForCausalLM.from_pretrained(
                BackendConfig.MODEL_PATH,
                local_files_only=True,
                trust_remote_code=True,
                torch_dtype=torch.float32,  # Usar float32 para CPU
                low_cpu_mem_usage=True,
                device_map=None  # N√£o usar device_map autom√°tico
            )
            
            # Mover explicitamente para CPU
            self.model = self.model.to('cpu')
            self.model.eval()
            
            logger.info("‚úÖ Modelo carregado com disk offload")
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Disk offload falhou: {e}")
            return False
            
            # Aplicar disk offload
            offload_folder = os.path.join(os.path.dirname(BackendConfig.MODEL_PATH), "offload")
            os.makedirs(offload_folder, exist_ok=True)
            
            self.model = disk_offload(
                model=self.model,
                offload_dir=offload_folder,
                execution_device="cpu"
            )
            
            logger.info("‚úÖ Modelo carregado com disk offload")
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Disk offload falhou: {e}")
            return False
    
    def _load_tokenizer_only(self):
        """Carregar apenas tokenizer para processamento b√°sico"""
        try:
            logger.info("üî§ Carregando apenas tokenizer...")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                BackendConfig.MODEL_PATH,
                local_files_only=True,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Tamb√©m carregar processor se dispon√≠vel
            try:
                self.processor = AutoProcessor.from_pretrained(
                    BackendConfig.MODEL_PATH,
                    local_files_only=True,
                    trust_remote_code=True
                )
                logger.info("‚úÖ Processor tamb√©m carregado")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Processor n√£o carregado: {e}")
            
            logger.info("‚úÖ Tokenizer carregado - modo b√°sico ativo")
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Carregamento do tokenizer falhou: {e}")
            return False
    
    def _fallback_to_simple_responses(self):
        """Configurar para usar apenas respostas simples"""
        self.loading_method = "fallback_only"
        self.is_initialized = True
        logger.info("‚úÖ Modo fallback ativado - respostas pr√©-definidas")
    
    def generate_response(self, prompt, language='pt-BR', subject='general'):
        """Gerar resposta com base no m√©todo dispon√≠vel"""
        try:
            # Limpar cache de mem√≥ria
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            if self.loading_method == "quantized" and self.model and self.tokenizer:
                return self._generate_with_quantized_model(prompt, language, subject)
            
            elif self.loading_method == "disk_offload" and self.model and self.tokenizer:
                return self._generate_with_disk_offload(prompt, language, subject)
            
            elif self.loading_method == "tokenizer_only" and self.tokenizer:
                return self._generate_with_tokenizer_processing(prompt, language, subject)
            
            else:
                return self._generate_fallback_response(prompt, language, subject)
                
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o: {e}")
            return self._generate_fallback_response(prompt, language, subject)
    
    def _generate_with_quantized_model(self, prompt, language, subject):
        """Gerar com modelo quantizado"""
        try:
            formatted_prompt = self._format_prompt(prompt, language, subject)
            
            inputs = self.tokenizer(
                formatted_prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512  # Reduzido para economizar mem√≥ria
            )
            
            # Configura√ß√µes conservativas para economia de mem√≥ria
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100,  # Reduzido
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
            
            if generated_text and len(generated_text.strip()) >= 10:
                return f"{generated_text}\n\n[Gerado por Gemma-3n Quantizado]"
            else:
                return self._generate_fallback_response(prompt, language, subject)
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Gera√ß√£o quantizada falhou: {e}")
            return self._generate_fallback_response(prompt, language, subject)
    
    def _generate_with_disk_offload(self, prompt, language, subject):
        """Gerar com modelo em disk offload com timeout usando threading"""
        import threading
        import time
        
        def _generate_internal():
            logger.info(f"üîß Gerando resposta disk offload para: {prompt[:50]}...")
            start_time = time.time()
            
            formatted_prompt = self._format_prompt(prompt, language, subject)
            logger.info(f"üìù Prompt formatado: {formatted_prompt[:100]}...")
            
            inputs = self.tokenizer(
                formatted_prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # Garantir que os inputs estejam na CPU
            inputs = {k: v.to('cpu') for k, v in inputs.items()}
            
            logger.info(f"üìä Input shape: {inputs['input_ids'].shape}")
            logger.info(f"üíæ Input device: {inputs['input_ids'].device}")
            
            # Configura√ß√µes otimizadas para velocidade
            generation_config = {
                'max_new_tokens': 64,   # Reduzido drasticamente para acelerar
                'min_new_tokens': 5,    # M√≠nimo muito baixo
                'do_sample': True,
                'temperature': 0.7,
                'top_p': 0.9,
                'top_k': 20,           # Muito reduzido
                'repetition_penalty': 1.05,
                'pad_token_id': self.tokenizer.pad_token_id,
                'eos_token_id': self.tokenizer.eos_token_id,
                'early_stopping': True,
                'num_beams': 1,
                'use_cache': True
            }
            
            logger.info("üß† Iniciando gera√ß√£o com o modelo...")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
            
            end_time = time.time()
            duration = end_time - start_time
            
            logger.info(f"üìù Texto gerado em {duration:.2f}s ({len(generated_text)} chars): {generated_text[:100]}...")
            
            if generated_text and len(generated_text.strip()) >= 5:
                from datetime import datetime
                timestamp = datetime.now().strftime("%H:%M:%S")
                full_response = f"{generated_text}\n\n[Gerado por Gemma-3n em {duration:.1f}s √†s {timestamp}]"
                logger.info(f"‚úÖ Resposta gerada com sucesso em {duration:.2f}s!")
                return full_response
            else:
                logger.warning(f"‚ö†Ô∏è Resposta muito curta ou vazia: '{generated_text}'")
                return None
        
        # Usar threading para timeout
        result_container = [None]
        exception_container = [None]
        
        def wrapper():
            try:
                result_container[0] = _generate_internal()
            except Exception as e:
                exception_container[0] = e
        
        thread = threading.Thread(target=wrapper)
        thread.daemon = True
        thread.start()
        
        # Aguardar com timeout de 60 segundos
        thread.join(timeout=60)
        
        if thread.is_alive():
            logger.error("‚è∞ Timeout na gera√ß√£o (60s excedidos)")
            return self._generate_fallback_response(prompt, language, subject)
        
        if exception_container[0]:
            logger.error(f"‚ùå Erro na gera√ß√£o: {exception_container[0]}")
            return self._generate_fallback_response(prompt, language, subject)
        
        if result_container[0]:
            return result_container[0]
        else:
            logger.warning("‚ö†Ô∏è Resultado vazio, usando fallback")
            return self._generate_fallback_response(prompt, language, subject)
    
    def _generate_with_tokenizer_processing(self, prompt, language, subject):
        """Gerar usando apenas processamento do tokenizer"""
        try:
            # Usar tokenizer para an√°lise e processamento b√°sico
            tokens = self.tokenizer.encode(prompt)
            token_count = len(tokens)
            
            # An√°lise b√°sica baseada no prompt
            analysis = self._analyze_prompt(prompt, subject)
            
            response = f"""
{analysis}

[An√°lise baseada em processamento de texto - Token count: {token_count}]
[Gemma-3n Tokenizer Processing Mode]
"""
            return response.strip()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Processamento tokenizer falhou: {e}")
            return self._generate_fallback_response(prompt, language, subject)
    
    def _analyze_prompt(self, prompt, subject):
        """An√°lise inteligente do prompt baseada em padr√µes"""
        prompt_lower = prompt.lower()
        
        # An√°lises por dom√≠nio
        if subject == 'medical' or any(word in prompt_lower for word in ['dor', 'sintoma', 'doen√ßa', 'm√©dico', 'sa√∫de']):
            return self._medical_analysis(prompt)
        elif subject == 'education' or any(word in prompt_lower for word in ['aprender', 'estudar', 'escola', 'ensino']):
            return self._education_analysis(prompt)
        elif subject == 'agriculture' or any(word in prompt_lower for word in ['planta', 'cultivo', 'agricultura', 'solo']):
            return self._agriculture_analysis(prompt)
        elif subject == 'wellness' or any(word in prompt_lower for word in ['bem-estar', 'exerc√≠cio', 'stress', 'vida']):
            return self._wellness_analysis(prompt)
        else:
            return self._general_analysis(prompt)
    
    def _medical_analysis(self, prompt):
        """An√°lise m√©dica baseada em padr√µes"""
        responses = [
            "Para quest√µes de sa√∫de, √© fundamental consultar um profissional m√©dico qualificado.",
            "Os sintomas descritos podem ter v√°rias causas. Procure orienta√ß√£o m√©dica.",
            "Recomendo manter uma dieta equilibrada, exerc√≠cios regulares e consultar seu m√©dico.",
            "Em caso de emerg√™ncia ou sintomas graves, procure atendimento m√©dico imediato."
        ]
        import random
        return random.choice(responses)
    
    def _education_analysis(self, prompt):
        """An√°lise educacional baseada em padr√µes"""
        responses = [
            "O aprendizado √© mais eficaz quando combinamos teoria e pr√°tica.",
            "Use diferentes m√©todos de estudo: leitura, exerc√≠cios, discuss√µes e resumos.",
            "Fa√ßa pausas regulares durante os estudos para melhor reten√ß√£o do conhecimento.",
            "Procure fontes confi√°veis e diversifique seus recursos de aprendizado."
        ]
        import random
        return random.choice(responses)
    
    def _agriculture_analysis(self, prompt):
        """An√°lise agr√≠cola baseada em padr√µes"""
        responses = [
            "Para melhores resultados, consulte um engenheiro agr√¥nomo da sua regi√£o.",
            "Mantenha o solo bem drenado e com pH adequado para suas culturas.",
            "Monitore suas plantas regularmente para detectar pragas e doen√ßas cedo.",
            "Use pr√°ticas sustent√°veis: rota√ß√£o de culturas, compostagem e controle biol√≥gico."
        ]
        import random
        return random.choice(responses)
    
    def _wellness_analysis(self, prompt):
        """An√°lise de bem-estar baseada em padr√µes"""
        responses = [
            "O bem-estar envolve equil√≠brio entre corpo, mente e relacionamentos.",
            "Pratique exerc√≠cios regulares, alimenta√ß√£o saud√°vel e tenha boas noites de sono.",
            "Gerencie o stress com t√©cnicas de relaxamento, medita√ß√£o ou hobbies.",
            "Mantenha conex√µes sociais positivas e reserve tempo para atividades prazerosas."
        ]
        import random
        return random.choice(responses)
    
    def _general_analysis(self, prompt):
        """An√°lise geral baseada em padr√µes"""
        responses = [
            "Esta √© uma quest√£o interessante que pode ter v√°rias perspectivas.",
            "Para uma resposta mais espec√≠fica, forne√ßa mais contexto sobre sua situa√ß√£o.",
            "Recomendo pesquisar em fontes confi√°veis e consultar especialistas da √°rea.",
            "Cada situa√ß√£o √© √∫nica, ent√£o considere seus objetivos e circunst√¢ncias espec√≠ficas."
        ]
        import random
        return random.choice(responses)
    
    def _generate_fallback_response(self, prompt, language, subject):
        """Gerar resposta usando o sistema de fallback"""
        return self.fallback_generator.generate_response(prompt, language, subject)
    
    def _format_prompt(self, prompt, language, subject):
        """Formatar prompt para o assunto espec√≠fico com estrutura melhorada"""
        prompts = {
            'medical': f"""Voc√™ √© um assistente de sa√∫de especializado. Responda √† seguinte pergunta m√©dica de forma clara e √∫til:

Pergunta: {prompt}

Forne√ßa orienta√ß√µes √∫teis, mas sempre recomende consultar um profissional de sa√∫de para casos espec√≠ficos.

Resposta:""",
            'education': f"""Voc√™ √© um professor experiente. Ajude com a seguinte quest√£o educacional:

Pergunta: {prompt}

Explique de forma did√°tica e completa, adequada para o aprendizado.

Resposta:""",
            'agriculture': f"""Voc√™ √© um especialista em agricultura sustent√°vel. Responda sobre a seguinte quest√£o agr√≠cola:

Pergunta: {prompt}

Forne√ßa conselhos pr√°ticos para agricultura sustent√°vel.

Resposta:""",
            'wellness': f"""Voc√™ √© um especialista em bem-estar. Ajude com a seguinte quest√£o:

Pergunta: {prompt}

Forne√ßa orienta√ß√µes para uma vida mais saud√°vel e equilibrada.

Resposta:""",
            'translate': f"""Voc√™ √© um tradutor especializado em portugu√™s e crioulo da Guin√©-Bissau. Traduza:

Texto: {prompt}

Forne√ßa uma tradu√ß√£o precisa e culturalmente apropriada.

Tradu√ß√£o:""",
            'environmental': f"""Voc√™ √© um especialista ambiental. Responda sobre:

Pergunta: {prompt}

Forne√ßa orienta√ß√µes ecol√≥gicas pr√°ticas.

Resposta:"""
        }
        
        return prompts.get(subject, f"""Voc√™ √© um assistente inteligente e √∫til. Responda √† seguinte pergunta:

Pergunta: {prompt}

Forne√ßa uma resposta completa e informativa.

Resposta:""")
    
    def get_status(self):
        """Obter status do servi√ßo otimizado"""
        return {
            'initialized': self.is_initialized,
            'loading_method': self.loading_method,
            'device': BackendConfig.get_device(),
            'model_path': BackendConfig.MODEL_PATH,
            'optimizations_enabled': True,
            'memory_optimized': True,
            'quantization_used': self.loading_method == "quantized",
            'disk_offload_used': self.loading_method == "disk_offload",
            'tokenizer_only': self.loading_method == "tokenizer_only",
            'fallback_mode': self.loading_method == "fallback_only",
            'model_version': 'gemma-3n-e2b-it-optimized'
        }
