"""
Servi√ßo Gemma 3n Oficial - Implementa√ß√£o Direta
Baseado na documenta√ß√£o oficial usando Gemma3nForConditionalGeneration
"""

import os
import logging
import torch
import time
from transformers import AutoProcessor, AutoModelForImageTextToText
from config.settings import BackendConfig
from utils.fallback_responses import FallbackResponseGenerator

logger = logging.getLogger(__name__)

class OfficialGemmaService:
    """Servi√ßo Gemma 3n usando implementa√ß√£o oficial direta"""
    
    def __init__(self):
        self.processor = None
        self.model = None
        self.is_initialized = False
        self.fallback_generator = FallbackResponseGenerator()
        self.model_path = BackendConfig.MODEL_PATH
        self.initialize()
    
    def initialize(self):
        """Inicializar modelo conforme documenta√ß√£o oficial"""
        try:
            logger.info("üìã Inicializando Official Gemma Service...")
            
            if not os.path.exists(self.model_path):
                logger.error(f"‚ùå Modelo n√£o encontrado: {self.model_path}")
                return False
            
            start_time = time.time()
            
            # Processor oficial
            logger.info("üì¶ Carregando AutoProcessor...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_path,
                local_files_only=True,
                trust_remote_code=True
            )
            
            # Modelo oficial com configura√ß√µes da documenta√ß√£o
            logger.info("üß† Carregando AutoModelForImageTextToText...")
            self.model = AutoModelForImageTextToText.from_pretrained(
                self.model_path,
                device_map=None,        # Evitar meta tensors
                torch_dtype=torch.float32,  # Usar float32 para estabilidade
                local_files_only=True,
                trust_remote_code=True
            ).eval()  # Modo de avalia√ß√£o para infer√™ncia
            
            load_time = time.time() - start_time
            self.is_initialized = True
            
            logger.info(f"‚úÖ Official Gemma carregado em {load_time:.2f}s!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o oficial: {e}")
            return False
    
    def _create_chat_messages(self, prompt, context="agricultura"):
        """Criar mensagens usando formato oficial da documenta√ß√£o"""
        
        # Sistemas especializados
        systems = {
            "agricultura": "Voc√™ √© um agr√¥nomo especialista em agricultura brasileira. Forne√ßa conselhos pr√°ticos sobre cultivo, manejo de solo, controle de pragas e t√©cnicas agr√≠colas sustent√°veis.",
            "medico": "Voc√™ √© um assistente m√©dico qualificado. Forne√ßa informa√ß√µes m√©dicas precisas e atualizadas, sempre enfatizando a import√¢ncia de consultar profissionais de sa√∫de.",
            "educacao": "Voc√™ √© um professor experiente e did√°tico. Explique conceitos de forma clara, estruturada e adaptada ao n√≠vel de compreens√£o do estudante.",
            "geral": "Voc√™ √© um assistente inteligente e √∫til. Responda de forma clara, precisa e informativa sobre qualquer t√≥pico."
        }
        
        system_prompt = systems.get(context, systems["geral"])
        
        # Formato oficial da documenta√ß√£o
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]
            },
            {
                "role": "user",
                "content": [{"type": "text", "text": prompt}]
            }
        ]
        
        return messages
    
    def ask_gemma(self, prompt, context="agricultura", timeout=90):
        """M√©todo principal usando implementa√ß√£o oficial"""
        try:
            if not self.is_initialized:
                logger.warning("‚ö†Ô∏è Modelo n√£o inicializado, usando fallback")
                return self._fallback_response(prompt, context)
            
            start_time = time.time()
            
            # Criar mensagens com chat template
            messages = self._create_chat_messages(prompt, context)
            
            logger.info(f"ü§ñ Processando com Official Gemma: {prompt[:50]}...")
            
            # Aplicar chat template conforme documenta√ß√£o oficial
            inputs = self.processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt"
            ).to(self.model.device, dtype=torch.bfloat16)
            
            input_len = inputs["input_ids"].shape[-1]
            
            # Gerar com torch.inference_mode() para m√°xima efici√™ncia
            with torch.inference_mode():
                generation = self.model.generate(
                    **inputs,
                    max_new_tokens=512,     # Tamanho adequado
                    do_sample=True,         # Amostragem ativa
                    temperature=0.8,        # Criatividade moderada
                    top_p=0.9,             # Nucleus sampling
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )
                
                # Extrair apenas a parte gerada
                generation = generation[0][input_len:]
            
            # Decodificar conforme documenta√ß√£o oficial
            decoded = self.processor.decode(
                generation, 
                skip_special_tokens=True
            )
            
            generation_time = time.time() - start_time
            
            if decoded and len(decoded.strip()) > 5:
                result = decoded.strip()
                logger.info(f"‚úÖ Resposta oficial gerada em {generation_time:.2f}s")
                return f"{result}\n\n[Official Gemma ‚Ä¢ {generation_time:.1f}s]"
            else:
                logger.warning("‚ö†Ô∏è Resposta vazia do modelo oficial")
                return self._fallback_response(prompt, context)
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o oficial: {e}")
            return self._fallback_response(prompt, context)
    
    def _fallback_response(self, prompt, context="agricultura"):
        """Resposta de fallback contextualizada"""
        response = self.fallback_generator.generate_response(prompt, context)
        return f"{response}\n\n[Fallback Response ‚Ä¢ Modelo oficial indispon√≠vel]"
    
    def get_status(self):
        """Status do servi√ßo oficial"""
        return {
            "service": "Official Gemma Service",
            "initialized": self.is_initialized,
            "model_path": self.model_path,
            "backend": "Gemma3nForConditionalGeneration",
            "optimizations": ["inference_mode", "bfloat16", "chat_templates", "eval_mode"]
        }
    
    def test_connection(self):
        """Teste r√°pido de funcionamento"""
        try:
            if not self.is_initialized:
                return False, "Modelo oficial n√£o inicializado"
            
            test_prompt = "Como cultivar tomates em casa?"
            start_time = time.time()
            
            response = self.ask_gemma(test_prompt, context="agricultura")
            test_time = time.time() - start_time
            
            if "Fallback" not in response:
                return True, f"Modelo oficial funcionando ({test_time:.1f}s)"
            else:
                return False, "Modelo oficial retornou fallback"
                
        except Exception as e:
            return False, f"Erro no teste oficial: {e}"
