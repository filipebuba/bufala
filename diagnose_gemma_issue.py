#!/usr/bin/env python3
"""
üîç DIAGN√ìSTICO ESPEC√çFICO: Por que o Gemma-3n n√£o est√° sendo usado
"""

import os
import sys
import traceback
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def diagnose_gemma_issue():
    print("üîç DIAGN√ìSTICO DO PROBLEMA GEMMA-3N")
    print("=" * 60)
    
    # 1. Verificar caminho do modelo
    model_path = r"C:\Users\fbg67\.cache\kagglehub\models\google\gemma-3n\transformers\gemma-3n-e2b-it\1"
    print(f"üìÅ Verificando caminho: {model_path}")
    
    if os.path.exists(model_path):
        print("‚úÖ Caminho do modelo existe")
        
        # Listar arquivos no diret√≥rio
        files = os.listdir(model_path)
        print(f"üìÑ Arquivos encontrados: {files}")
        
        # Verificar arquivos essenciais
        essential_files = ['config.json', 'tokenizer.json', 'pytorch_model.bin']
        missing_files = [f for f in essential_files if f not in files]
        if missing_files:
            print(f"‚ùå Arquivos essenciais faltando: {missing_files}")
        else:
            print("‚úÖ Arquivos essenciais presentes")
    else:
        print("‚ùå Caminho do modelo N√ÉO existe")
        return False
    
    # 2. Verificar se PyTorch funciona
    print("\nüß™ Testando PyTorch...")
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üîß Dispositivo dispon√≠vel: {device}")
        
        # Teste b√°sico de tensor
        test_tensor = torch.tensor([1, 2, 3])
        print(f"‚úÖ PyTorch funcionando: {test_tensor}")
    except Exception as e:
        print(f"‚ùå Erro no PyTorch: {e}")
        return False
    
    # 3. Tentar carregar tokenizer
    print("\nüìù Testando carregamento do tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            local_files_only=True,
            trust_remote_code=True
        )
        print("‚úÖ Tokenizer carregado com sucesso")
    except Exception as e:
        print(f"‚ùå Erro ao carregar tokenizer: {e}")
        print(f"‚ùå Traceback: {traceback.format_exc()}")
        return False
    
    # 4. Tentar carregar modelo (simplificado)
    print("\nüß† Testando carregamento do modelo...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,
            trust_remote_code=True,
            torch_dtype=torch.float32,  # Usar float32 para compatibilidade
            device_map=None  # N√£o usar device_map autom√°tico
        )
        print("‚úÖ Modelo carregado com sucesso")
        
        # 5. Teste simples de gera√ß√£o
        print("\nüöÄ Testando gera√ß√£o de texto...")
        test_prompt = "Ol√°"
        inputs = tokenizer(test_prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=5,
                do_sample=False,
                temperature=0.1
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"‚úÖ Gera√ß√£o funcionando: {response}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao carregar modelo: {e}")
        print(f"‚ùå Traceback: {traceback.format_exc()}")
        return False

def check_backend_initialization():
    """Verificar especificamente o problema na inicializa√ß√£o do backend"""
    print("\nüîß VERIFICANDO INICIALIZA√á√ÉO DO BACKEND")
    print("=" * 50)
    
    try:
        # Simular a classe do backend
        class TestBackend:
            def __init__(self):
                self.model = None
                self.tokenizer = None
                self.is_initialized = False
                self.transformers_model_path = r"C:\Users\fbg67\.cache\kagglehub\models\google\gemma-3n\transformers\gemma-3n-e2b-it\1"
                
                # Testar inicializa√ß√£o
                if self.load_gemma3n_model():
                    self.is_initialized = True
                    print("‚úÖ Backend inicializado com sucesso")
                else:
                    print("‚ùå Falha na inicializa√ß√£o do backend")
            
            def load_gemma3n_model(self):
                try:
                    print("üîß Carregando modelo no backend...")
                    
                    # Carregar tokenizer
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        self.transformers_model_path,
                        local_files_only=True,
                        trust_remote_code=True
                    )
                    
                    # Carregar modelo
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.transformers_model_path,
                        local_files_only=True,
                        trust_remote_code=True,
                        torch_dtype=torch.float32
                    )
                    
                    print("‚úÖ Modelo carregado no backend")
                    return True
                    
                except Exception as e:
                    print(f"‚ùå Erro no backend: {e}")
                    return False
            
            def test_generation(self):
                if not self.is_initialized:
                    print("‚ùå Backend n√£o inicializado")
                    return False
                
                try:
                    prompt = "Coach de bem-estar: Como posso relaxar?\nOrienta√ß√£o:"
                    inputs = self.tokenizer(prompt, return_tensors="pt")
                    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=20,
                            do_sample=True,
                            temperature=0.3,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                    
                    # Decodificar apenas a parte nova
                    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
                    response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    
                    print(f"‚úÖ Resposta gerada: {response}")
                    return True
                    
                except Exception as e:
                    print(f"‚ùå Erro na gera√ß√£o: {e}")
                    return False
        
        # Testar backend
        backend = TestBackend()
        if backend.is_initialized:
            backend.test_generation()
        
    except Exception as e:
        print(f"‚ùå Erro no teste do backend: {e}")
        print(f"‚ùå Traceback: {traceback.format_exc()}")

if __name__ == "__main__":
    # Executar diagn√≥sticos
    print("üîç Iniciando diagn√≥stico completo...\n")
    
    # Diagn√≥stico b√°sico
    basic_ok = diagnose_gemma_issue()
    
    if basic_ok:
        # Teste espec√≠fico do backend
        check_backend_initialization()
        
        print("\nüéØ CONCLUS√ÉO:")
        print("Se todos os testes passaram, o problema pode estar em:")
        print("1. Condi√ß√£o no c√≥digo que est√° for√ßando fallback")
        print("2. Erro silencioso na gera√ß√£o")
        print("3. Problema na l√≥gica de verifica√ß√£o is_initialized")
    else:
        print("\n‚ùå PROBLEMA ENCONTRADO:")
        print("O modelo Gemma-3n n√£o est√° carregando corretamente.")
        print("Verifique se o modelo foi baixado e est√° no caminho correto.")
